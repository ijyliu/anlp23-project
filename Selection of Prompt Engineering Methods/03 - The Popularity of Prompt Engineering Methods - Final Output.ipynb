{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Popularity of Prompt Engineering Methods\n",
    "\n",
    "This series of notebooks produces statistics on Semantic Scholar citations per day for all of the prompt engineering approaches listed at \"https://www.promptingguide.ai/papers\", \"https://en.wikipedia.org/wiki/Prompt_engineering#Text-to-text\", and the citations section for \"The Practicality of Prompt Engineering\".\n",
    "\n",
    "This file produces final Excel and LaTeX output from the citations per day data and hand-labelled supplementary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper title</th>\n",
       "      <th>semantic scholar title</th>\n",
       "      <th>ss_publication_date</th>\n",
       "      <th>ss_year</th>\n",
       "      <th>citation_count</th>\n",
       "      <th>query</th>\n",
       "      <th>day_queried</th>\n",
       "      <th>end_date</th>\n",
       "      <th>days_from_pub_to_end_date</th>\n",
       "      <th>citations_per_day</th>\n",
       "      <th>Prompt Engineering Method</th>\n",
       "      <th>Generalizable?</th>\n",
       "      <th>Ease of Implementation? (1=easy, 5=hard)</th>\n",
       "      <th>Performance Gain - Rough Order of Magnitude</th>\n",
       "      <th>Notes</th>\n",
       "      <th>Datasets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Language Models are Few-Shot Learners</td>\n",
       "      <td>Language Models are Few-Shot Learners</td>\n",
       "      <td>2020-05-28</td>\n",
       "      <td>2020</td>\n",
       "      <td>16440</td>\n",
       "      <td>Language Models are Few Shot Learners</td>\n",
       "      <td>2023-10-22</td>\n",
       "      <td>2023-10-22 13:33:02.768</td>\n",
       "      <td>1242.564615</td>\n",
       "      <td>13.230700</td>\n",
       "      <td>Few-Shot Learning</td>\n",
       "      <td>Y</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Original GPT-3 Paper</td>\n",
       "      <td>Various</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPT-4 Technical Report</td>\n",
       "      <td>GPT-4 Technical Report</td>\n",
       "      <td>2023-03-15</td>\n",
       "      <td>2023</td>\n",
       "      <td>1529</td>\n",
       "      <td>GPT 4 Technical Report</td>\n",
       "      <td>2023-10-22</td>\n",
       "      <td>2023-10-22 13:33:02.768</td>\n",
       "      <td>221.564615</td>\n",
       "      <td>6.900921</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Original GPT-4 Paper</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Training language models to follow instruction...</td>\n",
       "      <td>Training language models to follow instruction...</td>\n",
       "      <td>2022-03-04</td>\n",
       "      <td>2022</td>\n",
       "      <td>3009</td>\n",
       "      <td>Training language models to follow instruction...</td>\n",
       "      <td>2023-10-22</td>\n",
       "      <td>2023-10-22 13:33:02.768</td>\n",
       "      <td>597.564615</td>\n",
       "      <td>5.035439</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fine-tuning, introduces Instruct-GPT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chain of Thought Prompting Elicits Reasoning i...</td>\n",
       "      <td>Chain of Thought Prompting Elicits Reasoning i...</td>\n",
       "      <td>2022-01-28</td>\n",
       "      <td>2022</td>\n",
       "      <td>2105</td>\n",
       "      <td>Chain of Thought Prompting Elicits Reasoning i...</td>\n",
       "      <td>2023-10-22</td>\n",
       "      <td>2023-10-22 13:33:02.768</td>\n",
       "      <td>632.564615</td>\n",
       "      <td>3.327723</td>\n",
       "      <td>Chain-of-Thought Prompting</td>\n",
       "      <td>Y</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Focuses on PaLM</td>\n",
       "      <td>GSM8K, Last Letter, Various</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Scaling Instruction-Finetuned Language Models</td>\n",
       "      <td>Scaling Instruction-Finetuned Language Models</td>\n",
       "      <td>2022-10-20</td>\n",
       "      <td>2022</td>\n",
       "      <td>823</td>\n",
       "      <td>Scaling Instruction Finetuned Language Models</td>\n",
       "      <td>2023-10-22</td>\n",
       "      <td>2023-10-22 13:33:02.768</td>\n",
       "      <td>367.564615</td>\n",
       "      <td>2.239062</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fine-tuning, introduces Instruct-GPT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>More than you've asked for: A Comprehensive An...</td>\n",
       "      <td>More than you've asked for: A Comprehensive An...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2023</td>\n",
       "      <td>37</td>\n",
       "      <td>More than you've asked for: A Comprehensive An...</td>\n",
       "      <td>2023-10-22</td>\n",
       "      <td>2023-10-22 13:33:02.768</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>On the Advance of Making Language Models Bette...</td>\n",
       "      <td>On the Advance of Making Language Models Bette...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2022</td>\n",
       "      <td>106</td>\n",
       "      <td>On the Advance of Making Language Models Bette...</td>\n",
       "      <td>2023-10-22</td>\n",
       "      <td>2023-10-22 13:33:02.768</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Prefix-Tuning: Optimizing Continuous Prompts f...</td>\n",
       "      <td>Prefix-Tuning: Optimizing Continuous Prompts f...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2021</td>\n",
       "      <td>1682</td>\n",
       "      <td>Prefix Tuning: Optimizing Continuous Prompts f...</td>\n",
       "      <td>2023-10-22</td>\n",
       "      <td>2023-10-22 13:33:02.768</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Prompt Engineering for Large Language Models</td>\n",
       "      <td>Prompt Engineering for Large Language Models</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>Prompt Engineering for Large Language Models</td>\n",
       "      <td>2023-10-22</td>\n",
       "      <td>2023-10-22 13:33:02.768</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Reflexion: an autonomous agent with dynamic me...</td>\n",
       "      <td>Reflexion: an autonomous agent with dynamic me...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2023</td>\n",
       "      <td>105</td>\n",
       "      <td>Reflexion: an autonomous agent with dynamic me...</td>\n",
       "      <td>2023-10-22</td>\n",
       "      <td>2023-10-22 13:33:02.768</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>161 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           paper title  \\\n",
       "0                Language Models are Few-Shot Learners   \n",
       "1                               GPT-4 Technical Report   \n",
       "2    Training language models to follow instruction...   \n",
       "3    Chain of Thought Prompting Elicits Reasoning i...   \n",
       "4        Scaling Instruction-Finetuned Language Models   \n",
       "..                                                 ...   \n",
       "156  More than you've asked for: A Comprehensive An...   \n",
       "157  On the Advance of Making Language Models Bette...   \n",
       "158  Prefix-Tuning: Optimizing Continuous Prompts f...   \n",
       "159       Prompt Engineering for Large Language Models   \n",
       "160  Reflexion: an autonomous agent with dynamic me...   \n",
       "\n",
       "                                semantic scholar title ss_publication_date  \\\n",
       "0                Language Models are Few-Shot Learners          2020-05-28   \n",
       "1                               GPT-4 Technical Report          2023-03-15   \n",
       "2    Training language models to follow instruction...          2022-03-04   \n",
       "3    Chain of Thought Prompting Elicits Reasoning i...          2022-01-28   \n",
       "4        Scaling Instruction-Finetuned Language Models          2022-10-20   \n",
       "..                                                 ...                 ...   \n",
       "156  More than you've asked for: A Comprehensive An...                 NaT   \n",
       "157  On the Advance of Making Language Models Bette...                 NaT   \n",
       "158  Prefix-Tuning: Optimizing Continuous Prompts f...                 NaT   \n",
       "159       Prompt Engineering for Large Language Models                 NaT   \n",
       "160  Reflexion: an autonomous agent with dynamic me...                 NaT   \n",
       "\n",
       "     ss_year  citation_count  \\\n",
       "0       2020           16440   \n",
       "1       2023            1529   \n",
       "2       2022            3009   \n",
       "3       2022            2105   \n",
       "4       2022             823   \n",
       "..       ...             ...   \n",
       "156     2023              37   \n",
       "157     2022             106   \n",
       "158     2021            1682   \n",
       "159     2023               3   \n",
       "160     2023             105   \n",
       "\n",
       "                                                 query day_queried  \\\n",
       "0                Language Models are Few Shot Learners  2023-10-22   \n",
       "1                               GPT 4 Technical Report  2023-10-22   \n",
       "2    Training language models to follow instruction...  2023-10-22   \n",
       "3    Chain of Thought Prompting Elicits Reasoning i...  2023-10-22   \n",
       "4        Scaling Instruction Finetuned Language Models  2023-10-22   \n",
       "..                                                 ...         ...   \n",
       "156  More than you've asked for: A Comprehensive An...  2023-10-22   \n",
       "157  On the Advance of Making Language Models Bette...  2023-10-22   \n",
       "158  Prefix Tuning: Optimizing Continuous Prompts f...  2023-10-22   \n",
       "159       Prompt Engineering for Large Language Models  2023-10-22   \n",
       "160  Reflexion: an autonomous agent with dynamic me...  2023-10-22   \n",
       "\n",
       "                   end_date  days_from_pub_to_end_date  citations_per_day  \\\n",
       "0   2023-10-22 13:33:02.768                1242.564615          13.230700   \n",
       "1   2023-10-22 13:33:02.768                 221.564615           6.900921   \n",
       "2   2023-10-22 13:33:02.768                 597.564615           5.035439   \n",
       "3   2023-10-22 13:33:02.768                 632.564615           3.327723   \n",
       "4   2023-10-22 13:33:02.768                 367.564615           2.239062   \n",
       "..                      ...                        ...                ...   \n",
       "156 2023-10-22 13:33:02.768                        NaN                NaN   \n",
       "157 2023-10-22 13:33:02.768                        NaN                NaN   \n",
       "158 2023-10-22 13:33:02.768                        NaN                NaN   \n",
       "159 2023-10-22 13:33:02.768                        NaN                NaN   \n",
       "160 2023-10-22 13:33:02.768                        NaN                NaN   \n",
       "\n",
       "      Prompt Engineering Method Generalizable?  \\\n",
       "0             Few-Shot Learning              Y   \n",
       "1                           NaN            NaN   \n",
       "2                           NaN            NaN   \n",
       "3    Chain-of-Thought Prompting              Y   \n",
       "4                           NaN            NaN   \n",
       "..                          ...            ...   \n",
       "156                         NaN            NaN   \n",
       "157                         NaN            NaN   \n",
       "158                         NaN            NaN   \n",
       "159                         NaN            NaN   \n",
       "160                         NaN            NaN   \n",
       "\n",
       "     Ease of Implementation? (1=easy, 5=hard)  \\\n",
       "0                                         4.0   \n",
       "1                                         NaN   \n",
       "2                                         NaN   \n",
       "3                                         3.0   \n",
       "4                                         NaN   \n",
       "..                                        ...   \n",
       "156                                       NaN   \n",
       "157                                       NaN   \n",
       "158                                       NaN   \n",
       "159                                       NaN   \n",
       "160                                       NaN   \n",
       "\n",
       "    Performance Gain - Rough Order of Magnitude  \\\n",
       "0                                           0.1   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           0.4   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "156                                         NaN   \n",
       "157                                         NaN   \n",
       "158                                         NaN   \n",
       "159                                         NaN   \n",
       "160                                         NaN   \n",
       "\n",
       "                                    Notes                     Datasets  \n",
       "0                    Original GPT-3 Paper                      Various  \n",
       "1                    Original GPT-4 Paper                          NaN  \n",
       "2    Fine-tuning, introduces Instruct-GPT                          NaN  \n",
       "3                         Focuses on PaLM  GSM8K, Last Letter, Various  \n",
       "4    Fine-tuning, introduces Instruct-GPT                          NaN  \n",
       "..                                    ...                          ...  \n",
       "156                                   NaN                          NaN  \n",
       "157                                   NaN                          NaN  \n",
       "158                                   NaN                          NaN  \n",
       "159                                   NaN                          NaN  \n",
       "160                                   NaN                          NaN  \n",
       "\n",
       "[161 rows x 16 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load input Excel file \"Hand-Labeled Method and Implementation Considerations.xlsx\"\n",
    "# Column \"Performance Gain - Rough Order of Magnitude\" should be a string\n",
    "df = pd.read_excel(\"Hand-Labeled Method and Implementation Considerations.xlsx\", converters={'Performance Gain - Rough Order of Magnitude':str})\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper title</th>\n",
       "      <th>citations_per_day</th>\n",
       "      <th>Prompt Engineering Method</th>\n",
       "      <th>Generalizable?</th>\n",
       "      <th>Ease of Implementation? (1=easy, 5=hard)</th>\n",
       "      <th>Performance Gain - Rough Order of Magnitude</th>\n",
       "      <th>Datasets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Language Models are Few-Shot Learners</td>\n",
       "      <td>13.230700</td>\n",
       "      <td>Few-Shot Learning</td>\n",
       "      <td>Y</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Various</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chain of Thought Prompting Elicits Reasoning in Large Language Models</td>\n",
       "      <td>3.327723</td>\n",
       "      <td>Chain-of-Thought Prompting</td>\n",
       "      <td>Y</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>GSM8K, Last Letter, Various</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Large Language Models are Zero-Shot Reasoners</td>\n",
       "      <td>1.707434</td>\n",
       "      <td>Zero-Shot Chain-of-Thought</td>\n",
       "      <td>Y</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>GSM8K, Last Letter, Coin Flip, Date Understanding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tree of Thoughts: Deliberate Problem Solving with Large Language Models</td>\n",
       "      <td>1.431593</td>\n",
       "      <td>Tree-of-Thought</td>\n",
       "      <td>Y</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>Game of 24, Creative Writing, Mini Crosswords</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Self-Refine: Iterative Refinement with Self-Feedback</td>\n",
       "      <td>0.973061</td>\n",
       "      <td>Self-Refine</td>\n",
       "      <td>Y</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>GSM8K, PIE, CommonGen, CodeNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ReAct: Synergizing Reasoning and Acting in Language Models</td>\n",
       "      <td>0.870102</td>\n",
       "      <td>ReAct</td>\n",
       "      <td>Y</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10-34%</td>\n",
       "      <td>HotpotQA, Fever, ALFWorld, WebShop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Least-to-most prompting enables complex reasoning in large language models</td>\n",
       "      <td>0.710210</td>\n",
       "      <td>Least-to-Most Prompting</td>\n",
       "      <td>Y</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0-15%</td>\n",
       "      <td>Last Letter, SCAN, GSM8K, DROP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>PAL: Program-aided Language Models</td>\n",
       "      <td>0.584822</td>\n",
       "      <td>Program Aided Language Models</td>\n",
       "      <td>N</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Large Language Models Are Human-Level Prompt Engineers</td>\n",
       "      <td>0.548697</td>\n",
       "      <td>Automatic Prompt Engineer</td>\n",
       "      <td>Y</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Big-Bench Instruction Induction (BBII), Big-Bench Hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>How Can We Know What Language Models Know?</td>\n",
       "      <td>0.535602</td>\n",
       "      <td>Prompt Mining, Prompt Paraphrasing</td>\n",
       "      <td>N</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>LAMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Automatic Chain of Thought Prompting in Large Language Models</td>\n",
       "      <td>0.528163</td>\n",
       "      <td>Automatic Chain of Thought Prompting</td>\n",
       "      <td>Y</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5-10%</td>\n",
       "      <td>MultiArith, GSM8K, CommonSenseQA, SVAMP, AQUA-RAT, StrategyQA, Last Letter, Coin Flip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Show Your Work: Scratchpads for Intermediate Computation with Language Models</td>\n",
       "      <td>0.455489</td>\n",
       "      <td>Scratchpads</td>\n",
       "      <td>Y</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10-15%</td>\n",
       "      <td>Long addition, division, execution of arbitrary programs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Multimodal Chain-of-Thought Reasoning in Language Models</td>\n",
       "      <td>0.346581</td>\n",
       "      <td>Multimodal CoT</td>\n",
       "      <td>N</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>ScienceQA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm</td>\n",
       "      <td>0.323613</td>\n",
       "      <td>Metaprompt</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale Language Model Society</td>\n",
       "      <td>0.311338</td>\n",
       "      <td>Role-Playing</td>\n",
       "      <td>Y</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Chain-of-Verification Reduces Hallucination in Large Language Models</td>\n",
       "      <td>0.307082</td>\n",
       "      <td>Chain-of-Verification</td>\n",
       "      <td>Y</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20-30%</td>\n",
       "      <td>Wikidata, QUEST, MultiSpanQA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Complexity-Based Prompting for Multi-Step Reasoning</td>\n",
       "      <td>0.301640</td>\n",
       "      <td>Complexity-Based Prompting</td>\n",
       "      <td>Y</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5-20%</td>\n",
       "      <td>GSM8K, MultiArith, MathQA, Date Understanding, Penguins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Decomposed Prompting: A Modular Approach for Solving Complex Tasks</td>\n",
       "      <td>0.271850</td>\n",
       "      <td>Decomposed Prompting</td>\n",
       "      <td>Y</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5-10%</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models</td>\n",
       "      <td>0.271283</td>\n",
       "      <td>Plan-and-Solve Prompting</td>\n",
       "      <td>Y</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5-5%</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Large Language Models Can Be Easily Distracted by Irrelevant Context</td>\n",
       "      <td>0.264586</td>\n",
       "      <td>Instruction to Ignore Irrelevant Information</td>\n",
       "      <td>N</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Unclear</td>\n",
       "      <td>GSM-IC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers</td>\n",
       "      <td>0.212966</td>\n",
       "      <td>EvoPrompt</td>\n",
       "      <td>Y</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15-25%</td>\n",
       "      <td>SAMSum, ASSET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts</td>\n",
       "      <td>0.195040</td>\n",
       "      <td>Chaining</td>\n",
       "      <td>Maybe</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Peer review writing, flashcards</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Prompting GPT-3 To Be Reliable</td>\n",
       "      <td>0.194298</td>\n",
       "      <td>Prompting for Reliability</td>\n",
       "      <td>Y</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP</td>\n",
       "      <td>0.190913</td>\n",
       "      <td>Demonstrate-Search-Predict</td>\n",
       "      <td>Y</td>\n",
       "      <td>4.0</td>\n",
       "      <td>40-300% relative gains</td>\n",
       "      <td>Open-SQuAD, HotPotQA, QReCC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>ART: Automatic multi-step reasoning and tool-use for large language models</td>\n",
       "      <td>0.176819</td>\n",
       "      <td>Automatic Reasoning and Tool-Use</td>\n",
       "      <td>Y</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10-15%</td>\n",
       "      <td>GSM8K, AQUA-RATE, BigBench, MMLU, SQUAD, TriviaQA, SVAMP, MAWPS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Promptagator: Few-Shot Dense Retrieval From 8 Examples</td>\n",
       "      <td>0.167273</td>\n",
       "      <td>Few-Shot Dense Retrieval</td>\n",
       "      <td>N</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>Fever, SciFact, SciDocs, HotpotQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations</td>\n",
       "      <td>0.166484</td>\n",
       "      <td>Maieutic Prompting</td>\n",
       "      <td>Y</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Com2Sense, CommonSenseQA, CREAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Reframing Instructional Prompts to GPTk's Language</td>\n",
       "      <td>0.157847</td>\n",
       "      <td>Reframing</td>\n",
       "      <td>Y</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5-15%</td>\n",
       "      <td>MC-TACO, QASC, Quoref, WinoGrande, CosmosQA, MultiRC, Essential-Terms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Generated Knowledge Prompting for Commonsense Reasoning</td>\n",
       "      <td>0.154563</td>\n",
       "      <td>Generated Knowledge Prompting</td>\n",
       "      <td>Maybe</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10-32%</td>\n",
       "      <td>NumerSense, CommonSenseQA, CommonSenseQA2, QASC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Teaching Algorithmic Reasoning via In-context Learning</td>\n",
       "      <td>0.152241</td>\n",
       "      <td>Algorithmic Prompting</td>\n",
       "      <td>Maybe</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>GSM8K, GSM8K-Hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery</td>\n",
       "      <td>0.147536</td>\n",
       "      <td>Gradient-Based Prompt Optimization</td>\n",
       "      <td>Y</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0-2%</td>\n",
       "      <td>SST, Amazon, AGNEWS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              paper title  \\\n",
       "0                                                                   Language Models are Few-Shot Learners   \n",
       "3                                   Chain of Thought Prompting Elicits Reasoning in Large Language Models   \n",
       "6                                                           Large Language Models are Zero-Shot Reasoners   \n",
       "8                                 Tree of Thoughts: Deliberate Problem Solving with Large Language Models   \n",
       "14                                                   Self-Refine: Iterative Refinement with Self-Feedback   \n",
       "15                                             ReAct: Synergizing Reasoning and Acting in Language Models   \n",
       "18                             Least-to-most prompting enables complex reasoning in large language models   \n",
       "22                                                                     PAL: Program-aided Language Models   \n",
       "23                                                 Large Language Models Are Human-Level Prompt Engineers   \n",
       "24                                                             How Can We Know What Language Models Know?   \n",
       "26                                          Automatic Chain of Thought Prompting in Large Language Models   \n",
       "27                          Show Your Work: Scratchpads for Intermediate Computation with Language Models   \n",
       "32                                               Multimodal Chain-of-Thought Reasoning in Language Models   \n",
       "33                             Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm   \n",
       "34               CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale Language Model Society   \n",
       "35                                   Chain-of-Verification Reduces Hallucination in Large Language Models   \n",
       "37                                                    Complexity-Based Prompting for Multi-Step Reasoning   \n",
       "40                                     Decomposed Prompting: A Modular Approach for Solving Complex Tasks   \n",
       "41      Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models   \n",
       "42                                   Large Language Models Can Be Easily Distracted by Irrelevant Context   \n",
       "44        Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers   \n",
       "48  AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts   \n",
       "49                                                                         Prompting GPT-3 To Be Reliable   \n",
       "50        Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP   \n",
       "52                             ART: Automatic multi-step reasoning and tool-use for large language models   \n",
       "55                                                 Promptagator: Few-Shot Dense Retrieval From 8 Examples   \n",
       "56                         Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations   \n",
       "58                                                     Reframing Instructional Prompts to GPTk's Language   \n",
       "59                                                Generated Knowledge Prompting for Commonsense Reasoning   \n",
       "60                                                 Teaching Algorithmic Reasoning via In-context Learning   \n",
       "62           Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery   \n",
       "\n",
       "    citations_per_day                     Prompt Engineering Method  \\\n",
       "0           13.230700                             Few-Shot Learning   \n",
       "3            3.327723                    Chain-of-Thought Prompting   \n",
       "6            1.707434                    Zero-Shot Chain-of-Thought   \n",
       "8            1.431593                               Tree-of-Thought   \n",
       "14           0.973061                                   Self-Refine   \n",
       "15           0.870102                                         ReAct   \n",
       "18           0.710210                       Least-to-Most Prompting   \n",
       "22           0.584822                 Program Aided Language Models   \n",
       "23           0.548697                     Automatic Prompt Engineer   \n",
       "24           0.535602            Prompt Mining, Prompt Paraphrasing   \n",
       "26           0.528163          Automatic Chain of Thought Prompting   \n",
       "27           0.455489                                   Scratchpads   \n",
       "32           0.346581                                Multimodal CoT   \n",
       "33           0.323613                                    Metaprompt   \n",
       "34           0.311338                                  Role-Playing   \n",
       "35           0.307082                         Chain-of-Verification   \n",
       "37           0.301640                    Complexity-Based Prompting   \n",
       "40           0.271850                          Decomposed Prompting   \n",
       "41           0.271283                      Plan-and-Solve Prompting   \n",
       "42           0.264586  Instruction to Ignore Irrelevant Information   \n",
       "44           0.212966                                     EvoPrompt   \n",
       "48           0.195040                                      Chaining   \n",
       "49           0.194298                     Prompting for Reliability   \n",
       "50           0.190913                    Demonstrate-Search-Predict   \n",
       "52           0.176819              Automatic Reasoning and Tool-Use   \n",
       "55           0.167273                      Few-Shot Dense Retrieval   \n",
       "56           0.166484                            Maieutic Prompting   \n",
       "58           0.157847                                     Reframing   \n",
       "59           0.154563                 Generated Knowledge Prompting   \n",
       "60           0.152241                         Algorithmic Prompting   \n",
       "62           0.147536            Gradient-Based Prompt Optimization   \n",
       "\n",
       "   Generalizable?  Ease of Implementation? (1=easy, 5=hard)  \\\n",
       "0               Y                                       4.0   \n",
       "3               Y                                       3.0   \n",
       "6               Y                                       1.0   \n",
       "8               Y                                       3.0   \n",
       "14              Y                                       3.0   \n",
       "15              Y                                       4.0   \n",
       "18              Y                                       2.0   \n",
       "22              N                                       4.0   \n",
       "23              Y                                       4.0   \n",
       "24              N                                       4.0   \n",
       "26              Y                                       4.0   \n",
       "27              Y                                       4.0   \n",
       "32              N                                       4.0   \n",
       "33              Y                                       NaN   \n",
       "34              Y                                       4.0   \n",
       "35              Y                                       3.0   \n",
       "37              Y                                       3.0   \n",
       "40              Y                                       4.0   \n",
       "41              Y                                       2.0   \n",
       "42              N                                       2.0   \n",
       "44              Y                                       4.0   \n",
       "48          Maybe                                       4.0   \n",
       "49              Y                                       4.0   \n",
       "50              Y                                       4.0   \n",
       "52              Y                                       4.0   \n",
       "55              N                                       4.0   \n",
       "56              Y                                       4.0   \n",
       "58              Y                                       4.0   \n",
       "59          Maybe                                       3.0   \n",
       "60          Maybe                                       4.0   \n",
       "62              Y                                       4.0   \n",
       "\n",
       "   Performance Gain - Rough Order of Magnitude  \\\n",
       "0                                          0.1   \n",
       "3                                          0.4   \n",
       "6                                          0.3   \n",
       "8                                          0.7   \n",
       "14                                         0.2   \n",
       "15                                      10-34%   \n",
       "18                                       0-15%   \n",
       "22                                        0.15   \n",
       "23                                         0.1   \n",
       "24                                         0.1   \n",
       "26                                       5-10%   \n",
       "27                                      10-15%   \n",
       "32                                        0.16   \n",
       "33                                         NaN   \n",
       "34                                         NaN   \n",
       "35                                      20-30%   \n",
       "37                                       5-20%   \n",
       "40                                       5-10%   \n",
       "41                                      2.5-5%   \n",
       "42                                     Unclear   \n",
       "44                                      15-25%   \n",
       "48                                         NaN   \n",
       "49                                         NaN   \n",
       "50                      40-300% relative gains   \n",
       "52                                      10-15%   \n",
       "55                                        0.05   \n",
       "56                                         0.2   \n",
       "58                                       5-15%   \n",
       "59                                      10-32%   \n",
       "60                                         0.4   \n",
       "62                                        0-2%   \n",
       "\n",
       "                                                                                 Datasets  \n",
       "0                                                                                 Various  \n",
       "3                                                             GSM8K, Last Letter, Various  \n",
       "6                                       GSM8K, Last Letter, Coin Flip, Date Understanding  \n",
       "8                                           Game of 24, Creative Writing, Mini Crosswords  \n",
       "14                                                         GSM8K, PIE, CommonGen, CodeNet  \n",
       "15                                                     HotpotQA, Fever, ALFWorld, WebShop  \n",
       "18                                                         Last Letter, SCAN, GSM8K, DROP  \n",
       "22                                                                                    NaN  \n",
       "23                                 Big-Bench Instruction Induction (BBII), Big-Bench Hard  \n",
       "24                                                                                   LAMA  \n",
       "26  MultiArith, GSM8K, CommonSenseQA, SVAMP, AQUA-RAT, StrategyQA, Last Letter, Coin Flip  \n",
       "27                               Long addition, division, execution of arbitrary programs  \n",
       "32                                                                              ScienceQA  \n",
       "33                                                                                    NaN  \n",
       "34                                                                                    NaN  \n",
       "35                                                           Wikidata, QUEST, MultiSpanQA  \n",
       "37                                GSM8K, MultiArith, MathQA, Date Understanding, Penguins  \n",
       "40                                                                                    NaN  \n",
       "41                                                                                    NaN  \n",
       "42                                                                                 GSM-IC  \n",
       "44                                                                          SAMSum, ASSET  \n",
       "48                                                        Peer review writing, flashcards  \n",
       "49                                                                                    NaN  \n",
       "50                                                            Open-SQuAD, HotPotQA, QReCC  \n",
       "52                        GSM8K, AQUA-RATE, BigBench, MMLU, SQUAD, TriviaQA, SVAMP, MAWPS  \n",
       "55                                                       Fever, SciFact, SciDocs, HotpotQ  \n",
       "56                                                        Com2Sense, CommonSenseQA, CREAK  \n",
       "58                  MC-TACO, QASC, Quoref, WinoGrande, CosmosQA, MultiRC, Essential-Terms  \n",
       "59                                        NumerSense, CommonSenseQA, CommonSenseQA2, QASC  \n",
       "60                                                                      GSM8K, GSM8K-Hard  \n",
       "62                                                                    SST, Amazon, AGNEWS  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Items with non-missing column \"Prompt Engineeering Method\" and \"Ease of Implementation? (1=easy, 5=hard)\" not 5\n",
    "limitations = df[(df[\"Prompt Engineering Method\"].notnull()) & (df[\"Ease of Implementation? (1=easy, 5=hard)\"] != 5)]\n",
    "\n",
    "# Drop irrelevant columns\n",
    "limitations = limitations.drop(df.columns[1:9], axis=1)\n",
    "\n",
    "# Drop \"Notes\" column\n",
    "limitations = limitations.drop(\"Notes\", axis=1)\n",
    "\n",
    "# Print full width\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "limitations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of rows in table\n",
    "len(limitations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GSM8K            8\n",
       "Last Letter      4\n",
       "CommonSenseQA    3\n",
       "SVAMP            2\n",
       "QASC             2\n",
       "                ..\n",
       "QUEST            1\n",
       "MultiSpanQA      1\n",
       "Penguins         1\n",
       "GSM-IC           1\n",
       "AGNEWS           1\n",
       "Name: count, Length: 67, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all values of column \"Datasets\"\n",
    "\n",
    "datasets = limitations[\"Datasets\"].unique()\n",
    "\n",
    "# Convert array to list\n",
    "datasets = datasets.tolist()\n",
    "\n",
    "# Split list items by comma\n",
    "ds_list = []\n",
    "for ds in datasets:\n",
    "    ds_list.append(str(ds).split(\", \"))\n",
    "\n",
    "# Flatten list of lists\n",
    "ds_list = [item for sublist in ds_list for item in sublist]\n",
    "\n",
    "# Frequency of list items\n",
    "ds_list = pd.Series(ds_list).value_counts()\n",
    "\n",
    "ds_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output LaTeX table of top general methods\n",
    "\n",
    "# Keep columns \"paper title\", \"Prompt Engineering Method\", \"citations_per_day\"\n",
    "latex_table = limitations[[\"paper title\", \"Prompt Engineering Method\", \"citations_per_day\"]]\n",
    "\n",
    "# Rename columns\n",
    "latex_table.columns = [\"Paper Title\", \"Prompt Engineering Method\", \"Citations per Day\"]\n",
    "\n",
    "# Sort by citations per day\n",
    "latex_table = latex_table.sort_values(by=\"Citations per Day\", ascending=False)\n",
    "\n",
    "# Round citations per day to 2 decimal places\n",
    "latex_table[\"Citations per Day\"] = latex_table[\"Citations per Day\"].round(2)\n",
    "\n",
    "# Remove zeroes from the end of citations per day\n",
    "latex_table[\"Citations per Day\"] = latex_table[\"Citations per Day\"].astype(str).str.rstrip(\"0\").str.rstrip(\".\")\n",
    "\n",
    "# Rename columns\n",
    "latex_table.columns = [\"Paper Title\", \"Prompt Engineering Method\", \"Citations Per Day Since Release\"]\n",
    "\n",
    "# Output LaTeX table to file\n",
    "latex_table.to_latex(\"selected_methods_popularity.tex\", index=False, column_format='llc')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
