{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated_Grading.ipynb\n",
    "\n",
    "Automatically grade LLM responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample or full run\n",
    "run_type = 'sample'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Load functions\n",
    "from Grading_Functions import *\n",
    "\n",
    "# Load conversations\n",
    "with open('Conversation_Lists.pkl', 'rb') as file:\n",
    "    loaded_data = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign model_task_methods and conversation numbers and put conversations in a list\n",
    "model_task_methods = []\n",
    "conversation_numbers = []\n",
    "flat_conversations_list = []\n",
    "for model_task_method, conversations in loaded_data.items():\n",
    "    for i, conversation in enumerate(conversations):\n",
    "        model_task_methods.append(model_task_method)\n",
    "        conversation_numbers.append(i+1)\n",
    "        flat_conversations_list.append(conversation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "td3_gsm8k_self_refine_responses\n",
      "gpt4_cw_least_to_most_responses\n",
      "td3_gsm8k_least_to_most_responses\n",
      "td3_cw_manual_cot_responses\n",
      "td3_gsm8k_tree_of_thought_responses\n",
      "gpt4_gsm8k_zero_shot_cot_responses\n",
      "td3_cw_tree_of_thought_responses\n",
      "td3_gsm8k_zero_shot_cot_responses\n",
      "td3_cw_self_refine_responses\n",
      "gpt4_gsm8k_ape_zero_shot_cot_responses\n",
      "gpt4_cw_manual_few_shot_responses\n",
      "gpt4_gsm8k_manual_cot_responses\n",
      "gpt4_cw_ape_zero_shot_cot_responses\n",
      "gpt4_gsm8k_least_to_most_responses\n",
      "gpt4_gsm8k_tree_of_thought_responses\n",
      "td3_cw_least_to_most_responses\n",
      "gpt4_cw_tree_of_thought_responses\n",
      "td3_gsm8k_direct_prompting_responses\n",
      "td3_gsm8k_manual_few_shot_responses\n",
      "gpt4_gsm8k_manual_few_shot_responses\n",
      "td3_gsm8k_manual_cot_responses\n",
      "gpt4_cw_zero_shot_cot_responses\n",
      "gpt4_gsm8k_direct_prompting_responses\n",
      "td3_cw_direct_prompting_responses\n",
      "td3_cw_zero_shot_cot_responses\n",
      "td3_gsm8k_ape_zero_shot_cot_responses\n",
      "td3_cw_ape_zero_shot_cot_responses\n",
      "td3_cw_manual_few_shot_responses\n",
      "gpt4_cw_manual_cot_responses\n",
      "gpt4_gsm8k_self_refine_responses\n",
      "gpt4_cw_self_refine_responses\n",
      "gpt4_cw_direct_prompting_responses\n"
     ]
    }
   ],
   "source": [
    "# Print element of set(model_task_methods) to see what model_task_methods are available\n",
    "for item in set(model_task_methods):\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limit to desired model task method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_mtm = 'td3_cw_direct_prompting_responses'\n",
    "conversation_numbers = [cn for cn, mtm in zip(conversation_numbers, model_task_methods) if mtm == desired_mtm]\n",
    "flat_conversations_list = [c for c, mtm in zip(flat_conversations_list, model_task_methods) if mtm == desired_mtm]\n",
    "model_task_methods = [mtm for mtm in model_task_methods if mtm == desired_mtm]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Task: Write a coherent passage of 2 short paragraphs. The end sentence of each paragraph must be: 1. It isn't difficult to do a handstand if you just stand on your hands. 2. It caught him off guard that space smelled of seared steak.\\nResponse: \", \"Curious about activities that test gravity and balance, Bob decided to try a handstand. It wasn't as difficult as he expected; in fact, once he got used to the 'falling' sensation, it became quite enjoyable. All he needed to do was to focus on the controlled falling and stand on his hands. It isn't difficult to do a handstand if you just stand on your hands.\\n\\nBob then moved on to his next challenge – a zero-gravity environment. On the first day of the space mission, he discovered something interesting. The air was not tacky like he expected; instead, it had a distinct, and unique smoky aroma. It caught him off guard that space smelled of seared steak.\", 'Your Task: Provide feedback on the correctness and coherence of the response and a rating on a scale of 1-10. If it is already coherent and correct to the extent you would award a 10, output 10 and the word STOP.\\nResponse: ', '9 STOP']\n",
      "Curious about activities that test gravity and balance, Bob decided to try a handstand. It wasn't as difficult as he expected; in fact, once he got used to the 'falling' sensation, it became quite enjoyable. All he needed to do was to focus on the controlled falling and stand on his hands. It isn't difficult to do a handstand if you just stand on your hands.\n",
      "\n",
      "Bob then moved on to his next challenge – a zero-gravity environment. On the first day of the space mission, he discovered something interesting. The air was not tacky like he expected; instead, it had a distinct, and unique smoky aroma. It caught him off guard that space smelled of seared steak.\n"
     ]
    }
   ],
   "source": [
    "print(flat_conversations_list[0])\n",
    "print(flat_conversations_list[0][-3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit based on run type\n",
    "if run_type == 'sample':\n",
    "    conversation_numbers = conversation_numbers[:2]\n",
    "    flat_conversations_list = flat_conversations_list[:2]\n",
    "    model_task_methods = model_task_methods[:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creative Writing Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_of_item_to_grade = -1\n",
    "if 'self_refine' in desired_mtm or 'tree_of_thought' in desired_mtm:\n",
    "    index_of_item_to_grade = -3\n",
    "\n",
    "# Grade creative writing responses\n",
    "creative_writing_scores = []\n",
    "creative_writing_comments = []\n",
    "for idx, mtm in enumerate(model_task_methods):\n",
    "    # Check for \"_cw_\" in model_task_method\n",
    "    # If it's in the mtm, then grade the creative writing response and add the score to the list\n",
    "    # If not, add NA to the list\n",
    "    if \"_cw_\" in mtm:\n",
    "        rating, comments = grade_creative_writing_coherence_message(flat_conversations_list[idx][index_of_item_to_grade])\n",
    "        creative_writing_scores.append(rating)      \n",
    "        creative_writing_comments.append(comments)\n",
    "    else:\n",
    "        creative_writing_scores.append(\"NA\")\n",
    "        creative_writing_comments.append(\"NA\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model_task_method, conversation_numbers, creative_writing_scores, and ease_of_evaluation_scores to Excel\n",
    "# First put them in a pandas dataframe\n",
    "\n",
    "automated_grading_df = pd.DataFrame(list(zip(model_task_methods, conversation_numbers, creative_writing_scores, creative_writing_comments)),\n",
    "                                    columns = ['model_task_method', 'conversation_number', 'creative_writing_score', 'creative_writing_comments'])\n",
    "\n",
    "automated_grading_df.to_excel(\"Automated_Grading - CW Coherence - \" + desired_mtm + \".xlsx\", index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
