{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paired t-test for quality, means of other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load direct_prompting_comparison.xlsx\n",
    "direct_prompting_comparison = pd.read_excel('direct_prompting_comparison.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform paired t-test on creative writing data and all other metrics\n",
    "\n",
    "# Metrics to t-test: 'creative_writing_score', 'ease_of_evaluation_score', 'conversation_length', 'input_length', 'output_length', 'conversation_cost', 'gsm8k_length_vs_provided', 'length_vs_direct_prompting', 'num_linebreaks', 'num_sentences', 'num_step_i', 'num_1_dot_etc', 'sentence_length', 'fres', 'num_linebreaks_prompts', 'num_sentences_prompts', 'num_step_i_prompts', 'num_1_dot_etc_prompts', 'sentence_length_prompts', 'fres_prompts'\n",
    "metrics_to_t_test = ['coherence_1_incoherent_10_very_coherent', 'ease_of_review_1_easy_10_hard', 'conversation_length', 'input_length', 'output_length', 'conversation_cost', 'gsm8k_length_vs_provided', 'length_vs_direct_prompting', 'num_linebreaks', 'num_sentences', 'num_step_i', 'num_1_dot_etc', 'sentence_length', 'fres', 'num_linebreaks_prompts', 'num_sentences_prompts', 'num_step_i_prompts', 'num_1_dot_etc_prompts', 'sentence_length_prompts', 'fres_prompts']\n",
    "\n",
    "# Create table to hold results\n",
    "# Columns of model, task, method, mean metric, mean dp_metric, statistic, pvalue\n",
    "t_test_results = pd.DataFrame(columns=['model', 'task', 'method', 'metric', 'dp_mean', 'using_method_mean', 'statistic', 'pvalue'])\n",
    "# Iterate over model, task, method\n",
    "for model, task, method in direct_prompting_comparison[['model', 'task', 'method']].values:\n",
    "    # Iterate over metrics_to_t_test\n",
    "    for metric in metrics_to_t_test:\n",
    "        # Get data\n",
    "        # Sort by conversation_number\n",
    "        data = direct_prompting_comparison[(direct_prompting_comparison['model'] == model) & (direct_prompting_comparison['task'] == task) & (direct_prompting_comparison['method'] == method)].sort_values(by=['conversation_number'])\n",
    "        # direct_prompting holds the metric when using direct_prompting\n",
    "        direct_prompting = data['dp_' + metric]\n",
    "        # using_method holds the metric when using the method\n",
    "        using_method = data[metric]\n",
    "        # Perform the paired sample t-test \n",
    "        statistic, pvalue = stats.ttest_rel(direct_prompting, using_method)\n",
    "        # Add to table\n",
    "        t_test_results = pd.concat([t_test_results, pd.DataFrame([[model, task, method, metric, direct_prompting.mean(), using_method.mean(), statistic, pvalue]], columns=['model', 'task', 'method', 'metric', 'dp_mean', 'using_method_mean', 'statistic', 'pvalue'])])\n",
    "\n",
    "t_test_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows that are not significant at p < 0.05\n",
    "t_test_results[t_test_results['pvalue'] >= 0.05]\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
