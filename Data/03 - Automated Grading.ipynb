{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated_Grading.ipynb\n",
    "\n",
    "Automatically grade LLM responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Load functions\n",
    "from Grading_Functions import *\n",
    "\n",
    "# Load conversations\n",
    "with open('Conversation_Lists.pkl', 'rb') as file:\n",
    "    loaded_data = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign model_task_methods and conversation numbers and put conversations in a list\n",
    "model_task_methods = []\n",
    "conversation_numbers = []\n",
    "flat_conversations_list = []\n",
    "for model_task_method, conversations in loaded_data.items():\n",
    "    for i, conversation in enumerate(conversations):\n",
    "        model_task_methods.append(model_task_method)\n",
    "        conversation_numbers.append(i+1)\n",
    "        flat_conversations_list.append(conversation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creative Writing Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade creative writing responses\n",
    "creative_writing_scores = []\n",
    "for idx, mtm in enumerate(model_task_methods):\n",
    "    # Check for \"_cw_\" in model_task_method\n",
    "    # If it's in the mtm, then grade the creative writing response and add the score to the list\n",
    "    # If not, add NA to the list\n",
    "    if \"_cw_\" in mtm:\n",
    "        creative_writing_scores.append(grade_creative_writing_coherence(flat_conversations_list[idx]))      \n",
    "    else:\n",
    "        creative_writing_scores.append(\"NA\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ease of Evaluation (GSM8K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ease_of_evaluation\n",
    "ease_of_evaluation_scores = []\n",
    "for idx, mtm in enumerate(model_task_methods):\n",
    "    # Check for \"gsm8k\" in model_task_method\n",
    "    # If it's in the mtm, then get ease of evaluation and add to the list\n",
    "    # If not, add NA to the list\n",
    "    if \"gsm8k\" in mtm:\n",
    "        ease_of_evaluation_scores.append(grade_ease_of_evaluation(flat_conversations_list[idx]))      \n",
    "    else:\n",
    "        ease_of_evaluation_scores.append(\"NA\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model_task_method, conversation_numbers, creative_writing_scores, and ease_of_evaluation_scores to Excel\n",
    "# First put them in a pandas dataframe\n",
    "\n",
    "automated_grading_df = pd.DataFrame(list(zip(model_task_methods, conversation_numbers, creative_writing_scores, ease_of_evaluation_scores)),\n",
    "                                    columns = ['model_task_method', 'conversation_number', 'creative_writing_score', 'ease_of_evaluation_score'])\n",
    "\n",
    "automated_grading_df.to_excel(\"Automated_Grading.xlsx\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
