{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6u5glSljDB3"
      },
      "source": [
        "# Sentence-BERT Inter-Paragraph Coherence\n",
        "\n",
        "Compute inter-paragraph coherence scores for cw responses using Sentence-BERT embeddings and average cosine similarity.\n",
        "\n",
        "Run on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UG-dDlp_jDB6",
        "outputId": "18078c5b-3797-4f0b-a6cd-fb9d8043a4fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# sentence-transformers package setup\n",
        "!pip install sentence-transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ov9KMAS2jDB7",
        "outputId": "f9417502-7207-4c70-ee49-7e4b4a45dda8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Google Drive setup\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "filename_to_load = '/content/drive/My Drive/ANLP23_Project_Data/actual_cw_passage_content.xlsx'\n",
        "filename_to_save = '/content/drive/My Drive/ANLP23_Project_Data/actual_cw_passage_content_with_inter_paragraph_sim.xlsx'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vryFuYuFjDB7",
        "outputId": "a76c6355-a930-4c08-f44c-4ddb4e9362b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                     model_task_method  conversation_number  \\\n",
            "0  gpt4_cw_ape_zero_shot_cot_responses                    1   \n",
            "1  gpt4_cw_ape_zero_shot_cot_responses                    2   \n",
            "2  gpt4_cw_ape_zero_shot_cot_responses                    3   \n",
            "3  gpt4_cw_ape_zero_shot_cot_responses                    4   \n",
            "4  gpt4_cw_ape_zero_shot_cot_responses                    5   \n",
            "\n",
            "                                            response  \n",
            "0  Joe was an astronaut with a rather unusual hob...  \n",
            "1  There lived a hawk on the hill, tall and proud...  \n",
            "2  Being an outdoors enthusiast, one might consid...  \n",
            "3  Edward was a corporate executive, laden with p...  \n",
            "4  Joe was a curious and inventive character, alw...  \n"
          ]
        }
      ],
      "source": [
        "# Get passage content\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read the Excel file\n",
        "df = pd.read_excel(filename_to_load)\n",
        "\n",
        "# Display the first few rows of the dataframe\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHMguxsXjDB7",
        "outputId": "4a961863-305e-4435-8b45-73a18c6b5940"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Packages\n",
        "from math import sqrt\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "a3qiGeh2jDB8"
      },
      "outputs": [],
      "source": [
        "# Load model\n",
        "sentence_model = SentenceTransformer('sentence-transformers/all-distilroberta-v1')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdmQ190ujDB8",
        "outputId": "894b5a6b-f13e-4dbb-fd74-2e531c6bda84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(768,)\n"
          ]
        }
      ],
      "source": [
        "# Testing embedding creation\n",
        "embedding=sentence_model.encode(\"this is a sentence\")\n",
        "print(embedding.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "i7brN__bjDB8"
      },
      "outputs": [],
      "source": [
        "# Cosine similarity between embeddings\n",
        "def cosine(one, two):\n",
        "  return np.dot(one,two)/(sqrt(np.dot(one,one)) * sqrt(np.dot(two,two)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Trying to split responses into paragraphs\n",
        "\n",
        "# Split on newlines\n",
        "paragraph_lists = [response.split('\\n') for response in df['response']]\n",
        "\n",
        "# Print 100 random elements of paragraph_lists\n",
        "import random\n",
        "random.seed(42)\n",
        "print(random.sample(paragraph_lists, 100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to get average cosine similarity between paragraph embeddings - averages of sentence embeddings\n",
        "def get_passage_interparagraph_cosine_sim(paragraph_list):\n",
        "    # List of paragraph embeddings\n",
        "    paragraph_embeddings = []\n",
        "    # List of paragraph number of sentences\n",
        "    paragraph_num_sentences = []\n",
        "    # Loop through paragraphs\n",
        "    for paragraph in paragraph_list:\n",
        "        # Split into sentences\n",
        "        sentences = nltk.sent_tokenize(paragraph)\n",
        "        # Get sentence embeddings\n",
        "        sentence_embeddings = [sentence_model.encode(sentence) for sentence in sentences]\n",
        "        # Average sentence embeddings\n",
        "        paragraph_embeddings.append(np.mean(sentence_embeddings, axis=0))\n",
        "        # Saving number of sentences there were embeddings for\n",
        "        paragraph_num_sentences.append(len(sentence_embeddings))\n",
        "    # Number of paragraphs\n",
        "    num_paragraphs = len(paragraph_embeddings)\n",
        "    # Get cosine similarity between paragraph and the one after it\n",
        "    cosine_sim = []\n",
        "    if num_paragraphs > 1:\n",
        "        for i in range(len(paragraph_embeddings)-1):\n",
        "            cosine_sim.append(cosine(paragraph_embeddings[i], paragraph_embeddings[i+1]))\n",
        "        avg_cosine_sim = np.mean(cosine_sim)\n",
        "    else:\n",
        "        avg_cosine_sim = 1\n",
        "    # Return average cosine similarity\n",
        "    return avg_cosine_sim, num_paragraphs, paragraph_num_sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOAHN4Ar1afj",
        "outputId": "19b43c00-a7a3-4aa6-e48a-d11c90a03b46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test 1\n",
            "0.4240155942444682\n",
            "2\n",
            "test 2\n",
            "0.07097828921740414\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "# Test function\n",
        "test_1_avg, test_1_num_para, test_1_num_sent = get_passage_interparagraph_cosine_sim([\"My life has been filled with hardships, but I continue to power forward. I'm a firm believer that every difficulty I face can be used to strengthen my character. I'm not the same person I was at the start, and for that I'm thankful. I'm a living furnace, forging my character so that I may become a stronger, more successful person.\", \"In some ways, life is like a rose - beautiful, but with thorns. The thorns represent those moments of hardship, challenge, and heartache we face. Yet despite the difficulties we might endure, there is still beauty to be found in life. There's a reason that roses have thorns, despite their beauty. They provide the strength and courage to help us overcome any obstacle we may face.\"])\n",
        "print('test 1')\n",
        "print(test_1_avg)\n",
        "print(test_1_num_para)\n",
        "print(test_1_num_sent)\n",
        "\n",
        "test_2_avg, test_2_num_para, test_2_num_sent = get_passage_interparagraph_cosine_sim(['The boat sailed out of the harbor. Paul joined the other construction workers on the job site.'])\n",
        "print('test 2')\n",
        "print(test_2_avg)\n",
        "print(test_2_num_para)\n",
        "print(test_2_num_sent)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3y1XbI09qQk",
        "outputId": "1a260cdb-62a4-4740-ad40-f89fb57817e9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 27%|██▋       | 436/1600 [07:33<15:51,  1.22it/s]"
          ]
        }
      ],
      "source": [
        "# Create lists of average_cosine_sim, passage_num_paragraphs, passage_num_sentences\n",
        "average_cosine_sims_responses = []\n",
        "passage_num_paragraphs_responses = []\n",
        "passage_num_sentences_responses = []\n",
        "for plist in tqdm(paragraph_lists):\n",
        "  avg, passage_num_paragraphs, passage_num_sentences = get_passage_interparagraph_cosine_sim(plist)\n",
        "  average_cosine_sims_responses.append(avg)\n",
        "  passage_num_paragraphs_responses.append(passage_num_paragraphs)\n",
        "  passage_num_sentences_responses.append(passage_num_sentences)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2GKfjCojDB9"
      },
      "outputs": [],
      "source": [
        "# Create column for average cosine similarity\n",
        "#df[['avg_cosine_sim', 'num_sentences']] = df.progress_apply(lambda row: get_passage_avg_cosine_sim(row['response']), axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEdBEALu-KHR"
      },
      "outputs": [],
      "source": [
        "# Create dataframe columns\n",
        "df['avg_inter_paragraph_cosine_sim'] = average_cosine_sims_responses\n",
        "df['num_paragraphs'] = passage_num_paragraphs_responses\n",
        "df['num_sentences'] = passage_num_sentences_responses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For avg_inter_paragraph_cosine_sim, replace 1 with NaN\n",
        "df['avg_inter_paragraph_cosine_sim'] = df['avg_inter_paragraph_cosine_sim'].replace(1, np.nan)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iJwbGcWjDB9"
      },
      "outputs": [],
      "source": [
        "# Write df to Excel\n",
        "df.to_excel(filename_to_save)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
