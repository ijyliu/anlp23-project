{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Metrics.ipynb\n",
    "\n",
    "Evaluate conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import Evaluation_Functions\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import textstat\n",
    "\n",
    "# Load conversations\n",
    "with open('Conversation_Lists.pkl', 'rb') as file:\n",
    "    loaded_data = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign model_task_methods and conversation numbers and put conversations in a list\n",
    "model_task_methods = []\n",
    "conversation_numbers = []\n",
    "flat_conversations_list = []\n",
    "for model_task_method, conversations in loaded_data.items():\n",
    "    for i, conversation in enumerate(conversations):\n",
    "        model_task_methods.append(model_task_method)\n",
    "        conversation_numbers.append(i+1)\n",
    "        flat_conversations_list.append(conversation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation elements by whether they are user or system\n",
    "# user_or_system_lists = []\n",
    "# for conversation in flat_conversations_list:\n",
    "#     # Even indices are user, odd are system\n",
    "#     us_list = []\n",
    "#     for index in range(len(conversation)):\n",
    "#         if index % 2 == 0:\n",
    "#             us_list.append('user')\n",
    "#         else:\n",
    "#             us_list.append('system')\n",
    "#     user_or_system_lists.append(us_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get versions of conversations with GPT-4 chat metadata removed\n",
    "flat_conversations_no_metadata = []\n",
    "for index, conversation in enumerate(flat_conversations_list):\n",
    "    # Remove metadata if it's a gpt-4 chat\n",
    "    if 'gpt-4' in model_task_methods[index]:\n",
    "        flat_conversations_no_metadata.append(strip_gpt4_meta(conversation))\n",
    "    else:\n",
    "        flat_conversations_no_metadata.append(conversation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create versions of conversations that are one concatenated string without metadata\n",
    "conversation_strings = [\"\\n\".join(conversation) for conversation in flat_conversations_no_metadata]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create versions of conversations that are one concatenated string without metadata\n",
    "# Input data only - keep even indices of the conversation\n",
    "conversation_strings_input = [\"\\n\".join(conversation[::2]) for conversation in flat_conversations_no_metadata]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create versions of conversations that are one concatenated string without metadata\n",
    "# Output data only - keep odd indices of the conversation\n",
    "conversation_strings_output = [\"\\n\".join(conversation[1::2]) for conversation in flat_conversations_no_metadata]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create versions of conversations that are one concatenated string without metadata\n",
    "# Prompt only - first item in conversation\n",
    "conversation_strings_prompt = [conversation[0] for conversation in flat_conversations_no_metadata]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models\n",
    "model_list = []\n",
    "for model_task_method in model_task_methods:\n",
    "    # Split on underscores\n",
    "    if model_task_method.split('_')[0] == \"gpt4\":\n",
    "        model_list.append(\"gpt-4-0613\")\n",
    "    else:\n",
    "        model_list.append(\"text-davinci-003\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the entire conversation in tokens\n",
    "conversation_lengths = []\n",
    "for i, conversation_string in enumerate(conversation_strings):\n",
    "    conversation_lengths.append(get_length(conversation_strings, model_list[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lengths = []\n",
    "for i, conversation_string in enumerate(conversation_strings_input):\n",
    "    input_lengths.append(get_length(conversation_strings_input, model_list[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_lengths = []\n",
    "for i, conversation_string in enumerate(conversation_strings_output):\n",
    "    output_lengths.append(get_length(conversation_strings_output, model_list[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost\n",
    "\n",
    "text-davinci-003:\n",
    "\n",
    "2 cents per 1000 tokens\n",
    "\n",
    "GPT-4:\n",
    "\n",
    "Input: 3 cents per 1000 tokens\n",
    "\n",
    "Output 6 cents per 1000 tokens\n",
    "\n",
    "As of November 11, 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_costs = []\n",
    "for i in range(len(conversation_lengths)):\n",
    "    if model_list[i] == \"gpt-4-0613\":\n",
    "        conversation_costs.append(input_lengths[i] * 0.03 + output_lengths[i] * 0.06)\n",
    "    else:\n",
    "        conversation_costs.append(conversation_lengths[i] * 0.02)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Length Relative to Baseline\n",
    "\n",
    "Length of the entire interaction in tokens relative to the length of the task/question + a baseline answer. For GSM8K, the baselines are the provided (OpenAI) solution and the answer achieved via direct prompting. For the Creative Writing task, the baseline is the answer achieved via direct prompting. How much is prompt engineering stretching the interaction out? The ratio of engineered answer length to baseline lengths can be informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load questions and answers\n",
    "import json\n",
    "\n",
    "# Load the test question from test.jsonl\n",
    "with open('GSM8k/test.jsonl', 'r') as f:\n",
    "    test = f.readlines()\n",
    "\n",
    "questions = [json.loads(t)['question'] for t in test]\n",
    "answers = [json.loads(t)['answer'] for t in test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over conversations\n",
    "# For each conversation that has mtm containing gsm8k, \n",
    "# loop over questions until one is found that is contained in the conversation's first list element\n",
    "conversation_gsm8k_question_index = []\n",
    "for conversation_index, conversation in enumerate(flat_conversations_list):\n",
    "    # Grade gsm8k items\n",
    "    if 'gsm8k' in model_task_methods[conversation_index]:\n",
    "        for question_index, question in enumerate(questions):\n",
    "            if question in conversation[0]:\n",
    "                conversation_gsm8k_question_index.append(question_index)\n",
    "    # Add NAs for non-gsm8k items\n",
    "    else:\n",
    "        conversation_gsm8k_question_index.append('NA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare length with provided gsm8k question and answer\n",
    "gsm8k_length_vs_provided = []\n",
    "for idx, conversation_length in enumerate(conversation_lengths):\n",
    "    if conversation_gsm8k_question_index[idx] != \"NA\":\n",
    "        gsm8k_length_vs_provided.append(conversation_length / (get_length(questions[conversation_gsm8k_question_index[idx]] + \"\\n\" + answers[conversation_gsm8k_question_index[idx]], model_list[idx])))\n",
    "    else:\n",
    "        gsm8k_length_vs_provided.append('NA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list that repeats the sequence of direct prompting conversations for comparison with each conversation\n",
    "#td3_gsm8k_direct_prompting_responses = loaded_data['td3_gsm8k_direct_prompting_responses']\n",
    "#gpt4_gsm8k_direct_prompting_responses = loaded_data['gpt4_gsm8k_direct_prompting_responses']\n",
    "#td3_cw_direct_prompting_responses = loaded_data['td3_cw_direct_prompting_responses']\n",
    "#gpt4_cw_direct_prompting_responses = loaded_data['gpt4_cw_direct_prompting_responses']\n",
    "\n",
    "td3_gsm8k_direct_prompting_conversations = []\n",
    "gpt4_gsm8k_direct_prompting_conversations = []\n",
    "td3_cw_direct_prompting_conversations = []\n",
    "gpt4_cw_direct_prompting_conversations = []\n",
    "for idx, conversation in enumerate(flat_conversations_list):\n",
    "    if 'td3_gsm8k_direct_prompting_responses' in model_task_methods[idx]:\n",
    "        td3_gsm8k_direct_prompting_conversations.append(conversation)\n",
    "    if 'gpt4_gsm8k_direct_prompting_responses' in model_task_methods[idx]:\n",
    "        gpt4_gsm8k_direct_prompting_conversations.append(conversation)\n",
    "    if 'td3_cw_direct_prompting_responses' in model_task_methods[idx]:\n",
    "        td3_cw_direct_prompting_conversations.append(conversation)\n",
    "    if 'gpt4_cw_direct_prompting_responses' in model_task_methods[idx]:\n",
    "        gpt4_cw_direct_prompting_conversations.append(conversation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of model_task_methods as they appear\n",
    "# Drop duplicates\n",
    "model_task_methods_unique = []\n",
    "for model_task_method in model_task_methods:\n",
    "    if model_task_method not in model_task_methods_unique:\n",
    "        model_task_methods_unique.append(model_task_method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct big list of direct prompting conversations\n",
    "direct_prompting_conversations = []\n",
    "for mtm in model_task_methods_unique:\n",
    "    if 'td3_gsm8k' in mtm:\n",
    "        direct_prompting_conversations.extend(td3_gsm8k_direct_prompting_conversations)\n",
    "    elif 'gpt4_gsm8k' in mtm:\n",
    "        direct_prompting_conversations.extend(gpt4_gsm8k_direct_prompting_conversations)\n",
    "    elif 'td3_cw' in mtm:\n",
    "        direct_prompting_conversations.extend(td3_cw_direct_prompting_conversations)\n",
    "    elif 'gpt4_cw' in mtm:\n",
    "        direct_prompting_conversations.extend(gpt4_cw_direct_prompting_conversations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare length with direct prompting\n",
    "# Also save direct prompting lengths\n",
    "direct_prompting_lengths = []\n",
    "length_vs_direct_prompting = []\n",
    "for idx, conversation_length in enumerate(conversation_lengths):\n",
    "    direct_prompting_lengths.append(get_length(direct_prompting_conversations[idx], model_list[idx]))\n",
    "    length_vs_direct_prompting.append(conversation_length / (get_length(direct_prompting_conversations[idx], model_list[idx])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Reasoning Steps\n",
    "\n",
    "Number of reasoning steps - linebreaks, sentences (NLTK sentence tokenizer), strings \"step i\" and \"1. \", \"2. \", \"3. \", etc. in the response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_linebreaks = []\n",
    "num_sentences = []\n",
    "num_step_i = []\n",
    "num_1_dot_etc = []\n",
    "for output_string in conversation_strings_output:\n",
    "    num_linebreaks.append(output_string.count('\\n'))\n",
    "    num_sentences.append(len(nltk.sent_tokenize(output_string)))\n",
    "    # Convert the string to lowercase\n",
    "    text_lower = output_string.lower()\n",
    "    # Regular expression to find all occurrences of \"step\" followed by a number\n",
    "    matches = re.findall(r\"step \\d+\", text_lower)\n",
    "    # Count the number of occurrences\n",
    "    num_occurrences = len(matches)\n",
    "    num_step_i.append(num_occurrences)\n",
    "    # Regular expression to find all occurrences of a number followed by a period and a space\n",
    "    matches = re.findall(r\"\\d+\\.\", output_string)\n",
    "    # Count the number of occurrences\n",
    "    num_occurrences = len(matches)\n",
    "    num_1_dot_etc.append(num_occurrences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Length (Response)\n",
    "\n",
    "For creative writing\n",
    "\n",
    "NLTK word and sentence tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_length = []\n",
    "for idx, output_string in enumerate(conversation_strings_output):\n",
    "    if \"cw\" in model_task_method[idx]:\n",
    "        sentence_length.append(get_nltk_sentence_length(output_string))\n",
    "    else:\n",
    "        sentence_length.append('NA')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flesch Reading Ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fres = []\n",
    "for idx, output_string in enumerate(conversation_strings_output):\n",
    "    if \"cw\" in model_task_method[idx]:\n",
    "        fres.append(textstat.flesch_reading_ease(output_string))\n",
    "    else:\n",
    "        fres.append('NA')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differences in Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference of the scores above in responses vs. prompts, additionally responses vs. provided answer for GSM8K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compexity of Provided Answers - GSM8K Complexity Measure"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
