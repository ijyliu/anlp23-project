{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interannotator Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.stats.contingency_tables import mcnemar \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import simpledorff\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Combined_Data.xlsx\n",
    "combined_data = pd.read_excel('Combined_Data.xlsx')\n",
    "\n",
    "# GSM8k data - limit to task = \"gsm8k\"\n",
    "gsm8k_data = combined_data[combined_data['task'] == 'gsm8k']\n",
    "\n",
    "# Creative writing data - limit to task = \"cw\"\n",
    "cw_data = combined_data[combined_data['task'] == 'cw']\n",
    "\n",
    "# Load direct_prompting_comparison.xlsx\n",
    "direct_prompting_comparison = pd.read_excel('direct_prompting_comparison.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['model_task_method', 'conversation_number',\n",
      "       'coherence_1_incoherent_10_very_coherent',\n",
      "       'task_constraints_followed_0_not_followed_1_followed',\n",
      "       'ease_of_review_1_easy_10_hard', 'correct',\n",
      "       'Prediction_Based_On_First_10', 'Prediction_Based_On_Last_10',\n",
      "       'Aggregated_Prediction', 'Prediction_Based_On_First_10_LP',\n",
      "       'response_Based_On_First_10_LP', 'Prediction_Based_On_Last_10_LP',\n",
      "       'response_Based_On_Last_10_LP', 'response_LP',\n",
      "       'Aggregated_Prediction_LP', 'Prediction_Based_On_First_50_LP',\n",
      "       'response_Based_On_First_50_LP', 'Prediction_Based_On_Last_50_LP',\n",
      "       'response_Based_On_Last_50_LP', 'Aggregated_Prediction_50_LP',\n",
      "       'Prediction_Based_On_random_50_LP_1',\n",
      "       'response_Based_On_random_50_LP_1',\n",
      "       'Prediction_Based_On_random_50_LP_2',\n",
      "       'response_Based_On_random_50_LP_2',\n",
      "       'Aggregated_Prediction_random_50_LP', 'Unnamed: 0_x', 'response_x',\n",
      "       'replace_slash_n_slash_n_with_newline_x',\n",
      "       'replace_slash_n_slash_n_with_newline_values_x',\n",
      "       'replace_slash_n_with_newline_x',\n",
      "       'replace_slash_n_with_newline_values_x', 'avg_cosine_sim',\n",
      "       'num_sentences_x', 'Unnamed: 0_y', 'response_y',\n",
      "       'replace_slash_n_slash_n_with_newline_y',\n",
      "       'replace_slash_n_slash_n_with_newline_values_y',\n",
      "       'replace_slash_n_with_newline_y',\n",
      "       'replace_slash_n_with_newline_values_y',\n",
      "       'avg_inter_paragraph_cosine_sim', 'num_paragraphs', 'num_sentences_y',\n",
      "       'cosine_sims', 'conversation_length', 'input_length', 'output_length',\n",
      "       'conversation_cost', 'gsm8k_question_index', 'gsm8k_answer',\n",
      "       'gsm8k_length_vs_provided', 'length_vs_direct_prompting',\n",
      "       'num_linebreaks', 'num_sentences', 'num_step_i', 'num_1_dot_etc',\n",
      "       'sentence_length', 'fres', 'num_linebreaks_prompts',\n",
      "       'num_sentences_prompts', 'num_step_i_prompts', 'num_1_dot_etc_prompts',\n",
      "       'sentence_length_prompts', 'fres_prompts', 'num_linebreaks_provided',\n",
      "       'num_sentences_provided', 'num_step_i_provided',\n",
      "       'num_1_dot_etc_provided', 'compliance',\n",
      "       'coherence_1_incoherent_10_very_coherent_compliance_adjusted', 'model',\n",
      "       'task', 'method', 'accuracy_quality',\n",
      "       'accuracy_quality_compliance_adjusted'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(cw_data.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Checks on First-Pass Fine-Tuning Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       coherence_1_incoherent_10_very_coherent  Prediction_Based_On_First_10  \\\n",
      "count                              1600.000000                   1440.000000   \n",
      "mean                                  4.974375                      5.403472   \n",
      "std                                   2.612563                      2.175833   \n",
      "min                                   1.000000                      1.000000   \n",
      "25%                                   3.000000                      3.000000   \n",
      "50%                                   5.000000                      7.000000   \n",
      "75%                                   7.000000                      7.000000   \n",
      "max                                  10.000000                      8.000000   \n",
      "\n",
      "       Prediction_Based_On_Last_10  Aggregated_Prediction  \n",
      "count                  1440.000000            1600.000000  \n",
      "mean                      4.066667               4.722500  \n",
      "std                       2.569885               2.265422  \n",
      "min                       1.000000               1.000000  \n",
      "25%                       2.000000               3.000000  \n",
      "50%                       3.000000               4.500000  \n",
      "75%                       7.000000               7.000000  \n",
      "max                       8.000000               8.000000  \n"
     ]
    }
   ],
   "source": [
    "# Summary statistics for variables: coherence_1_incoherent_10_very_coherent, Prediction_Based_On_First_10, Prediction_Based_On_Last_10, Aggregated_Prediction\n",
    "print(cw_data[['coherence_1_incoherent_10_very_coherent', 'Prediction_Based_On_First_10', 'Prediction_Based_On_Last_10', 'Aggregated_Prediction']].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DescribeResult(nobs=1600, minmax=(-7.0, 8.0), mean=0.251875, variance=5.838020247029392, skewness=-0.006411691981771768, kurtosis=0.18078123260822876)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([  2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         16.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   6.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,  23.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,  10.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,  36.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,  22.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,  63.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,  34.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,  99.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,  44.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0., 184.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,  76.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0., 221.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0., 101.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0., 190.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,  64.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0., 119.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,  45.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,  82.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,  25.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,  48.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,  15.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  49.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   9.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   6.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   5.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   3.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   2.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.]),\n",
       " array([-7.   , -6.985, -6.97 , ...,  7.97 ,  7.985,  8.   ]),\n",
       " <BarContainer object of 1000 artists>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Differences between coherence_1_incoherent_10_very_coherent and Aggregated_Prediction\n",
    "print(stats.describe(cw_data['coherence_1_incoherent_10_very_coherent'] - cw_data['Aggregated_Prediction']))\n",
    "\n",
    "# Histogram\n",
    "plt.hist(cw_data['coherence_1_incoherent_10_very_coherent'] - cw_data['Aggregated_Prediction'], bins = 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['model_task_method', 'conversation_number',\n",
      "       'coherence_1_incoherent_10_very_coherent',\n",
      "       'task_constraints_followed_0_not_followed_1_followed',\n",
      "       'ease_of_review_1_easy_10_hard', 'correct',\n",
      "       'Prediction_Based_On_First_10', 'Prediction_Based_On_Last_10',\n",
      "       'Aggregated_Prediction', 'Prediction_Based_On_First_10_LP',\n",
      "       'response_Based_On_First_10_LP', 'Prediction_Based_On_Last_10_LP',\n",
      "       'response_Based_On_Last_10_LP', 'response_LP',\n",
      "       'Aggregated_Prediction_LP', 'Prediction_Based_On_First_50_LP',\n",
      "       'response_Based_On_First_50_LP', 'Prediction_Based_On_Last_50_LP',\n",
      "       'response_Based_On_Last_50_LP', 'Aggregated_Prediction_50_LP',\n",
      "       'Prediction_Based_On_random_50_LP_1',\n",
      "       'response_Based_On_random_50_LP_1',\n",
      "       'Prediction_Based_On_random_50_LP_2',\n",
      "       'response_Based_On_random_50_LP_2',\n",
      "       'Aggregated_Prediction_random_50_LP', 'Unnamed: 0_x', 'response_x',\n",
      "       'replace_slash_n_slash_n_with_newline_x',\n",
      "       'replace_slash_n_slash_n_with_newline_values_x',\n",
      "       'replace_slash_n_with_newline_x',\n",
      "       'replace_slash_n_with_newline_values_x', 'avg_cosine_sim',\n",
      "       'num_sentences_x', 'Unnamed: 0_y', 'response_y',\n",
      "       'replace_slash_n_slash_n_with_newline_y',\n",
      "       'replace_slash_n_slash_n_with_newline_values_y',\n",
      "       'replace_slash_n_with_newline_y',\n",
      "       'replace_slash_n_with_newline_values_y',\n",
      "       'avg_inter_paragraph_cosine_sim', 'num_paragraphs', 'num_sentences_y',\n",
      "       'cosine_sims', 'conversation_length', 'input_length', 'output_length',\n",
      "       'conversation_cost', 'gsm8k_question_index', 'gsm8k_answer',\n",
      "       'gsm8k_length_vs_provided', 'length_vs_direct_prompting',\n",
      "       'num_linebreaks', 'num_sentences', 'num_step_i', 'num_1_dot_etc',\n",
      "       'sentence_length', 'fres', 'num_linebreaks_prompts',\n",
      "       'num_sentences_prompts', 'num_step_i_prompts', 'num_1_dot_etc_prompts',\n",
      "       'sentence_length_prompts', 'fres_prompts', 'num_linebreaks_provided',\n",
      "       'num_sentences_provided', 'num_step_i_provided',\n",
      "       'num_1_dot_etc_provided', 'compliance',\n",
      "       'coherence_1_incoherent_10_very_coherent_compliance_adjusted', 'model',\n",
      "       'task', 'method', 'accuracy_quality',\n",
      "       'accuracy_quality_compliance_adjusted'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(cw_data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall KA\n",
      "0.10686929311619198\n",
      "no first 10 KA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ijyli\\AppData\\Local\\Temp\\ipykernel_27760\\1729583903.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cw_data['model_task_method_conversation_id'] = cw_data['model'] + '_' + cw_data['task'] + '_' + cw_data['method'] + '_' + cw_data['conversation_number'].astype(str)\n",
      "C:\\Users\\ijyli\\AppData\\Local\\Temp\\ipykernel_27760\\1729583903.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cw_data['Human'] = cw_data['coherence_1_incoherent_10_very_coherent']\n",
      "C:\\Users\\ijyli\\AppData\\Local\\Temp\\ipykernel_27760\\1729583903.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cw_data['GPT-3.5 - First 10'] = cw_data['Prediction_Based_On_First_10']\n",
      "C:\\Users\\ijyli\\AppData\\Local\\Temp\\ipykernel_27760\\1729583903.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cw_data['GPT-3.5 - Last 10'] = cw_data['Prediction_Based_On_Last_10']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10782815485733388\n",
      "no last 10 KA\n",
      "0.11024510661584563\n",
      "no first and last 10 KA\n",
      "0.11158211702182685\n"
     ]
    }
   ],
   "source": [
    "# Krippendorff's alpha\n",
    "# I am using the simpledorff package\n",
    "# experiment_col should be the conversation (row of the dataframe) - create model_task_method_conversation_id\n",
    "# annotator_col should be 'Human', 'GPT-3.5 - First 10', 'GPT-3.5 - Last 10'\n",
    "# class col should be the score from coherence_1_incoherent_10_very_coherent, Prediction_Based_On_First_10, Prediction_Based_On_Last_10\n",
    "\n",
    "# Manipulate and reshape dataframe\n",
    "cw_data['model_task_method_conversation_id'] = cw_data['model'] + '_' + cw_data['task'] + '_' + cw_data['method'] + '_' + cw_data['conversation_number'].astype(str)\n",
    "cw_data['Human'] = cw_data['coherence_1_incoherent_10_very_coherent']\n",
    "cw_data['GPT-3.5 - First 10'] = cw_data['Prediction_Based_On_First_10']\n",
    "cw_data['GPT-3.5 - Last 10'] = cw_data['Prediction_Based_On_Last_10']\n",
    "ka_data = cw_data[['model_task_method_conversation_id', 'Human', 'GPT-3.5 - First 10', 'GPT-3.5 - Last 10']]\n",
    "ka_data_melted = ka_data.melt(id_vars=['model_task_method_conversation_id'], value_vars=['Human', 'GPT-3.5 - First 10', 'GPT-3.5 - Last 10'])\n",
    "ka_data_melted = ka_data_melted.rename(columns={'variable': 'grader', 'value': 'coherence_score'})\n",
    "\n",
    "# Calculate Krippendorff's alpha\n",
    "print('overall KA')\n",
    "print(simpledorff.calculate_krippendorffs_alpha_for_df(ka_data_melted,\n",
    "                                                 experiment_col='model_task_method_conversation_id',\n",
    "                                                 annotator_col='grader',\n",
    "                                                 class_col='coherence_score'))\n",
    "\n",
    "# Version excluding first 10\n",
    "ka_data_no_first_10 = cw_data[cw_data['conversation_number'] > 10][['model_task_method_conversation_id', 'Human', 'GPT-3.5 - First 10', 'GPT-3.5 - Last 10']]\n",
    "ka_data_no_first_10_melted = ka_data_no_first_10.melt(id_vars=['model_task_method_conversation_id'], value_vars=['Human', 'GPT-3.5 - First 10', 'GPT-3.5 - Last 10'])\n",
    "ka_data_no_first_10_melted = ka_data_no_first_10_melted.rename(columns={'variable': 'grader', 'value': 'coherence_score'})\n",
    "print('no first 10 KA')\n",
    "print(simpledorff.calculate_krippendorffs_alpha_for_df(ka_data_no_first_10_melted,\n",
    "                                                 experiment_col='model_task_method_conversation_id',\n",
    "                                                 annotator_col='grader',\n",
    "                                                 class_col='coherence_score'))\n",
    "\n",
    "# Version excluding last 10\n",
    "ka_data_no_last_10 = cw_data[cw_data['conversation_number'] <= 90][['model_task_method_conversation_id', 'Human', 'GPT-3.5 - First 10', 'GPT-3.5 - Last 10']]\n",
    "ka_data_no_last_10_melted = ka_data_no_last_10.melt(id_vars=['model_task_method_conversation_id'], value_vars=['Human', 'GPT-3.5 - First 10', 'GPT-3.5 - Last 10'])\n",
    "ka_data_no_last_10_melted = ka_data_no_last_10_melted.rename(columns={'variable': 'grader', 'value': 'coherence_score'})\n",
    "print('no last 10 KA')\n",
    "print(simpledorff.calculate_krippendorffs_alpha_for_df(ka_data_no_last_10_melted,\n",
    "                                                 experiment_col='model_task_method_conversation_id',\n",
    "                                                 annotator_col='grader',\n",
    "                                                 class_col='coherence_score'))\n",
    "\n",
    "# Version excluding first and last 10\n",
    "ka_data_no_first_last_10 = cw_data[(cw_data['conversation_number'] > 10) & (cw_data['conversation_number'] <= 90)][['model_task_method_conversation_id', 'Human', 'GPT-3.5 - First 10', 'GPT-3.5 - Last 10']]\n",
    "ka_data_no_first_last_10_melted = ka_data_no_first_last_10.melt(id_vars=['model_task_method_conversation_id'], value_vars=['Human', 'GPT-3.5 - First 10', 'GPT-3.5 - Last 10'])\n",
    "ka_data_no_first_last_10_melted = ka_data_no_first_last_10_melted.rename(columns={'variable': 'grader', 'value': 'coherence_score'})\n",
    "print('no first and last 10 KA')\n",
    "print(simpledorff.calculate_krippendorffs_alpha_for_df(ka_data_no_first_last_10_melted,\n",
    "                                                 experiment_col='model_task_method_conversation_id',\n",
    "                                                 annotator_col='grader',\n",
    "                                                 class_col='coherence_score'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregated predictions KA\n",
      "0.04803932294886726\n"
     ]
    }
   ],
   "source": [
    "# Try aggregated predictions\n",
    "ka_data_agg = cw_data[['model_task_method_conversation_id', 'Human', 'Aggregated_Prediction']]\n",
    "ka_data_agg_melted = ka_data_agg.melt(id_vars=['model_task_method_conversation_id'], value_vars=['Human', 'Aggregated_Prediction'])\n",
    "ka_data_agg_melted = ka_data_agg_melted.rename(columns={'variable': 'grader', 'value': 'coherence_score'})\n",
    "print('aggregated predictions KA')\n",
    "print(simpledorff.calculate_krippendorffs_alpha_for_df(ka_data_agg_melted,\n",
    "                                                 experiment_col='model_task_method_conversation_id',\n",
    "                                                 annotator_col='grader',\n",
    "                                                 class_col='coherence_score'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18525258256529475\n"
     ]
    }
   ],
   "source": [
    "# Try to demand less of the data - in ka_data_melted, bin 1-10 into 1-5\n",
    "# Divide by 2 and take the ceiling\n",
    "ka_data_melted['coherence_1_to_5'] = ka_data_melted.dropna()['coherence_score'].apply(lambda x: math.ceil(x/2))\n",
    "\n",
    "# Calculate Krippendorff's alpha\n",
    "print(simpledorff.calculate_krippendorffs_alpha_for_df(ka_data_melted,\n",
    "                                                       experiment_col='model_task_method_conversation_id',\n",
    "                                                       annotator_col='grader',\n",
    "                                                       class_col='coherence_1_to_5'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2735720914000721\n"
     ]
    }
   ],
   "source": [
    "# Try coherence on a scale of 1 to 3\n",
    "ka_data_melted['coherence_1_to_3'] = ka_data_melted.dropna()['coherence_score'].apply(lambda x: math.ceil(x/3.33))\n",
    "\n",
    "# Calculate Krippendorff's alpha\n",
    "print(simpledorff.calculate_krippendorffs_alpha_for_df(ka_data_melted,\n",
    "                                                       experiment_col='model_task_method_conversation_id',\n",
    "                                                       annotator_col='grader',\n",
    "                                                       class_col='coherence_1_to_3'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4195238892015637\n"
     ]
    }
   ],
   "source": [
    "# Try coherence vs incoherence binary\n",
    "ka_data_melted['coherence_binary'] = ka_data_melted.dropna()['coherence_score'].apply(lambda x: 1 if x > 5 else 0)\n",
    "\n",
    "# Calculate Krippendorff's alpha\n",
    "print(simpledorff.calculate_krippendorffs_alpha_for_df(ka_data_melted,\n",
    "                                                       experiment_col='model_task_method_conversation_id',\n",
    "                                                       annotator_col='grader',\n",
    "                                                       class_col='coherence_binary'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning with longer prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall KA\n",
      "0.14655382629441482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ijyli\\AppData\\Local\\Temp\\ipykernel_27760\\800768595.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cw_data['GPT-3.5 - First 10 - LP'] = cw_data['Prediction_Based_On_First_10_LP']\n",
      "C:\\Users\\ijyli\\AppData\\Local\\Temp\\ipykernel_27760\\800768595.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cw_data['GPT-3.5 - Last 10 - LP'] = cw_data['Prediction_Based_On_Last_10_LP']\n"
     ]
    }
   ],
   "source": [
    "# Manipulate and reshape dataframe\n",
    "cw_data['GPT-3.5 - First 10 - LP'] = cw_data['Prediction_Based_On_First_10_LP']\n",
    "cw_data['GPT-3.5 - Last 10 - LP'] = cw_data['Prediction_Based_On_Last_10_LP']\n",
    "ka_data_lp = cw_data[['model_task_method_conversation_id', 'Human', 'GPT-3.5 - First 10 - LP', 'GPT-3.5 - Last 10 - LP']]\n",
    "ka_data_melted_lp = ka_data_lp.melt(id_vars=['model_task_method_conversation_id'], value_vars=['Human', 'GPT-3.5 - First 10 - LP', 'GPT-3.5 - Last 10 - LP'])\n",
    "ka_data_melted_lp = ka_data_melted_lp.rename(columns={'variable': 'grader', 'value': 'coherence_score'})\n",
    "\n",
    "# Calculate Krippendorff's alpha\n",
    "print('overall KA')\n",
    "print(simpledorff.calculate_krippendorffs_alpha_for_df(ka_data_melted_lp,\n",
    "                                                 experiment_col='model_task_method_conversation_id',\n",
    "                                                 annotator_col='grader',\n",
    "                                                 class_col='coherence_score'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregated predictions KA\n",
      "0.06459910514503153\n"
     ]
    }
   ],
   "source": [
    "# Try aggregated predictions\n",
    "ka_data_agg_lp = cw_data[['model_task_method_conversation_id', 'Human', 'Aggregated_Prediction_LP']]\n",
    "ka_data_agg_melted_lp = ka_data_agg_lp.melt(id_vars=['model_task_method_conversation_id'], value_vars=['Human', 'Aggregated_Prediction_LP'])\n",
    "ka_data_agg_melted_lp = ka_data_agg_melted_lp.rename(columns={'variable': 'grader', 'value': 'coherence_score'})\n",
    "print('aggregated predictions KA')\n",
    "print(simpledorff.calculate_krippendorffs_alpha_for_df(ka_data_agg_melted_lp,\n",
    "                                                 experiment_col='model_task_method_conversation_id',\n",
    "                                                 annotator_col='grader',\n",
    "                                                 class_col='coherence_score'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using half the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall KA\n",
      "0.14723679091121322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ijyli\\AppData\\Local\\Temp\\ipykernel_27760\\672138764.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cw_data['GPT-3.5 - First 50 - LP'] = cw_data['Prediction_Based_On_First_50_LP']\n",
      "C:\\Users\\ijyli\\AppData\\Local\\Temp\\ipykernel_27760\\672138764.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cw_data['GPT-3.5 - Last 50 - LP'] = cw_data['Prediction_Based_On_Last_50_LP']\n"
     ]
    }
   ],
   "source": [
    "# Manipulate and reshape dataframe\n",
    "cw_data['GPT-3.5 - First 50 - LP'] = cw_data['Prediction_Based_On_First_50_LP']\n",
    "cw_data['GPT-3.5 - Last 50 - LP'] = cw_data['Prediction_Based_On_Last_50_LP']\n",
    "ka_data_lp_50 = cw_data[['model_task_method_conversation_id', 'Human', 'GPT-3.5 - First 50 - LP', 'GPT-3.5 - Last 50 - LP']]\n",
    "ka_data_melted_lp_50 = ka_data_lp_50.melt(id_vars=['model_task_method_conversation_id'], value_vars=['Human', 'GPT-3.5 - First 50 - LP', 'GPT-3.5 - Last 50 - LP'])\n",
    "ka_data_melted_lp_50 = ka_data_melted_lp_50.rename(columns={'variable': 'grader', 'value': 'coherence_score'})\n",
    "\n",
    "# Calculate Krippendorff's alpha\n",
    "print('overall KA')\n",
    "print(simpledorff.calculate_krippendorffs_alpha_for_df(ka_data_melted_lp_50,\n",
    "                                                 experiment_col='model_task_method_conversation_id',\n",
    "                                                 annotator_col='grader',\n",
    "                                                 class_col='coherence_score'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['model_task_method', 'conversation_number',\n",
      "       'coherence_1_incoherent_10_very_coherent',\n",
      "       'task_constraints_followed_0_not_followed_1_followed',\n",
      "       'ease_of_review_1_easy_10_hard', 'correct',\n",
      "       'Prediction_Based_On_First_10', 'Prediction_Based_On_Last_10',\n",
      "       'Aggregated_Prediction', 'Prediction_Based_On_First_10_LP',\n",
      "       'response_Based_On_First_10_LP', 'Prediction_Based_On_Last_10_LP',\n",
      "       'response_Based_On_Last_10_LP', 'response_LP',\n",
      "       'Aggregated_Prediction_LP', 'Prediction_Based_On_First_50_LP',\n",
      "       'response_Based_On_First_50_LP', 'Prediction_Based_On_Last_50_LP',\n",
      "       'response_Based_On_Last_50_LP', 'Aggregated_Prediction_50_LP',\n",
      "       'Prediction_Based_On_random_50_LP_1',\n",
      "       'response_Based_On_random_50_LP_1',\n",
      "       'Prediction_Based_On_random_50_LP_2',\n",
      "       'response_Based_On_random_50_LP_2',\n",
      "       'Aggregated_Prediction_random_50_LP', 'Unnamed: 0_x', 'response_x',\n",
      "       'replace_slash_n_slash_n_with_newline_x',\n",
      "       'replace_slash_n_slash_n_with_newline_values_x',\n",
      "       'replace_slash_n_with_newline_x',\n",
      "       'replace_slash_n_with_newline_values_x', 'avg_cosine_sim',\n",
      "       'num_sentences_x', 'Unnamed: 0_y', 'response_y',\n",
      "       'replace_slash_n_slash_n_with_newline_y',\n",
      "       'replace_slash_n_slash_n_with_newline_values_y',\n",
      "       'replace_slash_n_with_newline_y',\n",
      "       'replace_slash_n_with_newline_values_y',\n",
      "       'avg_inter_paragraph_cosine_sim', 'num_paragraphs', 'num_sentences_y',\n",
      "       'cosine_sims', 'conversation_length', 'input_length', 'output_length',\n",
      "       'conversation_cost', 'gsm8k_question_index', 'gsm8k_answer',\n",
      "       'gsm8k_length_vs_provided', 'length_vs_direct_prompting',\n",
      "       'num_linebreaks', 'num_sentences', 'num_step_i', 'num_1_dot_etc',\n",
      "       'sentence_length', 'fres', 'num_linebreaks_prompts',\n",
      "       'num_sentences_prompts', 'num_step_i_prompts', 'num_1_dot_etc_prompts',\n",
      "       'sentence_length_prompts', 'fres_prompts', 'num_linebreaks_provided',\n",
      "       'num_sentences_provided', 'num_step_i_provided',\n",
      "       'num_1_dot_etc_provided', 'compliance',\n",
      "       'coherence_1_incoherent_10_very_coherent_compliance_adjusted', 'model',\n",
      "       'task', 'method', 'accuracy_quality',\n",
      "       'accuracy_quality_compliance_adjusted',\n",
      "       'model_task_method_conversation_id', 'Human', 'GPT-3.5 - First 10',\n",
      "       'GPT-3.5 - Last 10', 'GPT-3.5 - First 10 - LP',\n",
      "       'GPT-3.5 - Last 10 - LP', 'GPT-3.5 - First 50 - LP',\n",
      "       'GPT-3.5 - Last 50 - LP'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(cw_data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregated predictions KA\n",
      "0.14723679091121322\n"
     ]
    }
   ],
   "source": [
    "# Try aggregated predictions\n",
    "ka_data_agg_lp_50 = cw_data[['model_task_method_conversation_id', 'Human', 'Aggregated_Prediction_50_LP']]\n",
    "ka_data_agg_melted_lp_50 = ka_data_agg_lp_50.melt(id_vars=['model_task_method_conversation_id'], value_vars=['Human', 'Aggregated_Prediction_50_LP'])\n",
    "ka_data_agg_melted_lp_50 = ka_data_agg_melted_lp_50.rename(columns={'variable': 'grader', 'value': 'coherence_score'})\n",
    "print('aggregated predictions KA')\n",
    "print(simpledorff.calculate_krippendorffs_alpha_for_df(ka_data_agg_melted_lp_50,\n",
    "                                                 experiment_col='model_task_method_conversation_id',\n",
    "                                                 annotator_col='grader',\n",
    "                                                 class_col='coherence_score'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using random 50% split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall KA\n",
      "0.16701274040997383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ijyli\\AppData\\Local\\Temp\\ipykernel_27760\\2104129242.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cw_data['GPT-3.5 - Random 50 - LP - 1'] = cw_data['Prediction_Based_On_random_50_LP_1']\n",
      "C:\\Users\\ijyli\\AppData\\Local\\Temp\\ipykernel_27760\\2104129242.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cw_data['GPT-3.5 - Random 50 - LP - 2'] = cw_data['Prediction_Based_On_random_50_LP_2']\n"
     ]
    }
   ],
   "source": [
    "# Manipulate and reshape dataframe\n",
    "cw_data['GPT-3.5 - Random 50 - LP - 1'] = cw_data['Prediction_Based_On_random_50_LP_1']\n",
    "cw_data['GPT-3.5 - Random 50 - LP - 2'] = cw_data['Prediction_Based_On_random_50_LP_2']\n",
    "ka_data_random_lp_50 = cw_data[['model_task_method_conversation_id', 'Human', 'GPT-3.5 - Random 50 - LP - 1', 'GPT-3.5 - Random 50 - LP - 2']]\n",
    "ka_data_melted_random_lp_50 = ka_data_random_lp_50.melt(id_vars=['model_task_method_conversation_id'], value_vars=['Human', 'GPT-3.5 - Random 50 - LP - 1', 'GPT-3.5 - Random 50 - LP - 2'])\n",
    "ka_data_melted_random_lp_50 = ka_data_melted_random_lp_50.rename(columns={'variable': 'grader', 'value': 'coherence_score'})\n",
    "\n",
    "# Calculate Krippendorff's alpha\n",
    "print('overall KA')\n",
    "print(simpledorff.calculate_krippendorffs_alpha_for_df(ka_data_melted_random_lp_50,\n",
    "                                                 experiment_col='model_task_method_conversation_id',\n",
    "                                                 annotator_col='grader',\n",
    "                                                 class_col='coherence_score'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregated predictions KA\n",
      "0.16701274040997383\n"
     ]
    }
   ],
   "source": [
    "# Try aggregated predictions\n",
    "ka_data_agg_random_lp_50 = cw_data[['model_task_method_conversation_id', 'Human', 'Aggregated_Prediction_random_50_LP']]\n",
    "ka_data_agg_melted_random_lp_50 = ka_data_agg_random_lp_50.melt(id_vars=['model_task_method_conversation_id'], value_vars=['Human', 'Aggregated_Prediction_random_50_LP'])\n",
    "ka_data_agg_melted_random_lp_50 = ka_data_agg_melted_random_lp_50.rename(columns={'variable': 'grader', 'value': 'coherence_score'})\n",
    "print('aggregated predictions KA')\n",
    "print(simpledorff.calculate_krippendorffs_alpha_for_df(ka_data_agg_melted_random_lp_50,\n",
    "                                                 experiment_col='model_task_method_conversation_id',\n",
    "                                                 annotator_col='grader',\n",
    "                                                 class_col='coherence_score'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate Preferences\n",
    "\n",
    "Best KA is the last one. See how often it has similar preferences to human coherence score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    model task human_preferred_method  conversation_number  Human  \\\n",
      "0    gpt4   cw        tree_of_thought                    1    9.0   \n",
      "1    gpt4   cw        tree_of_thought                    1    9.0   \n",
      "2    gpt4   cw        manual_few_shot                    2    9.0   \n",
      "3    gpt4   cw        manual_few_shot                    2    9.0   \n",
      "4    gpt4   cw        manual_few_shot                    2    9.0   \n",
      "..    ...  ...                    ...                  ...    ...   \n",
      "789   td3   cw          zero_shot_cot                   97    6.0   \n",
      "790   td3   cw          zero_shot_cot                   97    6.0   \n",
      "791   td3   cw          zero_shot_cot                   98    9.0   \n",
      "792   td3   cw        manual_few_shot                   99    8.0   \n",
      "793   td3   cw          zero_shot_cot                  100    9.0   \n",
      "\n",
      "    agg_preferred_method  Aggregated_Prediction_random_50_LP  \n",
      "0        tree_of_thought                                 8.0  \n",
      "1             manual_cot                                 8.0  \n",
      "2       direct_prompting                                 8.0  \n",
      "3          zero_shot_cot                                 8.0  \n",
      "4        tree_of_thought                                 8.0  \n",
      "..                   ...                                 ...  \n",
      "789      manual_few_shot                                 6.0  \n",
      "790          self_refine                                 6.0  \n",
      "791          self_refine                                 8.0  \n",
      "792      manual_few_shot                                 8.0  \n",
      "793      manual_few_shot                                 8.0  \n",
      "\n",
      "[794 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "human_and_agg_random_lp_50 = cw_data[['model', 'task', 'method', 'conversation_number', 'Human', 'Aggregated_Prediction_random_50_LP']]\n",
    "\n",
    "# Human maxes\n",
    "# By model, task, conversation_number, keep the row with the max Human score\n",
    "human_maxes = human_and_agg_random_lp_50.groupby(['model', 'task', 'conversation_number'])['Human'].max().reset_index() \n",
    "#print(human_maxes)\n",
    "# Inner join with human_and_agg_random_lp_50\n",
    "human_max_rows = pd.merge(human_maxes, human_and_agg_random_lp_50, how='inner', on=['model', 'task', 'conversation_number', 'Human'])[['model', 'task', 'method', 'conversation_number', 'Human']].rename(columns = {'method': 'human_preferred_method'})\n",
    "\n",
    "# Aggregated maxes\n",
    "# By model, task, conversation_number, keep the row with the max Aggregated_Prediction_random_50_LP score\n",
    "agg_maxes = human_and_agg_random_lp_50.groupby(['model', 'task', 'conversation_number'])['Aggregated_Prediction_random_50_LP'].max().reset_index()\n",
    "#print(agg_maxes)\n",
    "# Inner join with human_and_agg_random_lp_50\n",
    "agg_max_rows = pd.merge(agg_maxes, human_and_agg_random_lp_50, how='inner', on=['model', 'task', 'conversation_number', 'Aggregated_Prediction_random_50_LP'])[['model', 'task', 'method', 'conversation_number', 'Aggregated_Prediction_random_50_LP']].rename(columns = {'method': 'agg_preferred_method'})\n",
    "\n",
    "# Inner join human_max_rows and agg_max_rows\n",
    "human_and_agg_random_lp_50_maxes = pd.merge(human_max_rows, agg_max_rows, how='inner', on=['model', 'task', 'conversation_number'])\n",
    "print(human_and_agg_random_lp_50_maxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique model, task, conversation_number combinations\n",
      "200\n",
      "groupings where at least one human_preferred_method == agg_preferred_method\n",
      "116\n",
      "probability of group having at least one human_preferred_method == agg_preferred_method\n",
      "0.58\n",
      "average number of combinations of methods that are the max for both groups\n",
      "1.3362068965517242\n"
     ]
    }
   ],
   "source": [
    "# Note there are ties\n",
    "\n",
    "# How many unique model, task, conversation_number combinations are there?\n",
    "print('unique model, task, conversation_number combinations')\n",
    "unique_mtcn_combs = len(cw_data.groupby(['model', 'task', 'conversation_number']).size())\n",
    "print(unique_mtcn_combs)\n",
    "# Should be 2 * 100 = 200, good\n",
    "\n",
    "# For how many groupings is there at least one row where human_preferred_method == agg_preferred_method?\n",
    "print('groupings where at least one human_preferred_method == agg_preferred_method')\n",
    "g_atl_one_human_eq_agg = len(human_and_agg_random_lp_50_maxes[human_and_agg_random_lp_50_maxes['human_preferred_method'] == human_and_agg_random_lp_50_maxes['agg_preferred_method']].groupby(['model', 'task', 'conversation_number']).size())\n",
    "print(g_atl_one_human_eq_agg)\n",
    "\n",
    "# Probability\n",
    "print('probability of group having at least one human_preferred_method == agg_preferred_method')\n",
    "p_atl_one_human_eq_agg = g_atl_one_human_eq_agg / unique_mtcn_combs\n",
    "print(p_atl_one_human_eq_agg)\n",
    "\n",
    "# Average number of rows per grouping\n",
    "import numpy as np\n",
    "print('average number of combinations of methods that are the max for both groups')\n",
    "print(np.mean(human_and_agg_random_lp_50_maxes[human_and_agg_random_lp_50_maxes['human_preferred_method'] == human_and_agg_random_lp_50_maxes['agg_preferred_method']].drop_duplicates().groupby(['model', 'task', 'conversation_number']).size()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    model task human_preferred_method  conversation_number  Human  \\\n",
      "0    gpt4   cw        tree_of_thought                    1    9.0   \n",
      "6    gpt4   cw        manual_few_shot                    2    9.0   \n",
      "11   gpt4   cw      ape_zero_shot_cot                    4    8.0   \n",
      "17   gpt4   cw             manual_cot                    5    9.0   \n",
      "21   gpt4   cw          zero_shot_cot                    7    9.0   \n",
      "..    ...  ...                    ...                  ...    ...   \n",
      "774   td3   cw          least_to_most                   87    8.0   \n",
      "776   td3   cw             manual_cot                   88    3.0   \n",
      "781   td3   cw        manual_few_shot                   93    3.0   \n",
      "788   td3   cw            self_refine                   96    9.0   \n",
      "792   td3   cw        manual_few_shot                   99    8.0   \n",
      "\n",
      "    agg_preferred_method  Aggregated_Prediction_random_50_LP  \n",
      "0        tree_of_thought                                 8.0  \n",
      "6        manual_few_shot                                 8.0  \n",
      "11     ape_zero_shot_cot                                 8.0  \n",
      "17            manual_cot                                 9.0  \n",
      "21         zero_shot_cot                                 8.0  \n",
      "..                   ...                                 ...  \n",
      "774        least_to_most                                 6.0  \n",
      "776           manual_cot                                 3.0  \n",
      "781      manual_few_shot                                 3.0  \n",
      "788          self_refine                                 7.0  \n",
      "792      manual_few_shot                                 8.0  \n",
      "\n",
      "[155 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(human_and_agg_random_lp_50_maxes[human_and_agg_random_lp_50_maxes['human_preferred_method'] == human_and_agg_random_lp_50_maxes['agg_preferred_method']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A brief (very tentative, heavy on assumptions, and possibly wrong) simulation study of the share of groupings where at least one human preferred method is equal to an aggregate preferred method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulation probability of agreement\n",
      "0.211997\n"
     ]
    }
   ],
   "source": [
    "# There are 8 methods\n",
    "# Agreement odds of 1/8 * 1/8 by chance under independent uniforms (without account for score values)\n",
    "# More complicated if you consider chances with 1-10 due to taking of means leading to multiple methods getting same scores\n",
    "\n",
    "# By chance, how often would you expect to see agreement on this scale\n",
    "# Tons of other baked in assumptions and such here but just a heuristic\n",
    "import random\n",
    "# Set seed\n",
    "random.seed(201)\n",
    "# Set up simulations\n",
    "num_sims = 1000000\n",
    "# Indicators for if at least one index of max method_human_scores == index of max of method_agg_scores\n",
    "at_least_one_idx_max_human_agg_equal = []\n",
    "# For each simulation - roughly equalivalent to a model, task, conversation_number combination\n",
    "for _ in range(num_sims):\n",
    "    # For each method, draw human score as number from 1 to 10, uniform\n",
    "    method_human_scores = [random.randint(1, 10) for _ in range(8)]\n",
    "    # Draw agg score component 1 as number from 1 to 10, uniform\n",
    "    method_agg_component_1_scores = [random.randint(1, 10) for _ in range(8)]\n",
    "    # Draw agg score 2 as number from 1 to 10, uniform\n",
    "    method_agg_component_2_scores = [random.randint(1, 10) for _ in range(8)]\n",
    "    # Aggregate agg scores\n",
    "    method_agg_scores = [(sum(x) / 2) for x in zip(method_agg_component_1_scores, method_agg_component_2_scores)]\n",
    "    # Get indices of max human score\n",
    "    indices_max_human_score = [i for i, x in enumerate(method_human_scores) if x == max(method_human_scores)]\n",
    "    # Get indices of max agg score\n",
    "    indices_max_agg_score = [i for i, x in enumerate(method_agg_scores) if x == max(method_agg_scores)]\n",
    "    # Append indicator for sharing at least one index\n",
    "    at_least_one_idx_max_human_agg_equal.append(len(set(indices_max_human_score).intersection(indices_max_agg_score)) > 0)\n",
    "    \n",
    "# Simulation probability of agreement\n",
    "print('simulation probability of agreement')\n",
    "print(np.mean(at_least_one_idx_max_human_agg_equal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.135, 0.11, 0.11, 0.09, 0.09, 0.105, 0.09, 0.15, 0.07, 0.05], [0.135, 0.13, 0.11, 0.05, 0.095, 0.105, 0.105, 0.145, 0.1, 0.025], [0.13, 0.105, 0.155, 0.115, 0.085, 0.135, 0.1, 0.1, 0.07, 0.005], [0.04, 0.115, 0.205, 0.13, 0.105, 0.085, 0.115, 0.12, 0.06, 0.025], [0.06, 0.1, 0.12, 0.09, 0.095, 0.095, 0.135, 0.2, 0.08, 0.025], [0.095, 0.08, 0.11, 0.12, 0.115, 0.075, 0.175, 0.17, 0.04, 0.02], [0.105, 0.18, 0.21, 0.16, 0.065, 0.09, 0.08, 0.085, 0.025, 0.0], [0.145, 0.095, 0.085, 0.105, 0.11, 0.07, 0.13, 0.145, 0.08, 0.035]]\n",
      "[[0.14, 0.11, 0.09, 0.04, 0.18, 0.05, 0.07, 0.22, 0.1, 0.0], [0.15, 0.18, 0.15, 0.03, 0.06, 0.03, 0.09, 0.3, 0.01, 0.0], [0.09, 0.23, 0.13, 0.04, 0.14, 0.01, 0.07, 0.28, 0.01, 0.0], [0.05, 0.11, 0.11, 0.13, 0.21, 0.08, 0.18, 0.12, 0.01, 0.0], [0.07, 0.14, 0.06, 0.05, 0.12, 0.04, 0.09, 0.39, 0.04, 0.0], [0.08, 0.11, 0.14, 0.04, 0.17, 0.05, 0.17, 0.21, 0.03, 0.0], [0.12, 0.2, 0.12, 0.04, 0.13, 0.06, 0.08, 0.22, 0.03, 0.0], [0.15, 0.15, 0.07, 0.05, 0.09, 0.03, 0.06, 0.34, 0.06, 0.0]]\n",
      "[[0.17, 0.06, 0.08, 0.06, 0.12, 0.05, 0.21, 0.25, 0.0, 0.0], [0.1, 0.12, 0.21, 0.09, 0.17, 0.01, 0.14, 0.16, 0.0, 0.0], [0.07, 0.06, 0.28, 0.18, 0.11, 0.0, 0.11, 0.19, 0.0, 0.0], [0.08, 0.07, 0.22, 0.14, 0.15, 0.07, 0.1, 0.17, 0.0, 0.0], [0.05, 0.05, 0.17, 0.07, 0.14, 0.07, 0.15, 0.3, 0.0, 0.0], [0.05, 0.08, 0.26, 0.13, 0.08, 0.0, 0.19, 0.21, 0.0, 0.0], [0.1, 0.08, 0.3, 0.12, 0.1, 0.03, 0.15, 0.12, 0.0, 0.0], [0.18, 0.04, 0.08, 0.07, 0.1, 0.08, 0.27, 0.18, 0.0, 0.0]]\n",
      "simulation probability of agreement\n",
      "0.23538\n",
      "num shared maxes\n",
      "0.241797\n"
     ]
    }
   ],
   "source": [
    "# We really should draw from something like the probability distribution of actual scores...\n",
    "# For each method, calculate distribution of Human and Aggregated_Prediction_random_50_LP\n",
    "# Then, for each model, task, conversation_number combination, draw from those distributions\n",
    "\n",
    "method_human_distributions = []\n",
    "method_lp_1_distributions = []\n",
    "method_lp_2_distributions = []\n",
    "for method in cw_data['method'].unique():\n",
    "    # Load data for method\n",
    "    method_data = cw_data[cw_data['method'] == method]\n",
    "    # Get counts of each value of Human as a list\n",
    "    method_human_counts = []\n",
    "    for value in range(1, 11):\n",
    "        method_human_counts.append(len(method_data[method_data['Human'] == value]))\n",
    "    # Divide by sum to get probabilities\n",
    "    method_human_probs = [x / sum(method_human_counts) for x in method_human_counts]\n",
    "    # Append to method_human_distributions\n",
    "    method_human_distributions.append(method_human_probs)\n",
    "    # Get counts of each value of Prediction_Based_On_random_50_LP_1\n",
    "    method_lp_1_counts = []\n",
    "    for value in range(1, 11):\n",
    "        method_lp_1_counts.append(len(method_data[method_data['Prediction_Based_On_random_50_LP_1'] == value]))\n",
    "    # Divide by sum to get probabilities\n",
    "    method_lp_1_probs = [x / sum(method_lp_1_counts) for x in method_lp_1_counts]\n",
    "    # Append to method_human_distributions\n",
    "    method_lp_1_distributions.append(method_lp_1_probs)\n",
    "    # Get counts of each value of Prediction_Based_On_random_50_LP_2\n",
    "    method_lp_2_counts = []\n",
    "    for value in range(1, 11):\n",
    "        method_lp_2_counts.append(len(method_data[method_data['Prediction_Based_On_random_50_LP_2'] == value]))\n",
    "    # Divide by sum to get probabilities\n",
    "    method_lp_2_probs = [x / sum(method_lp_2_counts) for x in method_lp_2_counts]\n",
    "    # Append to method_human_distributions\n",
    "    method_lp_2_distributions.append(method_lp_2_probs)\n",
    "    \n",
    "print(method_human_distributions)\n",
    "print(method_lp_1_distributions)\n",
    "print(method_lp_2_distributions)\n",
    "\n",
    "# Set seed\n",
    "random.seed(201)\n",
    "# Set up simulations\n",
    "num_sims = 1000000\n",
    "# Indicators for if at least one index of max method_human_scores == index of max of method_agg_scores\n",
    "at_least_one_idx_max_human_agg_equal_empirical = []\n",
    "# number of shared maxes\n",
    "num_shared_maxes = []\n",
    "# For each simulation - roughly equalivalent to a model, task, conversation_number combination\n",
    "for _ in range(num_sims):\n",
    "    # For each method, draw human score as number from 1 to 10, uniform\n",
    "    method_human_scores = [random.choices(range(1, 11), weights = method_human_distributions[i])[0] for i in range(8)]\n",
    "    # Draw lp 1 and lp 2 scores\n",
    "    lp_1_scores = [random.choices(range(1, 11), weights = method_lp_1_distributions[i])[0] for i in range(8)]\n",
    "    lp_2_scores = [random.choices(range(1, 11), weights = method_lp_2_distributions[i])[0] for i in range(8)]\n",
    "    # Aggregate scores\n",
    "    method_agg_scores = [sum(x) / 2 for x in zip(lp_1_scores, lp_2_scores)]\n",
    "    # Get indices of max human score\n",
    "    indices_max_human_score = [i for i, x in enumerate(method_human_scores) if x == max(method_human_scores)]\n",
    "    # Get indices of max agg score\n",
    "    indices_max_agg_score = [i for i, x in enumerate(method_agg_scores) if x == max(method_agg_scores)]\n",
    "    # Num shared maxes\n",
    "    num_shared_maxes.append(len(set(indices_max_human_score).intersection(indices_max_agg_score)))\n",
    "    # Append indicator for sharing at least one index\n",
    "    at_least_one_idx_max_human_agg_equal_empirical.append(len(set(indices_max_human_score).intersection(indices_max_agg_score)) > 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulation probability of agreement\n",
      "0.23538\n",
      "num shared maxes\n",
      "0.241797\n"
     ]
    }
   ],
   "source": [
    "# Simulation probability of agreement\n",
    "print('simulation probability of agreement')\n",
    "sim_prob_agreement_complex = np.mean(at_least_one_idx_max_human_agg_equal_empirical)\n",
    "print(sim_prob_agreement_complex)\n",
    "print('num shared maxes')\n",
    "print(np.mean(num_shared_maxes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A very psuedo-bootstrap against null distribution\n",
    "bootstrap_draws = 1000000\n",
    "empirical_prob_agreements = []\n",
    "for _ in range(bootstrap_draws):\n",
    "    # Draw 200 elements from at_least_one_idx_max_human_agg_equal_empirical\n",
    "    bootstrap_sample = random.choices(at_least_one_idx_max_human_agg_equal_empirical, k = 200)\n",
    "    # Append mean of bootstrap sample\n",
    "    empirical_prob_agreements.append(np.mean(bootstrap_sample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantile of p_atl_one_human_eq_agg in empirical_prob_agreements\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "# What quantile is p_atl_one_human_eq_agg in empirical_prob_agreements?\n",
    "print('quantile of p_atl_one_human_eq_agg in empirical_prob_agreements')\n",
    "print(stats.percentileofscore(empirical_prob_agreements, p_atl_one_human_eq_agg))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
