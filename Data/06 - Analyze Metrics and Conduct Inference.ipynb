{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Metrics and Conduct Inference.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.stats.contingency_tables import mcnemar \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Combined_Data.xlsx\n",
    "combined_data = pd.read_excel('Combined_Data.xlsx')\n",
    "\n",
    "# GSM8k data - limit to task = \"gsm8k\"\n",
    "gsm8k_data = combined_data[combined_data['task'] == 'gsm8k']\n",
    "\n",
    "# Creative writing data - limit to task = \"cw\"\n",
    "cw_data = combined_data[combined_data['task'] == 'cw']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy/Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce table - one column for each method, one row for each model by task type\n",
    "# Average values of correct_or_incorrect and creative_writing_score\n",
    "# Also get variance\n",
    "\n",
    "# Column for combined accuracy_quality - correct_or_incorrect if task = \"gsm8k\", creative_writing_score if task = \"cw\"\n",
    "combined_data['accuracy_quality'] = combined_data.apply(lambda row: row['correct_or_incorrect'] if row['task'] == 'gsm8k' else row['creative_writing_score'], axis=1)\n",
    "\n",
    "# Average accuracy_quality by model, method, task\n",
    "# Also get variance\n",
    "avg_accuracy_quality_with_variance = combined_data[['model', 'method', 'task', 'accuracy_quality']].groupby(['model', 'method', 'task']).agg(['mean', 'var'])['accuracy_quality'].reset_index()\n",
    "\n",
    "# Combine mean and variance into one column that is a string with the mean and then the variance in parentheses\n",
    "avg_accuracy_quality_with_variance['accuracy_quality'] = avg_accuracy_quality_with_variance.apply(lambda row: str(round(row['mean'], 2)) + ' (' + str(round(row['var'], 2)) + ')', axis=1)\n",
    "\n",
    "# Pivot table - column method should go wide\n",
    "avg_accuracy_quality_pivot = avg_accuracy_quality_with_variance.pivot_table(index=['model', 'task'], columns='method', values='accuracy_quality').reset_index()\n",
    "\n",
    "# Sort rows by task - gsm8k task first, then cw\n",
    "# Sort by model - text-davinci-003 first, then gpt4\n",
    "avg_accuracy_quality_pivot = avg_accuracy_quality_pivot.sort_values(by=['task', 'model'], ascending=[True, True])\n",
    "\n",
    "# Order columns: direct_prompting, zero_shot_cot, ape_zero_shot_cot, tree_of_thought, self_refine, least_to_most, manual_few_shot, manual_cot\n",
    "avg_accuracy_quality_pivot = avg_accuracy_quality_pivot[['model', 'task', 'direct_prompting', 'zero_shot_cot', 'ape_zero_shot_cot', 'tree_of_thought', 'self_refine', 'least_to_most', 'manual_few_shot', 'manual_cot']]\n",
    "\n",
    "# Output to LaTeX\n",
    "avg_accuracy_quality_pivot.to_latex('../Output/avg_accuracy_quality_pivot.tex', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform McNemar's Test on GSM8k data\n",
    "\n",
    "# Get in terms of number of questions answered correctly: sum accuracy_quality column by model, method, task\n",
    "gsm8k_num_correct= gsm8k_data[['model', 'method', 'task', 'accuracy_quality']].groupby(['model', 'method', 'task']).sum()['accuracy_quality'].reset_index()\n",
    "# Rename column to num_correct\n",
    "gsm8k_num_correct = gsm8k_num_correct.rename(columns={'accuracy_quality': 'num_correct'})\n",
    "\n",
    "# Make another table with counts of questions missed (number of observations minus accuracy_quality sum) by model, method, task\n",
    "gsm8k_num_obs = gsm8k_data[['model', 'method', 'task', 'accuracy_quality']].groupby(['model', 'method', 'task']).count()['accuracy_quality'].reset_index()\n",
    "# Rename column to num_obs\n",
    "gsm8k_num_obs = gsm8k_num_obs.rename(columns={'accuracy_quality': 'num_obs'})\n",
    "# Join together by model, method, task\n",
    "gsm8k_correct_incorrect_obs = gsm8k_num_correct.merge(gsm8k_num_obs, on=['model', 'method', 'task'], how='left')\n",
    "# Table should have model, method, task, num_correct, num_incorrect, num_obs\n",
    "gsm8k_correct_incorrect_obs['num_incorrect'] = gsm8k_correct_incorrect_obs['num_obs'] - gsm8k_correct_incorrect_obs['num_correct']\n",
    "\n",
    "gsm8k_correct_incorrect_obs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform McNemar's test\n",
    "# Accepts argument of name of model, method, task\n",
    "\n",
    "def perform_mcnemar(model, method, task):\n",
    "\n",
    "    # Create variables for data of interest\n",
    "    # direct_prompting_correct\n",
    "    direct_prompting_correct = gsm8k_correct_incorrect_obs[(gsm8k_correct_incorrect_obs['model'] == model) & (gsm8k_correct_incorrect_obs['method'] == \"direct_prompting\") & (gsm8k_correct_incorrect_obs['task'] == task)]['num_correct'].values[0]\n",
    "    # direct_prompting_incorrect\n",
    "    direct_prompting_incorrect = gsm8k_correct_incorrect_obs[(gsm8k_correct_incorrect_obs['model'] == model) & (gsm8k_correct_incorrect_obs['method'] == \"direct_prompting\") & (gsm8k_correct_incorrect_obs['task'] == task)]['num_incorrect'].values[0]\n",
    "    # method_correct\n",
    "    method_correct = gsm8k_correct_incorrect_obs[(gsm8k_correct_incorrect_obs['model'] == model) & (gsm8k_correct_incorrect_obs['method'] == method) & (gsm8k_correct_incorrect_obs['task'] == task)]['num_correct'].values[0]\n",
    "    # method_incorrect\n",
    "    method_incorrect = gsm8k_correct_incorrect_obs[(gsm8k_correct_incorrect_obs['model'] == model) & (gsm8k_correct_incorrect_obs['method'] == method) & (gsm8k_correct_incorrect_obs['task'] == task)]['num_incorrect'].values[0]\n",
    "\n",
    "    # Create a dataset \n",
    "    # Row for direct prompting then method\n",
    "    # Column for correct then incorrect\n",
    "    data = [[direct_prompting_correct, direct_prompting_incorrect], \n",
    "            [method_correct, method_incorrect]] \n",
    "    \n",
    "    print('McNemar\\'s Test (Exact) for ' + model + ' ' + method + ' ' + task)\n",
    "\n",
    "    # McNemar's Test, exact, without any continuity correction \n",
    "    print('No continuity correction')\n",
    "    print(mcnemar(data, exact=True, correction=False)) \n",
    "    \n",
    "    # McNemar's Test with the continuity correction \n",
    "    print('With continuity correction')\n",
    "    print(mcnemar(data, exact=True, correction=True)) \n",
    "\n",
    "    # Return data\n",
    "    return mcnemar(data, exact=True, correction=False)[0], mcnemar(data, exact=True, correction=False)[1], mcnemar(data, exact=True, correction=True)[0], mcnemar(data, exact=True, correction=True)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add McNemar's Test results to table\n",
    "# Iterate over all combinations of model, method, task\n",
    "# Except for method = \"direct_prompting\" and task = \"cw\"\n",
    "# For each combination, perform McNemar's Test and add to table\n",
    "\n",
    "# Create combos of model, method, task - all unique combinations of these three in combined_data\n",
    "# Get unique values of model, method, task\n",
    "models = combined_data['model'].unique()\n",
    "methods = combined_data['method'].unique()\n",
    "tasks = combined_data['task'].unique()\n",
    "# Create list of all combinations of model, method, task\n",
    "combinations = [(model, method, task) for model in models for method in methods for task in tasks]\n",
    "\n",
    "# Add as rows to a dataframe\n",
    "# Create empty dataframe\n",
    "mcnemars_results = pd.DataFrame(columns=['model', 'method', 'task', 'statistic', 'pvalue', 'statistic_with_correction', 'pvalue_with_correction'])\n",
    "\n",
    "# Iterate over combinations\n",
    "for model, method, task in combinations:\n",
    "    # Skip if method = \"direct_prompting\" or task = \"cw\"\n",
    "    if method == 'direct_prompting' or task == 'cw':\n",
    "        continue\n",
    "    # Perform McNemar's Test\n",
    "    statistic_without_correction, pvalue_without_correction, statistic_with_correction, pvalue_with_correction = perform_mcnemar(model, method, task)\n",
    "    # Add to table\n",
    "    mcnemars_results = mcnemars_results.concat(pd.DataFrame([[model, method, task, statistic_without_correction, pvalue_without_correction, statistic_with_correction, pvalue_with_correction]], columns=['model', 'method', 'task', 'statistic', 'pvalue', 'statistic_with_correction', 'pvalue_with_correction']))\n",
    "\n",
    "mcnemars_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows that are not significant at p < 0.05\n",
    "mcnemars_results[mcnemars_results['pvalue'] >= 0.05]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create table for comparing each model/task/method with the appropriate direct prompting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "no_direct_prompting_data = combined_data[combined_data['method'] != 'direct_prompting']\n",
    "direct_prompting_data = combined_data[combined_data['method'] == 'direct_prompting']\n",
    "\n",
    "# Add prefix dp_ to columns in direct_prompting_data\n",
    "direct_prompting_data = direct_prompting_data.add_prefix('dp_')\n",
    "\n",
    "# Left join datasets on model = dp_model, task = dp_task\n",
    "direct_prompting_comparison = no_direct_prompting_data.merge(direct_prompting_data, left_on=['model', 'task'], right_on=['dp_model', 'dp_task'], how='left')\n",
    "\n",
    "direct_prompting_comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paired t-test for quality, means of other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform paired t-test on creative writing data and all other metrics\n",
    "\n",
    "# Metrics to t-test: 'creative_writing_score', 'ease_of_evaluation_score', 'conversation_length', 'input_length', 'output_length', 'conversation_cost', 'gsm8k_length_vs_provided', 'length_vs_direct_prompting', 'num_linebreaks', 'num_sentences', 'num_step_i', 'num_1_dot_etc', 'sentence_length', 'fres', 'num_linebreaks_prompts', 'num_sentences_prompts', 'num_step_i_prompts', 'num_1_dot_etc_prompts', 'sentence_length_prompts', 'fres_prompts'\n",
    "metrics_to_t_test = ['creative_writing_score', 'ease_of_evaluation_score', 'conversation_length', 'input_length', 'output_length', 'conversation_cost', 'gsm8k_length_vs_provided', 'length_vs_direct_prompting', 'num_linebreaks', 'num_sentences', 'num_step_i', 'num_1_dot_etc', 'sentence_length', 'fres', 'num_linebreaks_prompts', 'num_sentences_prompts', 'num_step_i_prompts', 'num_1_dot_etc_prompts', 'sentence_length_prompts', 'fres_prompts']\n",
    "\n",
    "# Create table to hold results\n",
    "# Columns of model, task, method, mean metric, mean dp_metric, statistic, pvalue\n",
    "t_test_results = pd.DataFrame(columns=['model', 'task', 'method', 'metric', 'dp_mean', 'using_method_mean', 'statistic', 'pvalue'])\n",
    "# Iterate over model, task, method\n",
    "for model, task, method in direct_prompting_comparison[['model', 'task', 'method']].values:\n",
    "    # Iterate over metrics_to_t_test\n",
    "    for metric in metrics_to_t_test:\n",
    "        # Get data\n",
    "        # Sort by conversation_number\n",
    "        data = direct_prompting_comparison[(direct_prompting_comparison['model'] == model) & (direct_prompting_comparison['task'] == task) & (direct_prompting_comparison['method'] == method)].sort_values(by=['conversation_number'])\n",
    "        # direct_prompting holds the metric when using direct_prompting\n",
    "        direct_prompting = data['dp_' + metric]\n",
    "        # using_method holds the metric when using the method\n",
    "        using_method = data[metric]\n",
    "        # Perform the paired sample t-test \n",
    "        statistic, pvalue = stats.ttest_rel(direct_prompting, using_method)\n",
    "        # Add to table\n",
    "        t_test_results = t_test_results.concat(pd.DataFrame([[model, task, method, metric, direct_prompting.mean(), using_method.mean(), statistic, pvalue]], columns=['model', 'task', 'method', 'metric', 'dp_mean', 'using_method_mean', 'statistic', 'pvalue']))\n",
    "\n",
    "t_test_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows that are not significant at p < 0.05\n",
    "t_test_results[t_test_results['pvalue'] >= 0.05]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding release date to accuracy quality table (optional version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release date and gains versus direct prompting\n",
    "\n",
    "# Load Excel file \"..\\Selection of Prompt Engineering Methods\\Hand-Labeled Method and Implementation Considerations.xlsx\"\n",
    "hand_labeled_data = pd.read_excel('..\\Selection of Prompt Engineering Methods\\Hand-Labeled Method and Implementation Considerations.xlsx')\n",
    "\n",
    "# Keep columns ss_publication_date, \"Prompt Engineering Method\"\n",
    "hand_labeled_data = hand_labeled_data[['ss_publication_date', 'Prompt Engineering Method']]\n",
    "\n",
    "# Mapping names from this data to technqiue names\n",
    "# \"Few-Shot Learing\" -> \"manual_few_shot\"\n",
    "# \"Chain-of-Thought Prompting\" -> \"manual_cot\"\n",
    "# \"Zero-Shot Chain-of_Thought\" -> \"zero_shot_cot\"\n",
    "# \"Automatic Prompt Engineer\" -> \"ape_zero_shot_cot\"\n",
    "# \"Self-Refine\" -> \"self_refine\"\n",
    "# \"Least-to-Most Prompting\" -> \"least_to_most\"\n",
    "# \"Tree-of-Thought\" -> \"tree_of_thought\"\n",
    "# Set names in hand_labeled_data to these names\n",
    "hand_labeled_data['technique_name'] = hand_labeled_data['Prompt Engineering Method'].replace({'Few-Shot Learning': 'manual_few_shot', 'Chain-of-Thought Prompting': 'manual_cot', 'Zero-Shot Chain-of-Thought': 'zero_shot_cot', 'Automatic Prompt Engineer': 'ape_zero_shot_cot', 'Self-Refine': 'self_refine', 'Least-to-Most Prompting': 'least_to_most', 'Tree-of-Thought': 'tree_of_thought'})\n",
    "\n",
    "# Convert ss_publication_date to \"YYYY-MM-DD\"\n",
    "hand_labeled_data['ss_publication_date'] = hand_labeled_data['ss_publication_date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Add ss_publication_date to avg_accuracy_quality_pivot\n",
    "# The value should be for each column, right below the column name\n",
    "# Transpose accuracy_quality_pivot\n",
    "avg_accuracy_quality_pivot_transposed = avg_accuracy_quality_pivot.transpose().reset_index()\n",
    "\n",
    "# Set name of first column to \"technique_name\"\n",
    "avg_accuracy_quality_pivot_transposed = avg_accuracy_quality_pivot_transposed.rename(columns={'index': 'technique_name'})\n",
    "\n",
    "# Left join hand_labeled_data to avg_accuracy_quality_pivot_transposed on technique_name\n",
    "avg_accuracy_quality_pivot_transposed = avg_accuracy_quality_pivot_transposed.merge(hand_labeled_data, on='technique_name', how='left')\n",
    "\n",
    "# Transpose back\n",
    "avg_accuracy_quality_pivot_with_date = avg_accuracy_quality_pivot_transposed.transpose().reset_index()\n",
    "\n",
    "# Output to LaTeX\n",
    "avg_accuracy_quality_pivot_with_date.to_latex('../Output/avg_accuracy_quality_pivot_with_date.tex', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Function for a table of means for a metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def means_table(metric):\n",
    "\n",
    "    # Average combined_data metric by model, method, task\n",
    "    avg_combined_data_metric = combined_data[['model', 'method', 'task', metric]].groupby(['model', 'method', 'task']).agg(['mean'])[metric].reset_index()\n",
    "\n",
    "    # Pivot table - column method should go wide\n",
    "    avg_combined_data_metric_pivot = avg_combined_data_metric.pivot_table(index=['model', 'task'], columns='method', values=metric).reset_index()\n",
    "\n",
    "    # Sort rows by task - gsm8k task first, then cw\n",
    "    # Sort by model - text-davinci-003 first, then gpt4\n",
    "    avg_combined_data_metric_pivot = avg_combined_data_metric_pivot.sort_values(by=['task', 'model'], ascending=[True, True])\n",
    "\n",
    "    # Order columns: direct_prompting, zero_shot_cot, ape_zero_shot_cot, tree_of_thought, self_refine, least_to_most, manual_few_shot, manual_cot\n",
    "    avg_combined_data_metric_pivot = avg_combined_data_metric_pivot[['model', 'task', 'direct_prompting', 'zero_shot_cot', 'ape_zero_shot_cot', 'tree_of_thought', 'self_refine', 'least_to_most', 'manual_few_shot', 'manual_cot']]\n",
    "\n",
    "    # Output to LaTeX\n",
    "    avg_combined_data_metric_pivot.to_latex('../Output/avg_' + metric + '_pivot.tex', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Length\n",
    "\n",
    "Run generic function for length of entire interaction, length of all prompts, financial cost\n",
    "\n",
    "Baseline comparison ratios, change in accuracy/quality plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_table('conversation_length')\n",
    "means_table('input_length')\n",
    "means_table('conversation_cost')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot gsm8k_length_vs_provided in a bar chart\n",
    "# Limit to gsm8k data\n",
    "# Get means by model, method\n",
    "gsm8k_length_vs_provided_means = gsm8k_data[['model', 'method', 'gsm8k_length_vs_provided']].groupby(['model', 'method']).agg(['mean'])['gsm8k_length_vs_provided'].reset_index()\n",
    "\n",
    "# Bar chart by model, method\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x='model', y='mean', hue='method', data=gsm8k_length_vs_provided_means)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Average GSM8k Length vs. Provided Length')\n",
    "plt.title('Average GSM8k Length vs. Provided Length by Model and Method')\n",
    "plt.savefig('../Output/gsm8k_length_vs_provided_means.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot length_vs_direct_prompting in a bar chart\n",
    "# Get means by model, method, task\n",
    "length_vs_direct_prompting_means = combined_data[['model', 'method', 'task', 'length_vs_direct_prompting']].groupby(['model', 'method', 'task']).agg(['mean'])['length_vs_direct_prompting'].reset_index()\n",
    "\n",
    "# Bar chart by model, method, task\n",
    "\n",
    "# GSM8k plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x='model', y='mean', hue='method', data=length_vs_direct_prompting_means[length_vs_direct_prompting_means['task'] == 'gsm8k'])\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Average GSM8k Length vs. Direct Prompting Length')\n",
    "plt.title('Average GSM8k Length vs. Direct Prompting Length by Model and Method')\n",
    "plt.savefig('../Output/gsm8k_length_vs_direct_prompting_means.png')\n",
    "plt.show()\n",
    "\n",
    "# Creative writing plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x='model', y='mean', hue='method', data=length_vs_direct_prompting_means[length_vs_direct_prompting_means['task'] == 'cw'])\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Average Creative Writing Length vs. Direct Prompting Length')\n",
    "plt.title('Average Creative Writing Length vs. Direct Prompting Length by Model and Method')\n",
    "plt.savefig('../Output/cw_length_vs_direct_prompting_means.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute change in accuracy/quality per change in length (versus direct prompting)\n",
    "\n",
    "# In the direct_prompting_comparison table, get averages of conversation_length, dp_conversation_length, accuracy_quality, dp_accuracy_quality by model, method, task\n",
    "averages_for_changes = direct_prompting_comparison[['model', 'method', 'task', 'conversation_length', 'dp_conversation_length', 'accuracy_quality', 'dp_accuracy_quality']].groupby(['model', 'method', 'task']).agg(['mean']).reset_index()\n",
    "\n",
    "# Compute change in accuracy_quality per change in conversation_length\n",
    "# dp_accuracy_quality - accuracy_quality\n",
    "# divided by\n",
    "# dp_conversation_length - conversation_length\n",
    "averages_for_changes['change_in_accuracy_quality_per_change_in_conversation_length'] = (averages_for_changes['dp_accuracy_quality']['mean'] - averages_for_changes['accuracy_quality']['mean']) / (averages_for_changes['dp_conversation_length']['mean'] - averages_for_changes['conversation_length']['mean'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot change in accuracy_quality per change in conversation_length\n",
    "# Bar chart by model, method, task\n",
    "# GSM8k plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x='model', y='change_in_accuracy_quality_per_change_in_conversation_length', hue='method', data=averages_for_changes[averages_for_changes['task'] == 'gsm8k'])\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Change in Accuracy/Quality per Change in Conversation Length')\n",
    "plt.title('Change in Accuracy/Quality per Change in Conversation Length by Model and Method')\n",
    "plt.savefig('../Output/gsm8k_change_in_accuracy_quality_per_change_in_conversation_length.png')\n",
    "plt.show()\n",
    "\n",
    "# Creative writing plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x='model', y='change_in_accuracy_quality_per_change_in_conversation_length', hue='method', data=averages_for_changes[averages_for_changes['task'] == 'cw'])\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Change in Accuracy/Quality per Change in Conversation Length')\n",
    "plt.title('Change in Accuracy/Quality per Change in Conversation Length by Model and Method')\n",
    "plt.savefig('../Output/cw_change_in_accuracy_quality_per_change_in_conversation_length.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also create a table with the change in accuracy_quality per change in conversation_length\n",
    "\n",
    "# Sort and output table\n",
    "\n",
    "# Sort rows by task - gsm8k task first, then cw\n",
    "# Sort by model - text-davinci-003 first, then gpt4\n",
    "change_per_table = averages_for_changes.sort_values(by=['task', 'model'], ascending=[True, True])\n",
    "\n",
    "# Order columns: direct_prompting, zero_shot_cot, ape_zero_shot_cot, tree_of_thought, self_refine, least_to_most, manual_few_shot, manual_cot\n",
    "change_per_table = change_per_table[['model', 'task', 'direct_prompting', 'zero_shot_cot', 'ape_zero_shot_cot', 'tree_of_thought', 'self_refine', 'least_to_most', 'manual_few_shot', 'manual_cot']]\n",
    "\n",
    "# Output to LaTeX\n",
    "change_per_table.to_latex('../Output/change_per_table.tex', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complexity\n",
    "\n",
    "Run generic function for number of reasoning steps, sentence length, FRE\n",
    "\n",
    "Bar charts of differences\n",
    "\n",
    "Generic function for assessment of ease of review too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_table('num_linebreaks')\n",
    "means_table('num_sentences')\n",
    "means_table('num_step_i')\n",
    "means_table('num_1_dot_etc')\n",
    "means_table('sentence_length')\n",
    "means_table('fres')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differences of scores \n",
    "# Responses versus prompts\n",
    "# Responses versus provided answer for GSM8k\n",
    "\n",
    "# Variables: \n",
    "# 'num_linebreaks_prompts': num_linebreaks_prompts,\n",
    "# 'num_sentences_prompts': num_sentences_prompts,\n",
    "# 'num_step_i_prompts': num_step_i_prompts,\n",
    "# 'num_1_dot_etc_prompts': num_1_dot_etc_prompts,\n",
    "# 'sentence_length_prompts': sentence_length_prompts,\n",
    "# 'fres_prompts': fres_prompts,\n",
    "# 'num_linebreaks_provided': num_linebreaks_provided,\n",
    "# 'num_sentences_provided': num_sentences_provided,\n",
    "# 'num_step_i_provided': num_step_i_provided,\n",
    "# 'num_1_dot_etc_provided': num_1_dot_etc_provided\n",
    "# Loop over variables and create differences variables\n",
    "comparison_vars = ['num_linebreaks_prompts', 'num_sentences_prompts', 'num_step_i_prompts', 'num_1_dot_etc_prompts', 'sentence_length_prompts', 'fres_prompts', 'num_linebreaks_provided', 'num_sentences_provided', 'num_step_i_provided', 'num_1_dot_etc_provided']\n",
    "for var in comparison_vars:\n",
    "    # Create a variable that is the difference between the prompts and responses\n",
    "    # var + '_diff'\n",
    "    combined_data[var + '_diff'] = combined_data[var] - combined_data[var.replace('_prompts', '').replace('_provided', '')]\n",
    "\n",
    "# Aggregate \"_diff\" variables by model, task, method\n",
    "differences = combined_data[['model', 'task', 'method', 'num_linebreaks_prompts_diff', 'num_sentences_prompts_diff', 'num_step_i_prompts_diff', 'num_1_dot_etc_prompts_diff', 'sentence_length_prompts_diff', 'fres_prompts_diff', 'num_linebreaks_provided_diff', 'num_sentences_provided_diff', 'num_step_i_provided_diff', 'num_1_dot_etc_provided_diff']].groupby(['model', 'task', 'method']).agg(['mean']).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots of _diff variables by model, task, method\n",
    "\n",
    "# GSM8k plots\n",
    "# Loop over variables\n",
    "comparison_vars_gsm8k = ['num_linebreaks_prompts_diff', 'num_sentences_prompts_diff', 'num_step_i_prompts_diff', 'num_1_dot_etc_prompts_diff', 'num_linebreaks_provided_diff', 'num_sentences_provided_diff', 'num_step_i_provided_diff', 'num_1_dot_etc_provided_diff']\n",
    "for var in comparison_vars_gsm8k:\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(x='model', y=var, hue='method', data=differences[differences['task'] == 'gsm8k'])\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Average ' + var + ' Difference')\n",
    "    plt.title('Average ' + var + ' Difference by Model and Method')\n",
    "    plt.savefig('../Output/gsm8k_' + var + '.png')\n",
    "    plt.show()\n",
    "\n",
    "# Creative writing plots\n",
    "# Loop over variables\n",
    "comparison_vars_cw = ['num_linebreaks_prompts_diff', 'num_sentences_prompts_diff', 'num_step_i_prompts_diff', 'num_1_dot_etc_prompts_diff', 'sentence_length_prompts_diff', 'fres_prompts_diff']\n",
    "for var in comparison_vars_cw:\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(x='model', y=var, hue='method', data=differences[differences['task'] == 'cw'])\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Average ' + var + ' Difference')\n",
    "    plt.title('Average ' + var + ' Difference by Model and Method')\n",
    "    plt.savefig('../Output/cw_' + var + '.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_table('ease_of_evaluation_score')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
