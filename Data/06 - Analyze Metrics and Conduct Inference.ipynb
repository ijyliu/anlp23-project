{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Metrics and Conduct Inference.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.stats.contingency_tables import mcnemar \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Combined_Data.xlsx\n",
    "combined_data = pd.read_excel('Combined_Data.xlsx')\n",
    "\n",
    "# GSM8k data - limit to task = \"gsm8k\"\n",
    "gsm8k_data = combined_data[combined_data['task'] == 'gsm8k']\n",
    "\n",
    "# Creative writing data - limit to task = \"cw\"\n",
    "cw_data = combined_data[combined_data['task'] == 'cw']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy/Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce table - one column for each method, one row for each model by task type\n",
    "# Average values of correct_or_incorrect and creative_writing_score\n",
    "\n",
    "# Column for combined accuracy_quality - correct_or_incorrect if task = \"gsm8k\", creative_writing_score if task = \"cw\"\n",
    "combined_data['accuracy_quality'] = combined_data.apply(lambda row: row['correct_or_incorrect'] if row['task'] == 'gsm8k' else row['creative_writing_score'], axis=1)\n",
    "\n",
    "# Average accuracy_quality by model, method, task\n",
    "avg_accuracy_quality = combined_data[['model', 'method', 'task', 'accuracy_quality']].groupby(['model', 'method', 'task']).mean()['accuracy_quality'].reset_index()\n",
    "\n",
    "# Pivot table - column method should go wide\n",
    "avg_accuracy_quality_pivot = avg_accuracy_quality.pivot_table(index=['model', 'task'], columns='method', values='accuracy_quality').reset_index()\n",
    "\n",
    "# Sort rows by task - gsm8k task first, then cw\n",
    "# Sort by model - text-davinci-003 first, then gpt4\n",
    "avg_accuracy_quality_pivot = avg_accuracy_quality_pivot.sort_values(by=['task', 'model'], ascending=[True, True])\n",
    "\n",
    "# Order columns: direct_prompting, zero_shot_cot, ape_zero_shot_cot, tree_of_thought, self_refine, least_to_most, manual_few_shot, manual_cot\n",
    "avg_accuracy_quality_pivot = avg_accuracy_quality_pivot[['model', 'task', 'direct_prompting', 'zero_shot_cot', 'ape_zero_shot_cot', 'tree_of_thought', 'self_refine', 'least_to_most', 'manual_few_shot', 'manual_cot']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform McNemar's Test on GSM8k data\n",
    "\n",
    "# Get in terms of number of questions answered correctly: sum accuracy_quality column by model, method, task\n",
    "gsm8k_num_correct= gsm8k_data[['model', 'method', 'task', 'accuracy_quality']].groupby(['model', 'method', 'task']).sum()['accuracy_quality'].reset_index()\n",
    "# Rename column to num_correct\n",
    "gsm8k_num_correct = gsm8k_num_correct.rename(columns={'accuracy_quality': 'num_correct'})\n",
    "\n",
    "# Make another table with counts of questions missed (number of observations minus accuracy_quality sum) by model, method, task\n",
    "gsm8k_num_obs = gsm8k_data[['model', 'method', 'task', 'accuracy_quality']].groupby(['model', 'method', 'task']).count()['accuracy_quality'].reset_index()\n",
    "# Rename column to num_obs\n",
    "gsm8k_num_obs = gsm8k_num_obs.rename(columns={'accuracy_quality': 'num_obs'})\n",
    "# Join together by model, method, task\n",
    "gsm8k_correct_incorrect_obs = gsm8k_num_correct.merge(gsm8k_num_obs, on=['model', 'method', 'task'], how='left')\n",
    "# Table should have model, method, task, num_correct, num_incorrect, num_obs\n",
    "gsm8k_correct_incorrect_obs['num_incorrect'] = gsm8k_correct_incorrect_obs['num_obs'] - gsm8k_correct_incorrect_obs['num_correct']\n",
    "\n",
    "# Iterate over each column in gsm8k_correct_obs except for model, task, direct_prompting\n",
    "# Get column and direct prompting column\n",
    "\n",
    "# Create a dataset \n",
    "data = [[30, 20], \n",
    "        [10, 40]] \n",
    "  \n",
    "# McNemar's Test without any continuity correction \n",
    "print(mcnemar(data, exact=False)) \n",
    "  \n",
    "# McNemar's Test with the continuity correction \n",
    "print(mcnemar(data, exact=False, correction=False)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform paired t-test on creative writing data and all other metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find variance of correct_incorrect column by model, method, task\n",
    "\n",
    "# Find variance of creative_writing_score column by model, method, task\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
