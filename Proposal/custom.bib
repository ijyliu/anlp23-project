
@misc{openai_gpt-4_2023,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	urldate = {2023-09-25},
	publisher = {arXiv},
	author = {OpenAI},
	month = mar,
	year = {2023},
	note = {arXiv:2303.08774 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 100 pages},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\ZLQQGQUP\\2303.html:text/html;Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\LM72BFL3\\OpenAI - 2023 - GPT-4 Technical Report.pdf:application/pdf},
}

@misc{cheng_is_2023,
	title = {Is {GPT}-4 a {Good} {Data} {Analyst}?},
	url = {http://arxiv.org/abs/2305.15038},
	abstract = {As large language models (LLMs) have demonstrated their powerful capabilities in plenty of domains and tasks, including context understanding, code generation, language generation, data storytelling, etc., many data analysts may raise concerns if their jobs will be replaced by AI. This controversial topic has drawn a lot of attention in public. However, we are still at a stage of divergent opinions without any definitive conclusion. Motivated by this, we raise the research question of "is GPT-4 a good data analyst?" in this work and aim to answer it by conducting head-to-head comparative studies. In detail, we regard GPT-4 as a data analyst to perform end-to-end data analysis with databases from a wide range of domains. We propose a framework to tackle the problems by carefully designing the prompts for GPT-4 to conduct experiments. We also design several task-specific evaluation metrics to systematically compare the performance between several professional human data analysts and GPT-4. Experimental results show that GPT-4 can achieve comparable performance to humans. We also provide in-depth discussions about our results to shed light on further studies before we reach the conclusion that GPT-4 can replace data analysts.},
	urldate = {2023-09-25},
	publisher = {arXiv},
	author = {Cheng, Liying and Li, Xingxuan and Bing, Lidong},
	month = may,
	year = {2023},
	note = {arXiv:2305.15038 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 11 pages, 2 figures},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\4N77GWHJ\\2305.html:text/html;Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\QDTTIL6T\\Cheng et al. - 2023 - Is GPT-4 a Good Data Analyst.pdf:application/pdf},
}

@article{roose_conversation_2023,
	chapter = {Technology},
	title = {A {Conversation} {With} {Bing}’s {Chatbot} {Left} {Me} {Deeply} {Unsettled}},
	issn = {0362-4331},
	url = {https://www.nytimes.com/2023/02/16/technology/bing-chatbot-microsoft-chatgpt.html},
	abstract = {A very strange conversation with the chatbot built into Microsoft’s search engine led to it declaring its love for me.},
	language = {en-US},
	urldate = {2023-09-25},
	journal = {The New York Times},
	author = {Roose, Kevin},
	month = feb,
	year = {2023},
	keywords = {Artificial Intelligence, ChatGPT, Computers and the Internet, Conversation, internal-sub-only, Microsoft Corp, OpenAI Labs, Search Engines},
	file = {Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\XQCJKDEG\\bing-chatbot-microsoft-chatgpt.html:text/html},
}

@misc{ethan_mollick_emollick_i_2023,
	type = {Tweet},
	title = {I have a strong suspicion that “prompt engineering” is not going to be a big deal in the long-term \& prompt engineer is not the job of the future {AI} gets easier. {You} can already see in {Midjourney} how basic prompts went from complex in v3 to easy in v4. {Same} with {ChatGPT} to {Bing}. https://t.co/{BTtSN4oVF4}},
	url = {https://twitter.com/emollick/status/1627804798224580608},
	language = {en},
	urldate = {2023-09-26},
	journal = {Twitter},
	author = {{Ethan Mollick [@emollick]}},
	month = feb,
	year = {2023},
	file = {Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\EUFKYTRM\\1627804798224580608.html:text/html},
}

@inproceedings{wu_ai_2022,
	address = {New Orleans LA USA},
	title = {{AI} {Chains}: {Transparent} and {Controllable} {Human}-{AI} {Interaction} by {Chaining} {Large} {Language} {Model} {Prompts}},
	isbn = {978-1-4503-9157-3},
	shorttitle = {{AI} {Chains}},
	url = {https://dl.acm.org/doi/10.1145/3491102.3517582},
	doi = {10.1145/3491102.3517582},
	language = {en},
	urldate = {2023-09-26},
	booktitle = {{CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Wu, Tongshuang and Terry, Michael and Cai, Carrie Jun},
	month = apr,
	year = {2022},
	pages = {1--22},
	file = {Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\G3NM64QK\\Wu et al. - 2022 - AI Chains Transparent and Controllable Human-AI I.pdf:application/pdf},
}

@article{acar_ai_2023,
	title = {{AI} {Prompt} {Engineering} {Isn}’t the {Future}},
	issn = {0017-8012},
	url = {https://hbr.org/2023/06/ai-prompt-engineering-isnt-the-future},
	abstract = {Despite the buzz surrounding it, the prominence of prompt engineering may be fleeting. A more enduring and adaptable skill will keep enabling us to harness the potential of generative AI? It is called problem formulation — the ability to identify, analyze, and delineate problems.},
	urldate = {2023-09-26},
	journal = {Harvard Business Review},
	author = {Acar, Oguz A.},
	month = jun,
	year = {2023},
	note = {Section: Technology and analytics},
	keywords = {AI and machine learning, Algorithms, Analytics and data science, Automation, Cybersecurity and digital privacy, Data management, Enterprise computing, Information management, Technology and analytics, Web-based technologies},
	file = {Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\I72G55WX\\ai-prompt-engineering-isnt-the-future.html:text/html},
}

@misc{diao_active_2023,
	title = {Active {Prompting} with {Chain}-of-{Thought} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2302.12246},
	abstract = {The increasing scale of large language models (LLMs) brings emergent abilities to various complex tasks requiring reasoning, such as arithmetic and commonsense reasoning. It is known that the effective design of task-specific prompts is critical for LLMs' ability to produce high-quality answers. In particular, an effective approach for complex question-and-answer tasks is example-based prompting with chain-of-thought (CoT) reasoning, which significantly improves the performance of LLMs. However, current CoT methods rely on a fixed set of human-annotated exemplars, which are not necessarily the most effective examples for different tasks. This paper proposes a new method, Active-Prompt, to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning). For this purpose, we propose a solution to the key problem of determining which questions are the most important and helpful ones to annotate from a pool of task-specific queries. By borrowing ideas from the related problem of uncertainty-based active learning, we introduce several metrics to characterize the uncertainty so as to select the most uncertain questions for annotation. Experimental results demonstrate the superiority of our proposed method, achieving state-of-the-art on eight complex reasoning tasks. Further analyses of different uncertainty metrics, pool sizes, zero-shot learning, and accuracy-uncertainty relationship demonstrate the effectiveness of our method. Our code will be available at https://github.com/shizhediao/active-prompt.},
	urldate = {2023-09-26},
	publisher = {arXiv},
	author = {Diao, Shizhe and Wang, Pengcheng and Lin, Yong and Zhang, Tong},
	month = may,
	year = {2023},
	note = {arXiv:2302.12246 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 20 pages, 3 figures, 11 tables},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\WSCU8EX6\\2302.html:text/html;Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\EZGKACIN\\Diao et al. - 2023 - Active Prompting with Chain-of-Thought for Large L.pdf:application/pdf},
}

@misc{noauthor_promptbase_nodate,
	title = {{PromptBase}},
	shorttitle = {{PromptBase} {\textbar} {Prompt} {Marketplace}},
	url = {https://promptbase.com},
	abstract = {Search 100,000+ quality AI prompts from top prompt engineers. Produce better outputs, save on time \& API costs, sell your own prompts.},
	language = {en},
	urldate = {2023-09-26},
}

@misc{hebenstreit_automatically_2023,
	title = {An automatically discovered chain-of-thought prompt generalizes to novel models and datasets},
	url = {http://arxiv.org/abs/2305.02897},
	abstract = {Emergent chain-of-thought (CoT) reasoning capabilities promise to improve performance and explainability of large language models (LLMs). However, uncertainties remain about how reasoning strategies formulated for previous model generations generalize to new model generations and different datasets. In this small-scale study, we compare different reasoning strategies induced by zero-shot prompting across six recently released LLMs (davinci-002, davinci-003, GPT-3.5-turbo, GPT-4, Flan-T5-xxl and Cohere command-xlarge) on a mixture of six question-answering datasets, including datasets from scientific and medical domains. Our findings demonstrate that while some variations in effectiveness occur, gains from CoT reasoning strategies remain robust across different models and datasets. GPT-4 has the most benefit from current state-of-the-art reasoning strategies and exhibits the best performance by applying a prompt previously discovered through automated discovery.},
	urldate = {2023-09-26},
	publisher = {arXiv},
	author = {Hebenstreit, Konstantin and Praas, Robert and Kiesewetter, Louis P. and Samwald, Matthias},
	month = aug,
	year = {2023},
	note = {arXiv:2305.02897 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\EZIPW94R\\2305.html:text/html;Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\9DQ7M9VQ\\Hebenstreit et al. - 2023 - An automatically discovered chain-of-thought promp.pdf:application/pdf},
}

@article{wei_chain--thought_nodate,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	abstract = {We explore how generating a chain of thought—a series of intermediate reasoning steps—signiﬁcantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufﬁciently large language models via a simple method called chain-ofthought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even ﬁnetuned GPT-3 with a veriﬁer.},
	language = {en},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed H and Le, Quoc V and Zhou, Denny},
	file = {Wei et al. - Chain-of-Thought Prompting Elicits Reasoning in La.pdf:C\:\\Users\\ijyli\\Zotero\\storage\\8AG6AWPT\\Wei et al. - Chain-of-Thought Prompting Elicits Reasoning in La.pdf:application/pdf},
}

@misc{zhang_automatic_2022,
	title = {Automatic {Chain} of {Thought} {Prompting} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2210.03493},
	abstract = {Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like "Let's think step by step" to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the "Let's think step by step" prompt to generate reasoning chains for demonstrations one by one, i.e., let's think not just step by step, but also one by one. However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at https://github.com/amazon-research/auto-cot},
	urldate = {2023-09-26},
	publisher = {arXiv},
	author = {Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Smola, Alex},
	month = oct,
	year = {2022},
	note = {arXiv:2210.03493 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\8TAIA3HA\\2210.html:text/html;Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\BREVBII4\\Zhang et al. - 2022 - Automatic Chain of Thought Prompting in Large Lang.pdf:application/pdf},
}

@misc{dhuliawala_chain--verification_2023,
	title = {Chain-of-{Verification} {Reduces} {Hallucination} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2309.11495},
	abstract = {Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. We study the ability of language models to deliberate on the responses they give in order to correct their mistakes. We develop the Chain-of-Verification (CoVe) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. In experiments, we show CoVe decreases hallucinations across a variety of tasks, from list-based questions from Wikidata, closed book MultiSpanQA and longform text generation.},
	urldate = {2023-09-26},
	publisher = {arXiv},
	author = {Dhuliawala, Shehzaad and Komeili, Mojtaba and Xu, Jing and Raileanu, Roberta and Li, Xian and Celikyilmaz, Asli and Weston, Jason},
	month = sep,
	year = {2023},
	note = {arXiv:2309.11495 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\6HVW7SMC\\2309.html:text/html},
}

@misc{liu_generated_2022,
	title = {Generated {Knowledge} {Prompting} for {Commonsense} {Reasoning}},
	url = {http://arxiv.org/abs/2110.08387},
	abstract = {It remains an open question whether incorporating external knowledge benefits commonsense reasoning while maintaining the flexibility of pretrained sequence models. To investigate this question, we develop generated knowledge prompting, which consists of generating knowledge from a language model, then providing the knowledge as additional input when answering a question. Our method does not require task-specific supervision for knowledge integration, or access to a structured knowledge base, yet it improves performance of large-scale, state-of-the-art models on four commonsense reasoning tasks, achieving state-of-the-art results on numerical commonsense (NumerSense), general commonsense (CommonsenseQA 2.0), and scientific commonsense (QASC) benchmarks. Generated knowledge prompting highlights large-scale language models as flexible sources of external knowledge for improving commonsense reasoning. Our code is available at https://github.com/liujch1998/GKP},
	urldate = {2023-09-26},
	publisher = {arXiv},
	author = {Liu, Jiacheng and Liu, Alisa and Lu, Ximing and Welleck, Sean and West, Peter and Bras, Ronan Le and Choi, Yejin and Hajishirzi, Hannaneh},
	month = sep,
	year = {2022},
	note = {arXiv:2110.08387 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: ACL 2022 main conference},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\QWF7MR96\\2110.html:text/html;Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\74ILI65I\\Liu et al. - 2022 - Generated Knowledge Prompting for Commonsense Reas.pdf:application/pdf},
}

@misc{hulbert_using_2023,
	title = {Using {Tree}-of-{Thought} {Prompting} to boost {ChatGPT}'s reasoning},
	copyright = {MIT},
	url = {https://github.com/dave1010/tree-of-thought-prompting},
	abstract = {Using Tree-of-Thought Prompting to boost ChatGPT's reasoning},
	urldate = {2023-09-26},
	author = {Hulbert, Dave},
	month = sep,
	year = {2023},
	note = {original-date: 2023-05-22T19:03:27Z},
}

@misc{shackell_prompt_2023,
	title = {Prompt engineering: is being an {AI} 'whisperer' the job of the future or a short-lived fad?},
	shorttitle = {Prompt engineering},
	url = {http://theconversation.com/prompt-engineering-is-being-an-ai-whisperer-the-job-of-the-future-or-a-short-lived-fad-211833},
	abstract = {Media articles and influencers have helped give the impression that prompt engineering could be a ticket to a six-figure salary. The reality, as always, is a different story.},
	language = {en},
	urldate = {2023-09-26},
	journal = {The Conversation},
	author = {Shackell, Cameron},
	month = aug,
	year = {2023},
	file = {Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\A6IN6CYE\\prompt-engineering-is-being-an-ai-whisperer-the-job-of-the-future-or-a-short-lived-fad-211833.html:text/html},
}

@misc{martineau_what_2021,
	title = {What is prompt tuning?},
	copyright = {© Copyright IBM Corp. 2021},
	url = {https://research.ibm.com/blog/what-is-ai-prompt-tuning},
	abstract = {It's an efficient, low-cost way of adapting a foundation model to new tasks without retraining the model.},
	language = {en-US},
	urldate = {2023-09-26},
	journal = {IBM Research Blog},
	author = {Martineau, Kim},
	month = feb,
	year = {2021},
	file = {Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\TLSRX94R\\what-is-ai-prompt-tuning.html:text/html},
}

@misc{mann_microsoft_nodate,
	title = {Microsoft limits {Bing} chat exchanges and conversation lengths after 'creepy' interactions with some users},
	url = {https://www.businessinsider.com/microsoft-limits-bing-chat-exchanges-and-conversation-lengths-2023-2},
	abstract = {Bing users will get a prompt to start a new topic once they hit the new limits imposed with the search engine's AI chatbot, Microsoft said.},
	language = {en-US},
	urldate = {2023-09-26},
	journal = {Business Insider},
	author = {Mann, Jyoti},
	file = {Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\MFSIBBQJ\\microsoft-limits-bing-chat-exchanges-and-conversation-lengths-2023-2.html:text/html},
}

@inproceedings{zhou_large_2022,
	title = {Large {Language} {Models} are {Human}-{Level} {Prompt} {Engineers}},
	url = {https://openreview.net/forum?id=92gvk82DE-},
	abstract = {By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the "program," optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 21/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts.},
	language = {en},
	urldate = {2023-09-26},
	author = {Zhou, Yongchao and Muresanu, Andrei Ioan and Han, Ziwen and Paster, Keiran and Pitis, Silviu and Chan, Harris and Ba, Jimmy},
	month = sep,
	year = {2022},
	file = {Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\K2UC5R8B\\Zhou et al. - 2022 - Large Language Models are Human-Level Prompt Engin.pdf:application/pdf},
}
