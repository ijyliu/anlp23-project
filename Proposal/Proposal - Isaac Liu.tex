\pdfoutput=1
\documentclass[11pt]{article}
% Remove the "review" option to generate the final version.
\usepackage[review]{ACL2023}
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}

\title{Project Proposal: The Cost of Prompt Engineering}

\author{Isaac Liu \\
  University of California, Berkeley \\
  \texttt{ijyliu@berkeley.edu}}

\begin{document}
\maketitle
\begin{abstract}
What are the attributes of text generated by large language models, and which ones make them useful?
\end{abstract}

\section{Introduction}

Prompt engineering, the practice of developing specialized prompts and queries to improve the accuracy of Large Language Models is a growing topic of interest in the NLP community, and among the general public. It is not, however, without its critics. Some commentators believe that the practice will become irrelevant as models grow larger and more powerful, becoming more capable of directly interpreting a user's intent. (https://x.com/emollick/status/1627804798224580608?s=20). OTHERS QUESTION THE NEED FOR SPECIALIZED PROFESSIONALS AND TRAINING TO ATTAIN MINIMAL IMPROVEMENTS WHICH ARE NOT REPEATABLE ACROSS DOMAIN TYPES AND CONTEXTS (https://hbr.org/2023/06/ai-prompt-engineering-isnt-the-future). 

Despite such criticism, it is difficult to find empirical and quantitative analyses of the tradeoff between costs and accuracy benefits associated with advanced prompting. Papers introducing new prompting techniques often only include performance benchmarks concerning the techniques' efficacy, typically within a limited question domain. This paper proposes several metrics to evaluate the costs of prompt engineering, and analyzes the tradeoffs inherent in their application to standardized data.

Such an assessment is valuable on several dimensions. Beyond testing the practicability of prompt engineering as a whole, it can be used to compare the performance of different methods, in a world where so many competing techniques are available. 

Motivation
Roose conversation - length can lead to significant deterioration in quality. Microsoft limited converation/response length for Bing Chat afterwards.
https://blog.biocomm.ai/wp-content/uploads/2023/04/Kevin-Rooses-Conversation-With-Bings-Chatbot-Full-Transcript-The-New-York-Times-2.pdf

First assessment of some of these techniques since launch, when underlying LLMs have changed, and the costs with them.

Some light discussion of human costs... to advocate for an automated prompt engineering solution: https://arxiv.org/pdf/2302.12246.pdf

https://dl.acm.org/doi/pdf/10.1145/3491102.3517582 discusses chaining and some of the cost challenges - increased complexity, limitations on creativity/randomness

Costs are generally in terms of length and complexity.
Length imposes several costs. Potential for quality to decline, see Roose. Extra time to review output and check each step in real world scenarios, when some steps may be obvious to human reviewers. Token costs.

\section{Prompting Methods Assessed}

Zero-Shot Baseline
Few-Shot Prompting. On page 4, we see a comparison of the returns to in-context learning by model size https://arxiv.org/pdf/2005.14165.pdf. But this paper is from a time of smaller models (and indeed I don't think the newest GPT-4 tech report even considers few-shot/chain-of-thought)! Also, here we see that the correctness of examples is not important - instead, info about the distribution of the label space, input text dist, output format is key https://arxiv.org/pdf/2202.12837.pdf.
Manually provided chain-of-thought prompting. It was at one time claimed that this method could produce superior performance https://arxiv.org/pdf/2210.03493.pdf. Note some of these advanced LMs automatically do CoT
Chain of verification prompting: https://arxiv.org/pdf/2309.11495.pdf
Finish going through repo and also look up most popular techniques online...
Generated Knowledge Prompting
Directional Stimulus Prompting
https://en.wikipedia.org/wiki/Prompt_engineering

I even chose relatively easy to implement/low cost methods!

\section{Metrics}

Accuracy at the point a human is confident in the answer (modelling the real world)

Accuracy (according to the papers initially presenting the technique, in whatever domain it was tested)

Length of the entire interaction in tokens

Length of the entire interaction in tokens relative to the length of a human/solved out/generally accepted as correct answer. How much is the engineering stretching the interaction out? Is this stretching adding value/improving accuracy?

Length of the entire interaction in time (seconds)

Cost of this length

Human assessment of need for specialized knowledge/difficulty of implementation of the technique. This could be task specific (done for some real world example questions/prompting scenarios), or it could be done overall based on a simple description of the technique. Perhaps a balance of both is best.

Human assessment of output complexity (ease of evaluating results). This could be task specific (done for some real world example questions/prompting scenarios), or it could be done overall based on a simple description of the technique. Perhaps a balance of both is best.

\section{Data}

Probably real-world/professional, rather than academic tasks. Coding/Leetcode, then perhaps some very generalized standard tests (All GRE Sections)

Models: the few major ones: GPT-4, Claude, Bard. If time allows, assess on GPT-3.5 or older models closer to the original ones that motivated these prompt engineering techniques.

For some of the simpler metrics, in limited domains, it may be possible to use reported accuracy scores on the domain dataset coming from the original paper. It may also be possible to use prompts, LLM responses, and correct responses provided along with original papers. However, the more complex metrics will require human evaluation.

\section{Responsiblities}

I am the only author of this proposal, but I am open to working with others with similar interests. Expanding the group would be helpful in order to share the workload of data generation.

\section*{Limitations}

Hard to tell what methods are actually the most popular.

I expect things to change over time... in fact, they have changed since many of these techniques came on the scene. However, I only expect relative costs to increase as models get better.

Automated prompt engineering can certainly decrease at least some of the costs. https://arxiv.org/pdf/2211.01910.pdf. We did not consider it because of the difficulty of implementation and limited resources available.

\section*{Acknowledgements}
The template for this document was adapted by Jordan Boyd-Graber, Naoaki Okazaki, and Anna Rogers.

% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\end{document}
