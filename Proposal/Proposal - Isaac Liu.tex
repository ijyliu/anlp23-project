\pdfoutput=1
\documentclass[11pt]{article}
% Remove the "review" option to generate the final version.
\usepackage[review]{ACL2023}
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}

\title{Project Proposal: Measuring LLM Responses}

\author{Isaac Liu \\
  University of California, Berkeley \\
  \texttt{ijyliu@berkeley.edu}}

\begin{document}
\maketitle
\begin{abstract}
What are the attributes of text generated by large language models, and which ones make them useful?
\end{abstract}

\section{Introduction}

Speed can be measured in terms of time to produce a complete response or in terms of time per token.

Motivation
Roose conversation - length can lead to significant deterioration in quality. Microsoft limited converation/response length for Bing Chat afterwards.
https://blog.biocomm.ai/wp-content/uploads/2023/04/Kevin-Rooses-Conversation-With-Bings-Chatbot-Full-Transcript-The-New-York-Times-2.pdf

\section{Literature Review}

Speed: 

https://arxiv.org/pdf/2305.15038.pdf provides some speed benchmarks on data analysis tasks in Table 4 on Page 6. It doesn't really discuss methodology, though.

Context window

\section{Data}

For some questions, new data must be collected.

Annotation:
Accuracy
For missed/incorrect answers, location of the error

For other questions, pre-existing sources of LLM prompts and responses can be used.

Scope of Data:

Going for the same broad set of fields in the original GPT-4 technical paper. https://arxiv.org/pdf/2303.08774.pdf

Academic - General (not Subject-Specific) Standardized tests (All GRE Sections)
Coding - Leetcode Easy, Medium, Hard
Other Common NLP Benchmarks

Models:

GPT-4
Output maximum length is 24000 tokens. https://www.preprints.org/manuscript/202303.0422/v1
Claude
Bard

Consider the extent of repetition of the same prompt to the same model.

\section{Responsiblities}

I am the only author of this proposal, but I am open to working with others with similar interests.

\section*{Limitations}

The actual length of time required by an online LLM to answer a question is difficult to estimate given the presence of confounding factors such as connection speed and server load. To limit associated variation, responses were generated during off-peak hours.

\section*{Acknowledgements}
The template for this document was adapted by Jordan Boyd-Graber, Naoaki Okazaki, and Anna Rogers.

% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\end{document}
