Notes:

1. I devoted significant effort to carefully selecting prompt engineering methods, as this was a major feedback item from the initial proposal. The appendix table proxying popularity using Semantic Scholar citations is an attempt to adapt a data-driven approach.
2. I have changed the evaluation away from GRE questions as data contamination is rampant online - nearly every prep question I could find had been discussed on online forums, often with no dating so as to be able to tell if the forum page was included in OpenAI's training data. The intial GPT-4 technical report uses GRE questions - but OpenAI, having the actual training data, was able to perform string searches to test for contamination. A cloze-style test as in Chang, 2023 might be possible, but only for a very limited number of questions https://www.ets.org/pdfs/gre/gre-sample-questions.pdf.
3. I have downloaded and organized task questions and written out prompts for each method (see Appendix). I tested/optimized conversations using these prompts on a few questions in order to make sure the models are capable of following the instructions within them - including setting up and testing code and logic for requests via the OpenAI API. I've held off on collecting all the data in order to receive feedback, as I expect the data collection to incur financial cost (albeit low).
4. I've run all metrics for one single conversation.
