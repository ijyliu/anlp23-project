Notes:

1. I devoted significant effort to carefully selecting prompt engineering methods, as this was a major feedback item from the initial proposal. The appendix table proxying popularity using Semantic Scholar citations is an attempt to adopt a data-driven approach.
2. I have changed the evaluation away from GRE questions as data contamination is rampant online - nearly every prep question I could find had been discussed on online forums, often with no dating so as to be able to tell if the forum page was included in OpenAI's training data. The intial GPT-4 technical report uses GRE questions - but OpenAI, having the actual training data, was able to perform string searches to test for contamination. A cloze-style test as in Chang, 2023 might be possible, but only for a very limited number of questions https://www.ets.org/pdfs/gre/gre-sample-questions.pdf.
3. I have downloaded and organized task questions and written out prompts for each method (see Appendix). I tested/optimized conversations using these prompts on a few questions in order to make sure the models are capable of following the instructions within them - including setting up and testing code and logic for requests via the OpenAI API. I've held off on collecting all the data in order to receive feedback, as I expect the data collection to incur financial cost (albeit low). I've also run all metrics for a few conversations with each model and task type and selected and optimized them accordingly.
4. Grading will begin with the 100 creative writing questions, then proceed to GSM8K questions. There are 1,000+ test questions - I will begin with a random sample of 100 and then proceed to add more if time allows and if there are benefits in terms of statistical power.
