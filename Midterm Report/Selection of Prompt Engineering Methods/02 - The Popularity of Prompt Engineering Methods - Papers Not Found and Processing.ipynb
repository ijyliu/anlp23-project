{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Popularity of Prompt Engineering Methods\n",
    "\n",
    "This series of notebooks produces statistics on Semantic Scholar citations per day for all of the prompt engineering approaches listed at \"https://www.promptingguide.ai/papers\", \"https://en.wikipedia.org/wiki/Prompt_engineering#Text-to-text\", and the citations section for \"The Practicality of Prompt Engineering\".\n",
    "\n",
    "This file loads in information on papers not initially found and then completes processing of the citation statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-22 13:25:07.547878\n"
     ]
    }
   ],
   "source": [
    "# Print current datetime/run as of date\n",
    "import datetime\n",
    "print(datetime.datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper title</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Structured Prompting: Scaling In-Context Learning to 1,000 Examples</td>\n",
       "      <td>Structured Prompting: Scaling In Context Learning to 1,000 Examples</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Using Tree-of-Thought Prompting to boost ChatGPT's reasoning</td>\n",
       "      <td>Using Tree of Thought Prompting to boost ChatGPT's reasoning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           paper title  \\\n",
       "0  Structured Prompting: Scaling In-Context Learning to 1,000 Examples   \n",
       "1         Using Tree-of-Thought Prompting to boost ChatGPT's reasoning   \n",
       "\n",
       "                                                                 query  \n",
       "0  Structured Prompting: Scaling In Context Learning to 1,000 Examples  \n",
       "1         Using Tree of Thought Prompting to boost ChatGPT's reasoning  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in \"No Results Semantic Scholar - First Pass.xlsx\"\n",
    "no_results = pd.read_excel('No Results Semantic Scholar - First Pass.xlsx', sheet_name='Sheet1')\n",
    "\n",
    "# Print all dataframe rows and full width of columns\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "no_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Structured Prompting: Scaling In-Context Learning to 1, 000 Examples']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manual corrections:\n",
    "# The paper titles on Semantic Scholar are:\n",
    "# \"Structured Prompting: Scaling In-Context Learning to 1, 000 Examples\"\n",
    "\n",
    "# We can omit \"Using Tree-of-Thought Prompting to boost ChatGPT's reasoning\" because this is actually a blog post\n",
    "\n",
    "corrected_paper_titles = [\"Structured Prompting: Scaling In-Context Learning to 1, 000 Examples\"]\n",
    "\n",
    "corrected_paper_titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get semantic scholar citations\n",
    "\n",
    "# For sleep to avoid API limit\n",
    "import time\n",
    "# Import other packages needed\n",
    "import requests\n",
    "from datetime import date\n",
    "\n",
    "# Semantic scholar dataframe\n",
    "semantic_scholar_df_corrections = pd.DataFrame()\n",
    "no_results_df_corrections = pd.DataFrame()\n",
    "\n",
    "# Today's date\n",
    "today = date.today()\n",
    "\n",
    "# Loop over papers\n",
    "for paper_title in corrected_paper_titles:\n",
    "    # Replace hyphens with a space (per documentation)\n",
    "    query = paper_title.replace(\"-\", \" \")\n",
    "    # Query semantic scholar\n",
    "    r = requests.get(\n",
    "    'https://api.semanticscholar.org/graph/v1/paper/search?query=' + query + '&fields=title,citationCount,publicationDate,year&limit=1'\n",
    "    )\n",
    "    # Attempt for a returned result\n",
    "    try:\n",
    "        semantic_scholar_title = r.json()['data'][0]['title']\n",
    "        semantic_scholar_citation_count = r.json()['data'][0]['citationCount']\n",
    "        ss_publication_date = r.json()['data'][0]['publicationDate']\n",
    "        ss_year = r.json()['data'][0]['year']\n",
    "        new_record = pd.DataFrame([{\"paper title\":paper_title, \"semantic scholar title\": semantic_scholar_title, \"ss_publication_date\": ss_publication_date, \"ss_year\": ss_year, \"citation_count\": semantic_scholar_citation_count, \"query\": query, \"day_queried\": today}])\n",
    "        semantic_scholar_df_corrections = pd.concat([semantic_scholar_df_corrections, new_record], ignore_index=True)\n",
    "    # Error catch for no results\n",
    "    except:\n",
    "        new_record = pd.DataFrame([{\"paper title\":paper_title, \"query\": query}])\n",
    "        no_results_df_corrections = pd.concat([no_results_df_corrections, new_record], ignore_index=True)\n",
    "    # Wait one second to avoid exceeding API limit\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper title</th>\n",
       "      <th>semantic scholar title</th>\n",
       "      <th>ss_publication_date</th>\n",
       "      <th>ss_year</th>\n",
       "      <th>citation_count</th>\n",
       "      <th>query</th>\n",
       "      <th>day_queried</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Structured Prompting: Scaling In-Context Learning to 1, 000 Examples</td>\n",
       "      <td>Structured Prompting: Scaling In-Context Learning to 1, 000 Examples</td>\n",
       "      <td>2022-12-13</td>\n",
       "      <td>2022</td>\n",
       "      <td>20</td>\n",
       "      <td>Structured Prompting: Scaling In Context Learning to 1, 000 Examples</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            paper title  \\\n",
       "0  Structured Prompting: Scaling In-Context Learning to 1, 000 Examples   \n",
       "\n",
       "                                                 semantic scholar title  \\\n",
       "0  Structured Prompting: Scaling In-Context Learning to 1, 000 Examples   \n",
       "\n",
       "  ss_publication_date  ss_year  citation_count  \\\n",
       "0          2022-12-13     2022              20   \n",
       "\n",
       "                                                                  query  \\\n",
       "0  Structured Prompting: Scaling In Context Learning to 1, 000 Examples   \n",
       "\n",
       "  day_queried  \n",
       "0  2023-10-22  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_scholar_df_corrections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_results_df_corrections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper title</th>\n",
       "      <th>semantic scholar title</th>\n",
       "      <th>ss_publication_date</th>\n",
       "      <th>ss_year</th>\n",
       "      <th>citation_count</th>\n",
       "      <th>query</th>\n",
       "      <th>day_queried</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT</td>\n",
       "      <td>A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT</td>\n",
       "      <td>2023-02-21</td>\n",
       "      <td>2023</td>\n",
       "      <td>163</td>\n",
       "      <td>A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Taxonomy of Prompt Modifiers for Text-To-Image Generation</td>\n",
       "      <td>A Taxonomy of Prompt Modifiers for Text-To-Image Generation</td>\n",
       "      <td>2022-04-20</td>\n",
       "      <td>2022</td>\n",
       "      <td>43</td>\n",
       "      <td>A Taxonomy of Prompt Modifiers for Text To Image Generation</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts</td>\n",
       "      <td>AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts</td>\n",
       "      <td>2021-10-04</td>\n",
       "      <td>2021</td>\n",
       "      <td>146</td>\n",
       "      <td>AI Chains: Transparent and Controllable Human AI Interaction by Chaining Large Language Model Prompts</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ART: Automatic multi-step reasoning and tool-use for large language models</td>\n",
       "      <td>ART: Automatic multi-step reasoning and tool-use for large language models</td>\n",
       "      <td>2023-03-16</td>\n",
       "      <td>2023</td>\n",
       "      <td>39</td>\n",
       "      <td>ART: Automatic multi step reasoning and tool use for large language models</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Active Prompting with Chain-of-Thought for Large Language Models</td>\n",
       "      <td>Active Prompting with Chain-of-Thought for Large Language Models</td>\n",
       "      <td>2023-02-23</td>\n",
       "      <td>2023</td>\n",
       "      <td>34</td>\n",
       "      <td>Active Prompting with Chain of Thought for Large Language Models</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>An automatically discovered chain-of-thought prompt generalizes to novel models and datasets</td>\n",
       "      <td>An automatically discovered chain-of-thought prompt generalizes to novel models and datasets</td>\n",
       "      <td>2023-05-04</td>\n",
       "      <td>2023</td>\n",
       "      <td>6</td>\n",
       "      <td>An automatically discovered chain of thought prompt generalizes to novel models and datasets</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ask Me Anything: A simple strategy for prompting language models</td>\n",
       "      <td>Ask Me Anything: A simple strategy for prompting language models</td>\n",
       "      <td>2022-10-05</td>\n",
       "      <td>2022</td>\n",
       "      <td>65</td>\n",
       "      <td>Ask Me Anything: A simple strategy for prompting language models</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Atlas: Few-shot Learning with Retrieval Augmented Language Models</td>\n",
       "      <td>Few-shot Learning with Retrieval Augmented Language Models</td>\n",
       "      <td>2022-08-05</td>\n",
       "      <td>2022</td>\n",
       "      <td>202</td>\n",
       "      <td>Atlas: Few shot Learning with Retrieval Augmented Language Models</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts</td>\n",
       "      <td>Eliciting Knowledge from Language Models Using Automatically Generated Prompts</td>\n",
       "      <td>2020-10-29</td>\n",
       "      <td>2020</td>\n",
       "      <td>217</td>\n",
       "      <td>AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Automatic Chain of Thought Prompting in Large Language Models</td>\n",
       "      <td>Automatic Chain of Thought Prompting in Large Language Models</td>\n",
       "      <td>2022-10-07</td>\n",
       "      <td>2022</td>\n",
       "      <td>201</td>\n",
       "      <td>Automatic Chain of Thought Prompting in Large Language Models</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data</td>\n",
       "      <td>Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data</td>\n",
       "      <td>2023-02-24</td>\n",
       "      <td>2023</td>\n",
       "      <td>21</td>\n",
       "      <td>Automatic Prompt Augmentation and Selection with Chain of Thought from Labeled Data</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BERTese: Learning to Speak to BERT</td>\n",
       "      <td>BERTese: Learning to Speak to BERT</td>\n",
       "      <td>2021-03-09</td>\n",
       "      <td>2021</td>\n",
       "      <td>74</td>\n",
       "      <td>BERTese: Learning to Speak to BERT</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Batch Prompting: Efficient Inference with LLM APIs</td>\n",
       "      <td>Batch Prompting: Efficient Inference with Large Language Model APIs</td>\n",
       "      <td>2023-01-19</td>\n",
       "      <td>2023</td>\n",
       "      <td>10</td>\n",
       "      <td>Batch Prompting: Efficient Inference with LLM APIs</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Better Zero-Shot Reasoning with Self-Adaptive Prompting</td>\n",
       "      <td>Better Zero-Shot Reasoning with Self-Adaptive Prompting</td>\n",
       "      <td>2023-05-23</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>Better Zero Shot Reasoning with Self Adaptive Prompting</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Boosted Prompt Ensembles for Large Language Models</td>\n",
       "      <td>Boosted Prompt Ensembles for Large Language Models</td>\n",
       "      <td>2023-04-12</td>\n",
       "      <td>2023</td>\n",
       "      <td>12</td>\n",
       "      <td>Boosted Prompt Ensembles for Large Language Models</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints</td>\n",
       "      <td>Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints</td>\n",
       "      <td>2023-02-17</td>\n",
       "      <td>2023</td>\n",
       "      <td>6</td>\n",
       "      <td>Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale Language Model Society</td>\n",
       "      <td>CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale Language Model Society</td>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>2023</td>\n",
       "      <td>64</td>\n",
       "      <td>CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale Language Model Society</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Calibrate Before Use: Improving Few-Shot Performance of Language Models</td>\n",
       "      <td>Calibrate Before Use: Improving Few-Shot Performance of Language Models</td>\n",
       "      <td>2021-02-19</td>\n",
       "      <td>2021</td>\n",
       "      <td>616</td>\n",
       "      <td>Calibrate Before Use: Improving Few Shot Performance of Language Models</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT</td>\n",
       "      <td>Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT</td>\n",
       "      <td>2023-02-19</td>\n",
       "      <td>2023</td>\n",
       "      <td>73</td>\n",
       "      <td>Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine tuned BERT</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Can We Edit Factual Knowledge by In-Context Learning?</td>\n",
       "      <td>Can We Edit Factual Knowledge by In-Context Learning?</td>\n",
       "      <td>2023-05-22</td>\n",
       "      <td>2023</td>\n",
       "      <td>11</td>\n",
       "      <td>Can We Edit Factual Knowledge by In Context Learning?</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Chain of Hindsight Aligns Language Models with Feedback</td>\n",
       "      <td>Chain of Hindsight Aligns Language Models with Feedback</td>\n",
       "      <td>2023-02-06</td>\n",
       "      <td>2023</td>\n",
       "      <td>37</td>\n",
       "      <td>Chain of Hindsight Aligns Language Models with Feedback</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Chain of Thought Prompting Elicits Reasoning in Large Language Models</td>\n",
       "      <td>Chain of Thought Prompting Elicits Reasoning in Large Language Models</td>\n",
       "      <td>2022-01-28</td>\n",
       "      <td>2022</td>\n",
       "      <td>2105</td>\n",
       "      <td>Chain of Thought Prompting Elicits Reasoning in Large Language Models</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models</td>\n",
       "      <td>Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>Chain of Symbol Prompting Elicits Planning in Large Langauge Models</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Chain-of-Verification Reduces Hallucination in Large Language Models</td>\n",
       "      <td>Chain-of-Verification Reduces Hallucination in Large Language Models</td>\n",
       "      <td>2023-09-20</td>\n",
       "      <td>2023</td>\n",
       "      <td>10</td>\n",
       "      <td>Chain of Verification Reduces Hallucination in Large Language Models</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer</td>\n",
       "      <td>ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer</td>\n",
       "      <td>2023-06-13</td>\n",
       "      <td>2023</td>\n",
       "      <td>12</td>\n",
       "      <td>ChatGPT vs Human authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification</td>\n",
       "      <td>CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification</td>\n",
       "      <td>2023-03-07</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Complexity-Based Prompting for Multi-Step Reasoning</td>\n",
       "      <td>Complexity-Based Prompting for Multi-Step Reasoning</td>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>2022</td>\n",
       "      <td>116</td>\n",
       "      <td>Complexity Based Prompting for Multi Step Reasoning</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Compositional Exemplars for In-context Learning</td>\n",
       "      <td>Compositional Exemplars for In-context Learning</td>\n",
       "      <td>2023-02-11</td>\n",
       "      <td>2023</td>\n",
       "      <td>24</td>\n",
       "      <td>Compositional Exemplars for In context Learning</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt</td>\n",
       "      <td>Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt</td>\n",
       "      <td>2023-05-17</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>Compress, Then Prompt: Improving Accuracy Efficiency Trade off of LLM Inference with Transferable Prompt</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers</td>\n",
       "      <td>Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers</td>\n",
       "      <td>2023-09-15</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Constitutional AI: Harmlessness from AI Feedback</td>\n",
       "      <td>Constitutional AI: Harmlessness from AI Feedback</td>\n",
       "      <td>2022-12-15</td>\n",
       "      <td>2022</td>\n",
       "      <td>307</td>\n",
       "      <td>Constitutional AI: Harmlessness from AI Feedback</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Context-faithful Prompting for Large Language Models</td>\n",
       "      <td>Context-faithful Prompting for Large Language Models</td>\n",
       "      <td>2023-03-20</td>\n",
       "      <td>2023</td>\n",
       "      <td>21</td>\n",
       "      <td>Context faithful Prompting for Large Language Models</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>CooK: Empowering General-Purpose Language Models with Modular and Collaborative Knowledge</td>\n",
       "      <td>SHARED OPEN VOCABULARIES AND SEMANTIC MEDIA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>CooK: Empowering General Purpose Language Models with Modular and Collaborative Knowledge</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Decomposed Prompting: A Modular Approach for Solving Complex Tasks</td>\n",
       "      <td>Decomposed Prompting: A Modular Approach for Solving Complex Tasks</td>\n",
       "      <td>2022-10-05</td>\n",
       "      <td>2022</td>\n",
       "      <td>104</td>\n",
       "      <td>Decomposed Prompting: A Modular Approach for Solving Complex Tasks</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP</td>\n",
       "      <td>Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP</td>\n",
       "      <td>2022-12-28</td>\n",
       "      <td>2022</td>\n",
       "      <td>57</td>\n",
       "      <td>Demonstrate Search Predict: Composing retrieval and language models for knowledge intensive NLP</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Design Guidelines for Prompt Engineering Text-to-Image Generative Models</td>\n",
       "      <td>Design Guidelines for Prompt Engineering Text-to-Image Generative Models</td>\n",
       "      <td>2021-09-14</td>\n",
       "      <td>2021</td>\n",
       "      <td>142</td>\n",
       "      <td>Design Guidelines for Prompt Engineering Text to Image Generative Models</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Discovering Language Model Behaviors with Model-Written Evaluations</td>\n",
       "      <td>Discovering Language Model Behaviors with Model-Written Evaluations</td>\n",
       "      <td>2022-12-19</td>\n",
       "      <td>2022</td>\n",
       "      <td>87</td>\n",
       "      <td>Discovering Language Model Behaviors with Model Written Evaluations</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>DocPrompting: Generating Code by Retrieving the Docs</td>\n",
       "      <td>DocPrompting: Generating Code by Retrieving the Docs</td>\n",
       "      <td>2022-07-13</td>\n",
       "      <td>2022</td>\n",
       "      <td>27</td>\n",
       "      <td>DocPrompting: Generating Code by Retrieving the Docs</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning</td>\n",
       "      <td>Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning</td>\n",
       "      <td>2022-09-29</td>\n",
       "      <td>2022</td>\n",
       "      <td>51</td>\n",
       "      <td>Dynamic Prompt Learning via Policy Gradient for Semi structured Mathematical Reasoning</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Dynamic Prompting: A Unified Framework for Prompt Tuning</td>\n",
       "      <td>Dynamic Prompting: A Unified Framework for Prompt Tuning</td>\n",
       "      <td>2023-03-06</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>Dynamic Prompting: A Unified Framework for Prompt Tuning</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Effectiveness of Data Augmentation for Prefix Tuning with Limited Data</td>\n",
       "      <td>Effectiveness of Data Augmentation for Prefix Tuning with Limited Data</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>Effectiveness of Data Augmentation for Prefix Tuning with Limited Data</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Efficient Prompting via Dynamic In-Context Learning</td>\n",
       "      <td>Efficient Prompting via Dynamic In-Context Learning</td>\n",
       "      <td>2023-05-18</td>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>Efficient Prompting via Dynamic In Context Learning</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference</td>\n",
       "      <td>Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference</td>\n",
       "      <td>2022-11-21</td>\n",
       "      <td>2022</td>\n",
       "      <td>16</td>\n",
       "      <td>Enhancing Self Consistency and Performance of Pre Trained Language Models through Natural Language Inference</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Evaluating Large Language Models Trained on Code</td>\n",
       "      <td>Evaluating Large Language Models Trained on Code</td>\n",
       "      <td>2021-07-07</td>\n",
       "      <td>2021</td>\n",
       "      <td>1617</td>\n",
       "      <td>Evaluating Large Language Models Trained on Code</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Evaluating the Robustness of Discrete Prompts</td>\n",
       "      <td>Evaluating the Robustness of Discrete Prompts</td>\n",
       "      <td>2023-02-11</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>Evaluating the Robustness of Discrete Prompts</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples</td>\n",
       "      <td>Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples</td>\n",
       "      <td>2022-09-05</td>\n",
       "      <td>2022</td>\n",
       "      <td>12</td>\n",
       "      <td>Evaluating the Susceptibility of Pre Trained Language Models via Handcrafted Adversarial Examples</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>EvoPrompting: Language Models for Code-Level Neural Architecture Search</td>\n",
       "      <td>EvoPrompting: Language Models for Code-Level Neural Architecture Search</td>\n",
       "      <td>2023-02-28</td>\n",
       "      <td>2023</td>\n",
       "      <td>12</td>\n",
       "      <td>EvoPrompting: Language Models for Code Level Neural Architecture Search</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Exploring Lottery Prompts for Pre-trained Language Models</td>\n",
       "      <td>Exploring Lottery Prompts for Pre-trained Language Models</td>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>Exploring Lottery Prompts for Pre trained Language Models</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Fairness-guided Few-shot Prompting for Large Language Models</td>\n",
       "      <td>Fairness-guided Few-shot Prompting for Large Language Models</td>\n",
       "      <td>2023-03-23</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>Fairness guided Few shot Prompting for Large Language Models</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity</td>\n",
       "      <td>Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity</td>\n",
       "      <td>2021-04-18</td>\n",
       "      <td>2021</td>\n",
       "      <td>408</td>\n",
       "      <td>Fantastically Ordered Prompts and Where to Find Them: Overcoming Few Shot Prompt Order Sensitivity</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency</td>\n",
       "      <td>Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency</td>\n",
       "      <td>2023-05-18</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>Flatness Aware Prompt Selection Improves Accuracy and Sample Efficiency</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Flesch or Fumble? Evaluating Readability Standard Alignment of Instruction-Tuned Language Models</td>\n",
       "      <td>Flesch or Fumble? Evaluating Readability Standard Alignment of Instruction-Tuned Language Models</td>\n",
       "      <td>2023-09-11</td>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>Flesch or Fumble? Evaluating Readability Standard Alignment of Instruction Tuned Language Models</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Focused Prefix Tuning for Controllable Text Generation</td>\n",
       "      <td>Focused Prefix Tuning for Controllable Text Generation</td>\n",
       "      <td>2023-06-01</td>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>Focused Prefix Tuning for Controllable Text Generation</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting</td>\n",
       "      <td>From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting</td>\n",
       "      <td>2023-09-08</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>From Sparse to Dense: GPT 4 Summarization with Chain of Density Prompting</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>GPT-4 Technical Report</td>\n",
       "      <td>GPT-4 Technical Report</td>\n",
       "      <td>2023-03-15</td>\n",
       "      <td>2023</td>\n",
       "      <td>1529</td>\n",
       "      <td>GPT 4 Technical Report</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Generated Knowledge Prompting for Commonsense Reasoning</td>\n",
       "      <td>Generated Knowledge Prompting for Commonsense Reasoning</td>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>2021</td>\n",
       "      <td>114</td>\n",
       "      <td>Generated Knowledge Prompting for Commonsense Reasoning</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Global Prompt Cell: A Portable Control Module for Effective Prompt</td>\n",
       "      <td>Global Prompt Cell: A Portable Control Module for Effective Prompt</td>\n",
       "      <td>2023-04-12</td>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>Global Prompt Cell: A Portable Control Module for Effective Prompt</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Graph of Thoughts: Solving Elaborate Problems with Large Language Models</td>\n",
       "      <td>Graph of Thoughts: Solving Elaborate Problems with Large Language Models</td>\n",
       "      <td>2023-08-18</td>\n",
       "      <td>2023</td>\n",
       "      <td>35</td>\n",
       "      <td>Graph of Thoughts: Solving Elaborate Problems with Large Language Models</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks</td>\n",
       "      <td>GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks</td>\n",
       "      <td>2023-02-16</td>\n",
       "      <td>2023</td>\n",
       "      <td>16</td>\n",
       "      <td>GraphPrompt: Unifying Pre Training and Downstream Tasks for Graph Neural Networks</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Guiding Large Language Models via Directional Stimulus Prompting</td>\n",
       "      <td>Guiding Large Language Models via Directional Stimulus Prompting</td>\n",
       "      <td>2023-02-22</td>\n",
       "      <td>2023</td>\n",
       "      <td>10</td>\n",
       "      <td>Guiding Large Language Models via Directional Stimulus Prompting</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery</td>\n",
       "      <td>Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery</td>\n",
       "      <td>2023-02-07</td>\n",
       "      <td>2023</td>\n",
       "      <td>38</td>\n",
       "      <td>Hard Prompts Made Easy: Gradient Based Discrete Optimization for Prompt Tuning and Discovery</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Hierarchical Prompting Assists Large Language Model on Web Navigation</td>\n",
       "      <td>Hierarchical Prompting Assists Large Language Model on Web Navigation</td>\n",
       "      <td>2023-05-23</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>Hierarchical Prompting Assists Large Language Model on Web Navigation</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>How Can We Know What Language Models Know?</td>\n",
       "      <td>How Can We Know What Language Models Know?</td>\n",
       "      <td>2019-11-28</td>\n",
       "      <td>2019</td>\n",
       "      <td>763</td>\n",
       "      <td>How Can We Know What Language Models Know?</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>How Does In-Context Learning Help Prompt Tuning?</td>\n",
       "      <td>How Does In-Context Learning Help Prompt Tuning?</td>\n",
       "      <td>2023-02-22</td>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>How Does In Context Learning Help Prompt Tuning?</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks</td>\n",
       "      <td>How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>2023</td>\n",
       "      <td>24</td>\n",
       "      <td>How Robust is GPT 3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Ignore Previous Prompt: Attack Techniques For Language Models</td>\n",
       "      <td>Ignore Previous Prompt: Attack Techniques For Language Models</td>\n",
       "      <td>2022-11-17</td>\n",
       "      <td>2022</td>\n",
       "      <td>68</td>\n",
       "      <td>Ignore Previous Prompt: Attack Techniques For Language Models</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>In-Context Instruction Learning</td>\n",
       "      <td>In-Context Instruction Learning</td>\n",
       "      <td>2023-02-28</td>\n",
       "      <td>2023</td>\n",
       "      <td>19</td>\n",
       "      <td>In Context Instruction Learning</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>In-Context Learning of Large Language Models Explained as Kernel Regression</td>\n",
       "      <td>In-Context Learning of Large Language Models Explained as Kernel Regression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>In Context Learning of Large Language Models Explained as Kernel Regression</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Interactive Natural Language Processing</td>\n",
       "      <td>Interactive Natural Language Processing</td>\n",
       "      <td>2023-05-22</td>\n",
       "      <td>2023</td>\n",
       "      <td>14</td>\n",
       "      <td>Interactive Natural Language Processing</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Is GPT-4 a Good Data Analyst?</td>\n",
       "      <td>Is GPT-4 a Good Data Analyst?</td>\n",
       "      <td>2023-05-24</td>\n",
       "      <td>2023</td>\n",
       "      <td>17</td>\n",
       "      <td>Is GPT 4 a Good Data Analyst?</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning</td>\n",
       "      <td>Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning</td>\n",
       "      <td>2023-03-18</td>\n",
       "      <td>2023</td>\n",
       "      <td>16</td>\n",
       "      <td>Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Language Is Not All You Need: Aligning Perception with Language Models</td>\n",
       "      <td>Language Is Not All You Need: Aligning Perception with Language Models</td>\n",
       "      <td>2023-02-27</td>\n",
       "      <td>2023</td>\n",
       "      <td>154</td>\n",
       "      <td>Language Is Not All You Need: Aligning Perception with Language Models</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought</td>\n",
       "      <td>Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought</td>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>2022</td>\n",
       "      <td>58</td>\n",
       "      <td>Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain of Thought</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Language Models are Few-Shot Learners</td>\n",
       "      <td>Language Models are Few-Shot Learners</td>\n",
       "      <td>2020-05-28</td>\n",
       "      <td>2020</td>\n",
       "      <td>16440</td>\n",
       "      <td>Language Models are Few Shot Learners</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Large Language Model Guided Tree-of-Thought</td>\n",
       "      <td>Large Language Model Guided Tree-of-Thought</td>\n",
       "      <td>2023-05-15</td>\n",
       "      <td>2023</td>\n",
       "      <td>26</td>\n",
       "      <td>Large Language Model Guided Tree of Thought</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Large Language Models Are Human-Level Prompt Engineers</td>\n",
       "      <td>Large Language Models Are Human-Level Prompt Engineers</td>\n",
       "      <td>2022-11-03</td>\n",
       "      <td>2022</td>\n",
       "      <td>194</td>\n",
       "      <td>Large Language Models Are Human Level Prompt Engineers</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Large Language Models Can Be Easily Distracted by Irrelevant Context</td>\n",
       "      <td>Large Language Models Can Be Easily Distracted by Irrelevant Context</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>2023</td>\n",
       "      <td>70</td>\n",
       "      <td>Large Language Models Can Be Easily Distracted by Irrelevant Context</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Large Language Models are Zero-Shot Reasoners</td>\n",
       "      <td>Large Language Models are Zero-Shot Reasoners</td>\n",
       "      <td>2022-05-24</td>\n",
       "      <td>2022</td>\n",
       "      <td>882</td>\n",
       "      <td>Large Language Models are Zero Shot Reasoners</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Large Language Models are reasoners with Self-Verification</td>\n",
       "      <td>Large Language Models are reasoners with Self-Verification</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022</td>\n",
       "      <td>31</td>\n",
       "      <td>Large Language Models are reasoners with Self Verification</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Larger language models do in-context learning differently</td>\n",
       "      <td>Larger language models do in-context learning differently</td>\n",
       "      <td>2023-03-07</td>\n",
       "      <td>2023</td>\n",
       "      <td>87</td>\n",
       "      <td>Larger language models do in context learning differently</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Learning from Task Descriptions</td>\n",
       "      <td>Learning from Task Descriptions</td>\n",
       "      <td>2020-11-01</td>\n",
       "      <td>2020</td>\n",
       "      <td>64</td>\n",
       "      <td>Learning from Task Descriptions</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Learning to Generate Task-Specific Adapters from Task Description</td>\n",
       "      <td>Learning to Generate Task-Specific Adapters from Task Description</td>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>2021</td>\n",
       "      <td>18</td>\n",
       "      <td>Learning to Generate Task Specific Adapters from Task Description</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Learning to Transfer Prompts for Text Generation</td>\n",
       "      <td>Learning to Transfer Prompts for Text Generation</td>\n",
       "      <td>2022-05-03</td>\n",
       "      <td>2022</td>\n",
       "      <td>18</td>\n",
       "      <td>Learning to Transfer Prompts for Text Generation</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Least-to-most promptingenables complex reasoning in large language models</td>\n",
       "      <td>Least-to-Most Prompting Enables Complex Reasoning in Large Language Models</td>\n",
       "      <td>2022-05-21</td>\n",
       "      <td>2022</td>\n",
       "      <td>369</td>\n",
       "      <td>Least to most promptingenables complex reasoning in large language models</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses</td>\n",
       "      <td>Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses</td>\n",
       "      <td>2023-05-30</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning with LLMs</td>\n",
       "      <td>Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning with LLMs</td>\n",
       "      <td>2023-05-19</td>\n",
       "      <td>2023</td>\n",
       "      <td>6</td>\n",
       "      <td>Let's Sample Step by Step: Adaptive Consistency for Efficient Reasoning with LLMs</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Let's Verify Step by Step</td>\n",
       "      <td>Let's Verify Step by Step</td>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>2023</td>\n",
       "      <td>64</td>\n",
       "      <td>Let's Verify Step by Step</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning</td>\n",
       "      <td>MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning</td>\n",
       "      <td>2022-05-01</td>\n",
       "      <td>2022</td>\n",
       "      <td>31</td>\n",
       "      <td>MRKL Systems: A modular, neuro symbolic architecture that combines large language models, external knowledge sources and discrete reasoning</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods</td>\n",
       "      <td>Machine-Generated Text: A Comprehensive Survey of Threat Models and Detection Methods</td>\n",
       "      <td>2022-10-13</td>\n",
       "      <td>2022</td>\n",
       "      <td>28</td>\n",
       "      <td>Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations</td>\n",
       "      <td>Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations</td>\n",
       "      <td>2022-05-24</td>\n",
       "      <td>2022</td>\n",
       "      <td>86</td>\n",
       "      <td>Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Making Pre-trained Language Models Better Few-shot Learners</td>\n",
       "      <td>Making Pre-trained Language Models Better Few-shot Learners</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>2021</td>\n",
       "      <td>1082</td>\n",
       "      <td>Making Pre trained Language Models Better Few shot Learners</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Meta-in-context learning in large language models</td>\n",
       "      <td>Meta-in-context learning in large language models</td>\n",
       "      <td>2023-05-22</td>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>Meta in context learning in large language models</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Mixture of Soft Prompts for Controllable Data Generation</td>\n",
       "      <td>Mixture of Soft Prompts for Controllable Data Generation</td>\n",
       "      <td>2023-03-02</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>Mixture of Soft Prompts for Controllable Data Generation</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Model-tuning Via Prompts Makes NLP Models Adversarially Robust</td>\n",
       "      <td>Model-tuning Via Prompts Makes NLP Models Adversarially Robust</td>\n",
       "      <td>2023-03-13</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>Model tuning Via Prompts Makes NLP Models Adversarially Robust</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models</td>\n",
       "      <td>More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023</td>\n",
       "      <td>37</td>\n",
       "      <td>More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application Integrated Large Language Models</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of Thought Prompting</td>\n",
       "      <td>MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of Thought Prompting</td>\n",
       "      <td>2023-05-26</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>MultiTool CoT: GPT 3 Can Use Multiple External Tools with Chain of Thought Prompting</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Multimodal Chain-of-Thought Reasoning in Language Models</td>\n",
       "      <td>Multimodal Chain-of-Thought Reasoning in Language Models</td>\n",
       "      <td>2023-02-02</td>\n",
       "      <td>2023</td>\n",
       "      <td>91</td>\n",
       "      <td>Multimodal Chain of Thought Reasoning in Language Models</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning</td>\n",
       "      <td>Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning</td>\n",
       "      <td>2023-03-06</td>\n",
       "      <td>2023</td>\n",
       "      <td>21</td>\n",
       "      <td>Multitask Prompt Tuning Enables Parameter Efficient Transfer Learning</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Multitask Prompted Training Enables Zero-Shot Task Generalization</td>\n",
       "      <td>Multitask Prompted Training Enables Zero-Shot Task Generalization</td>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>2021</td>\n",
       "      <td>924</td>\n",
       "      <td>Multitask Prompted Training Enables Zero Shot Task Generalization</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning</td>\n",
       "      <td>On Second Thought, Lets Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning</td>\n",
       "      <td>2022-12-15</td>\n",
       "      <td>2022</td>\n",
       "      <td>23</td>\n",
       "      <td>On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero Shot Reasoning</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>On the Advance of Making Language Models Better Reasoners</td>\n",
       "      <td>On the Advance of Making Language Models Better Reasoners</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022</td>\n",
       "      <td>106</td>\n",
       "      <td>On the Advance of Making Language Models Better Reasoners</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>OpenICL: An Open-Source Framework for In-context Learning</td>\n",
       "      <td>OpenICL: An Open-Source Framework for In-context Learning</td>\n",
       "      <td>2023-03-06</td>\n",
       "      <td>2023</td>\n",
       "      <td>12</td>\n",
       "      <td>OpenICL: An Open Source Framework for In context Learning</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>PAL: Program-aided Language Models</td>\n",
       "      <td>PAL: Program-aided Language Models</td>\n",
       "      <td>2022-11-18</td>\n",
       "      <td>2022</td>\n",
       "      <td>198</td>\n",
       "      <td>PAL: Program aided Language Models</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents</td>\n",
       "      <td>PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents</td>\n",
       "      <td>2023-05-23</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>PPT: Pre-trained Prompt Tuning for Few-shot Learning</td>\n",
       "      <td>PPT: Pre-trained Prompt Tuning for Few-shot Learning</td>\n",
       "      <td>2021-09-09</td>\n",
       "      <td>2021</td>\n",
       "      <td>234</td>\n",
       "      <td>PPT: Pre trained Prompt Tuning for Few shot Learning</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models</td>\n",
       "      <td>Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models</td>\n",
       "      <td>2023-05-06</td>\n",
       "      <td>2023</td>\n",
       "      <td>46</td>\n",
       "      <td>Plan and Solve Prompting: Improving Zero Shot Chain of Thought Reasoning by Large Language Models</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Post Hoc Explanations of Language Models Can Improve Language Models</td>\n",
       "      <td>Post Hoc Explanations of Language Models Can Improve Language Models</td>\n",
       "      <td>2023-05-19</td>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>Post Hoc Explanations of Language Models Can Improve Language Models</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>Pre-Training to Learn in Context</td>\n",
       "      <td>Pre-Training to Learn in Context</td>\n",
       "      <td>2023-05-16</td>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>Pre Training to Learn in Context</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Prefix-Tuning: Optimizing Continuous Prompts for Generation</td>\n",
       "      <td>Prefix-Tuning: Optimizing Continuous Prompts for Generation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021</td>\n",
       "      <td>1682</td>\n",
       "      <td>Prefix Tuning: Optimizing Continuous Prompts for Generation</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Progressive Prompts: Continual Learning for Language Models</td>\n",
       "      <td>Progressive Prompts: Continual Learning for Language Models</td>\n",
       "      <td>2023-01-29</td>\n",
       "      <td>2023</td>\n",
       "      <td>20</td>\n",
       "      <td>Progressive Prompts: Continual Learning for Language Models</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Prompt Engineering for Large Language Models</td>\n",
       "      <td>Prompt Engineering for Large Language Models</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>Prompt Engineering for Large Language Models</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm</td>\n",
       "      <td>Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm</td>\n",
       "      <td>2021-02-15</td>\n",
       "      <td>2021</td>\n",
       "      <td>317</td>\n",
       "      <td>Prompt Programming for Large Language Models: Beyond the Few Shot Paradigm</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners</td>\n",
       "      <td>Prompt, Generate, Then Cache: Cascade of Foundation Models Makes Strong Few-Shot Learners</td>\n",
       "      <td>2023-03-03</td>\n",
       "      <td>2023</td>\n",
       "      <td>30</td>\n",
       "      <td>Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few shot Learners</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>PromptChainer: Chaining Large Language Model Prompts through Visual Programming</td>\n",
       "      <td>PromptChainer: Chaining Large Language Model Prompts through Visual Programming</td>\n",
       "      <td>2022-03-13</td>\n",
       "      <td>2022</td>\n",
       "      <td>78</td>\n",
       "      <td>PromptChainer: Chaining Large Language Model Prompts through Visual Programming</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Promptagator: Few-shot Dense Retrieval From 8 Examples</td>\n",
       "      <td>Promptagator: Few-shot Dense Retrieval From 8 Examples</td>\n",
       "      <td>2022-09-23</td>\n",
       "      <td>2022</td>\n",
       "      <td>66</td>\n",
       "      <td>Promptagator: Few shot Dense Retrieval From 8 Examples</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Prompted Opinion Summarization with GPT-3.5</td>\n",
       "      <td>Prompted Opinion Summarization with GPT-3.5</td>\n",
       "      <td>2022-11-29</td>\n",
       "      <td>2022</td>\n",
       "      <td>8</td>\n",
       "      <td>Prompted Opinion Summarization with GPT 3.5</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Prompting GPT-3 To Be Reliable</td>\n",
       "      <td>Prompting GPT-3 To Be Reliable</td>\n",
       "      <td>2022-10-17</td>\n",
       "      <td>2022</td>\n",
       "      <td>72</td>\n",
       "      <td>Prompting GPT 3 To Be Reliable</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL</td>\n",
       "      <td>Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL</td>\n",
       "      <td>2023-09-13</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>Query Dependent Prompt Evaluation and Optimization with Offline Inverse RL</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>REFINER: Reasoning Feedback on Intermediate Representations</td>\n",
       "      <td>REFINER: Reasoning Feedback on Intermediate Representations</td>\n",
       "      <td>2023-04-04</td>\n",
       "      <td>2023</td>\n",
       "      <td>35</td>\n",
       "      <td>REFINER: Reasoning Feedback on Intermediate Representations</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>Re-Reading Improves Reasoning in Language Models</td>\n",
       "      <td>Re-Reading Improves Reasoning in Language Models</td>\n",
       "      <td>2023-09-12</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>Re Reading Improves Reasoning in Language Models</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>ReAct: Synergizing Reasoning and Acting in Language Models</td>\n",
       "      <td>ReAct: Synergizing Reasoning and Acting in Language Models</td>\n",
       "      <td>2022-10-06</td>\n",
       "      <td>2022</td>\n",
       "      <td>332</td>\n",
       "      <td>ReAct: Synergizing Reasoning and Acting in Language Models</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Reasoning with Language Model is Planning with World Model</td>\n",
       "      <td>Reasoning with Language Model is Planning with World Model</td>\n",
       "      <td>2023-05-24</td>\n",
       "      <td>2023</td>\n",
       "      <td>34</td>\n",
       "      <td>Reasoning with Language Model is Planning with World Model</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>Recitation-Augmented Language Models</td>\n",
       "      <td>Recitation-Augmented Language Models</td>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>2022</td>\n",
       "      <td>37</td>\n",
       "      <td>Recitation Augmented Language Models</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Reflexion: an autonomous agent with dynamic memory and self-reflection</td>\n",
       "      <td>Reflexion: an autonomous agent with dynamic memory and self-reflection</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023</td>\n",
       "      <td>105</td>\n",
       "      <td>Reflexion: an autonomous agent with dynamic memory and self reflection</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Reframing Instructional Prompts to GPTk's Language</td>\n",
       "      <td>Reframing Instructional Prompts to GPTks Language</td>\n",
       "      <td>2021-09-16</td>\n",
       "      <td>2021</td>\n",
       "      <td>121</td>\n",
       "      <td>Reframing Instructional Prompts to GPTk's Language</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling</td>\n",
       "      <td>Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling</td>\n",
       "      <td>2023-05-17</td>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>Reprompting: Automated Chain of Thought Prompt Inference Through Gibbs Sampling</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?</td>\n",
       "      <td>Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?</td>\n",
       "      <td>2022-02-25</td>\n",
       "      <td>2022</td>\n",
       "      <td>431</td>\n",
       "      <td>Rethinking the Role of Demonstrations: What Makes In Context Learning Work?</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</td>\n",
       "      <td>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</td>\n",
       "      <td>2020-05-22</td>\n",
       "      <td>2020</td>\n",
       "      <td>956</td>\n",
       "      <td>Retrieval Augmented Generation for Knowledge Intensive NLP Tasks</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Revisiting Automated Prompting: Are We Actually Doing Better?</td>\n",
       "      <td>Revisiting Automated Prompting: Are We Actually Doing Better?</td>\n",
       "      <td>2023-04-07</td>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>Revisiting Automated Prompting: Are We Actually Doing Better?</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Satisfiability-Aided Language Models Using Declarative Prompting</td>\n",
       "      <td>Satisfiability-Aided Language Models Using Declarative Prompting</td>\n",
       "      <td>2023-05-16</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>Satisfiability Aided Language Models Using Declarative Prompting</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Scalable Prompt Generation for Semi-supervised Learning with Language Models</td>\n",
       "      <td>Scalable Prompt Generation for Semi-supervised Learning with Language Models</td>\n",
       "      <td>2023-02-18</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>Scalable Prompt Generation for Semi supervised Learning with Language Models</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>Scaling Instruction-Finetuned Language Models</td>\n",
       "      <td>Scaling Instruction-Finetuned Language Models</td>\n",
       "      <td>2022-10-20</td>\n",
       "      <td>2022</td>\n",
       "      <td>823</td>\n",
       "      <td>Scaling Instruction Finetuned Language Models</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>Scaling Laws for Neural Language Models</td>\n",
       "      <td>Scaling Laws for Neural Language Models</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>2020</td>\n",
       "      <td>1530</td>\n",
       "      <td>Scaling Laws for Neural Language Models</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Self-Consistency Improves Chain of Thought Reasoning in Language Models</td>\n",
       "      <td>Self-Consistency Improves Chain of Thought Reasoning in Language Models</td>\n",
       "      <td>2022-03-21</td>\n",
       "      <td>2022</td>\n",
       "      <td>659</td>\n",
       "      <td>Self Consistency Improves Chain of Thought Reasoning in Language Models</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Self-Critique Prompting with Large Language Models for Inductive Instructions</td>\n",
       "      <td>Self-Critique Prompting with Large Language Models for Inductive Instructions</td>\n",
       "      <td>2023-05-23</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>Self Critique Prompting with Large Language Models for Inductive Instructions</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Self-Refine: Iterative Refinement with Self-Feedback</td>\n",
       "      <td>Self-Refine: Iterative Refinement with Self-Feedback</td>\n",
       "      <td>2023-03-30</td>\n",
       "      <td>2023</td>\n",
       "      <td>201</td>\n",
       "      <td>Self Refine: Iterative Refinement with Self Feedback</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Show Your Work: Scratchpads for Intermediate Computation with Language Models</td>\n",
       "      <td>Show Your Work: Scratchpads for Intermediate Computation with Language Models</td>\n",
       "      <td>2021-11-30</td>\n",
       "      <td>2021</td>\n",
       "      <td>315</td>\n",
       "      <td>Show Your Work: Scratchpads for Intermediate Computation with Language Models</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding</td>\n",
       "      <td>Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding</td>\n",
       "      <td>2023-07-28</td>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>Skeleton of Thought: Large Language Models Can Do Parallel Decoding</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer</td>\n",
       "      <td>Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer</td>\n",
       "      <td>2023-03-03</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>Successive Prompting for Decomposing Complex Questions</td>\n",
       "      <td>Successive Prompting for Decomposing Complex Questions</td>\n",
       "      <td>2022-12-08</td>\n",
       "      <td>2022</td>\n",
       "      <td>40</td>\n",
       "      <td>Successive Prompting for Decomposing Complex Questions</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains</td>\n",
       "      <td>SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains</td>\n",
       "      <td>2023-02-14</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>SwitchPrompt: Learning Domain Specific Gated Soft Prompts for Classification in Low Resource Domains</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models</td>\n",
       "      <td>Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models</td>\n",
       "      <td>2023-02-01</td>\n",
       "      <td>2023</td>\n",
       "      <td>26</td>\n",
       "      <td>Synthetic Prompting: Generating Chain of Thought Demonstrations for Large Language Models</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks</td>\n",
       "      <td>TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks</td>\n",
       "      <td>2023-05-19</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Teaching Algorithmic Reasoning via In-context Learning</td>\n",
       "      <td>Teaching Algorithmic Reasoning via In-context Learning</td>\n",
       "      <td>2022-11-15</td>\n",
       "      <td>2022</td>\n",
       "      <td>52</td>\n",
       "      <td>Teaching Algorithmic Reasoning via In context Learning</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>The Capacity for Moral Self-Correction in Large Language Models</td>\n",
       "      <td>The Capacity for Moral Self-Correction in Large Language Models</td>\n",
       "      <td>2023-02-15</td>\n",
       "      <td>2023</td>\n",
       "      <td>49</td>\n",
       "      <td>The Capacity for Moral Self Correction in Large Language Models</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>The Power of Scale for Parameter-Efficient Prompt Tuning</td>\n",
       "      <td>The Power of Scale for Parameter-Efficient Prompt Tuning</td>\n",
       "      <td>2021-04-18</td>\n",
       "      <td>2021</td>\n",
       "      <td>1530</td>\n",
       "      <td>The Power of Scale for Parameter Efficient Prompt Tuning</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning</td>\n",
       "      <td>The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning</td>\n",
       "      <td>2022-05-06</td>\n",
       "      <td>2022</td>\n",
       "      <td>47</td>\n",
       "      <td>The Unreliability of Explanations in Few shot Prompting for Textual Reasoning</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>The Web Can Be Your Oyster for Improving Large Language Models</td>\n",
       "      <td>The Web Can Be Your Oyster for Improving Large Language Models</td>\n",
       "      <td>2023-05-18</td>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>The Web Can Be Your Oyster for Improving Large Language Models</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Toxicity Detection with Generative Prompt-based Inference</td>\n",
       "      <td>Toxicity Detection with Generative Prompt-based Inference</td>\n",
       "      <td>2022-05-24</td>\n",
       "      <td>2022</td>\n",
       "      <td>11</td>\n",
       "      <td>Toxicity Detection with Generative Prompt based Inference</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Training language models to follow instructions with human feedback</td>\n",
       "      <td>Training language models to follow instructions with human feedback</td>\n",
       "      <td>2022-03-04</td>\n",
       "      <td>2022</td>\n",
       "      <td>3009</td>\n",
       "      <td>Training language models to follow instructions with human feedback</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Tree of Thoughts: Deliberate Problem Solving with Large Language Models</td>\n",
       "      <td>Tree of Thoughts: Deliberate Problem Solving with Large Language Models</td>\n",
       "      <td>2023-05-17</td>\n",
       "      <td>2023</td>\n",
       "      <td>227</td>\n",
       "      <td>Tree of Thoughts: Deliberate Problem Solving with Large Language Models</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding</td>\n",
       "      <td>TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding</td>\n",
       "      <td>2023-05-19</td>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation</td>\n",
       "      <td>UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation</td>\n",
       "      <td>2023-03-15</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>UPRISE: Universal Prompt Retrieval for Improving Zero Shot Evaluation</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Universality and Limitations of Prompt Tuning</td>\n",
       "      <td>Universality and Limitations of Prompt Tuning</td>\n",
       "      <td>2023-05-30</td>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>Universality and Limitations of Prompt Tuning</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Visual-Language Prompt Tuning with Knowledge-guided Context Optimization</td>\n",
       "      <td>Visual-Language Prompt Tuning with Knowledge-Guided Context Optimization</td>\n",
       "      <td>2023-03-23</td>\n",
       "      <td>2023</td>\n",
       "      <td>11</td>\n",
       "      <td>Visual Language Prompt Tuning with Knowledge guided Context Optimization</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>What In-Context Learning \"Learns\" In-Context: Disentangling Task Recognition and Task Learning</td>\n",
       "      <td>What In-Context Learning \"Learns\" In-Context: Disentangling Task Recognition and Task Learning</td>\n",
       "      <td>2023-05-16</td>\n",
       "      <td>2023</td>\n",
       "      <td>19</td>\n",
       "      <td>What In Context Learning \"Learns\" In Context: Disentangling Task Recognition and Task Learning</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>Why think step-by-step? Reasoning emerges from the locality of experience</td>\n",
       "      <td>Why think step-by-step? Reasoning emerges from the locality of experience</td>\n",
       "      <td>2023-04-07</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>Why think step by step? Reasoning emerges from the locality of experience</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>ZeroPrompt: Streaming Acoustic Encoders are Zero-Shot Masked LMs</td>\n",
       "      <td>ZeroPrompt: Streaming Acoustic Encoders are Zero-Shot Masked LMs</td>\n",
       "      <td>2023-05-18</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>ZeroPrompt: Streaming Acoustic Encoders are Zero Shot Masked LMs</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>kNN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference</td>\n",
       "      <td>kNN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference</td>\n",
       "      <td>2023-03-24</td>\n",
       "      <td>2023</td>\n",
       "      <td>16</td>\n",
       "      <td>kNN Prompting: Beyond Context Learning with Calibration Free Nearest Neighbor Inference</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>-la-carte Prompt Tuning (APT): Combining Distinct Data Via Composable Prompting</td>\n",
       "      <td>-la-carte Prompt Tuning (APT): Combining Distinct Data Via Composable Prompting</td>\n",
       "      <td>2023-02-15</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td> la carte Prompt Tuning (APT): Combining Distinct Data Via Composable Prompting</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                     paper title  \\\n",
       "0                                                                            A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT   \n",
       "1                                                                                    A Taxonomy of Prompt Modifiers for Text-To-Image Generation   \n",
       "2                                          AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts   \n",
       "3                                                                     ART: Automatic multi-step reasoning and tool-use for large language models   \n",
       "4                                                                               Active Prompting with Chain-of-Thought for Large Language Models   \n",
       "5                                                   An automatically discovered chain-of-thought prompt generalizes to novel models and datasets   \n",
       "6                                                                               Ask Me Anything: A simple strategy for prompting language models   \n",
       "7                                                                              Atlas: Few-shot Learning with Retrieval Augmented Language Models   \n",
       "8                                                      AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts   \n",
       "9                                                                                  Automatic Chain of Thought Prompting in Large Language Models   \n",
       "10                                                           Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data   \n",
       "11                                                                                                            BERTese: Learning to Speak to BERT   \n",
       "12                                                                                            Batch Prompting: Efficient Inference with LLM APIs   \n",
       "13                                                                                       Better Zero-Shot Reasoning with Self-Adaptive Prompting   \n",
       "14                                                                                            Boosted Prompt Ensembles for Large Language Models   \n",
       "15                                            Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints   \n",
       "16                                                      CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale Language Model Society   \n",
       "17                                                                       Calibrate Before Use: Improving Few-Shot Performance of Language Models   \n",
       "18                                                                Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT   \n",
       "19                                                                                         Can We Edit Factual Knowledge by In-Context Learning?   \n",
       "20                                                                                       Chain of Hindsight Aligns Language Models with Feedback   \n",
       "21                                                                         Chain of Thought Prompting Elicits Reasoning in Large Language Models   \n",
       "22                                                                           Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models   \n",
       "23                                                                          Chain-of-Verification Reduces Hallucination in Large Language Models   \n",
       "24                                     ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer   \n",
       "25                                                           CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification   \n",
       "26                                                                                           Complexity-Based Prompting for Multi-Step Reasoning   \n",
       "27                                                                                               Compositional Exemplars for In-context Learning   \n",
       "28                                      Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt   \n",
       "29                                               Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers   \n",
       "30                                                                                              Constitutional AI: Harmlessness from AI Feedback   \n",
       "31                                                                                          Context-faithful Prompting for Large Language Models   \n",
       "32                                                     CooK: Empowering General-Purpose Language Models with Modular and Collaborative Knowledge   \n",
       "33                                                                            Decomposed Prompting: A Modular Approach for Solving Complex Tasks   \n",
       "34                                               Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP   \n",
       "35                                                                      Design Guidelines for Prompt Engineering Text-to-Image Generative Models   \n",
       "36                                                                           Discovering Language Model Behaviors with Model-Written Evaluations   \n",
       "37                                                                                          DocPrompting: Generating Code by Retrieving the Docs   \n",
       "38                                                        Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning   \n",
       "39                                                                                      Dynamic Prompting: A Unified Framework for Prompt Tuning   \n",
       "40                                                                        Effectiveness of Data Augmentation for Prefix Tuning with Limited Data   \n",
       "41                                                                                           Efficient Prompting via Dynamic In-Context Learning   \n",
       "42                                  Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference   \n",
       "43                                                                                              Evaluating Large Language Models Trained on Code   \n",
       "44                                                                                                 Evaluating the Robustness of Discrete Prompts   \n",
       "45                                             Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples   \n",
       "46                                                                       EvoPrompting: Language Models for Code-Level Neural Architecture Search   \n",
       "47                                                                                     Exploring Lottery Prompts for Pre-trained Language Models   \n",
       "48                                                                                  Fairness-guided Few-shot Prompting for Large Language Models   \n",
       "49                                            Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity   \n",
       "50                                                                       Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency   \n",
       "51                                              Flesch or Fumble? Evaluating Readability Standard Alignment of Instruction-Tuned Language Models   \n",
       "52                                                                                        Focused Prefix Tuning for Controllable Text Generation   \n",
       "53                                                                     From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting   \n",
       "54                                                                                                                        GPT-4 Technical Report   \n",
       "55                                                                                       Generated Knowledge Prompting for Commonsense Reasoning   \n",
       "56                                                                            Global Prompt Cell: A Portable Control Module for Effective Prompt   \n",
       "57                                                                      Graph of Thoughts: Solving Elaborate Problems with Large Language Models   \n",
       "58                                                             GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks   \n",
       "59                                                                              Guiding Large Language Models via Directional Stimulus Prompting   \n",
       "60                                                  Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery   \n",
       "61                                                                         Hierarchical Prompting Assists Large Language Model on Web Navigation   \n",
       "62                                                                                                    How Can We Know What Language Models Know?   \n",
       "63                                                                                              How Does In-Context Learning Help Prompt Tuning?   \n",
       "64                                                  How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks   \n",
       "65                                                                                 Ignore Previous Prompt: Attack Techniques For Language Models   \n",
       "66                                                                                                               In-Context Instruction Learning   \n",
       "67                                                                   In-Context Learning of Large Language Models Explained as Kernel Regression   \n",
       "68                                                                                                       Interactive Natural Language Processing   \n",
       "69                                                                                                                 Is GPT-4 a Good Data Analyst?   \n",
       "70                                                          Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning   \n",
       "71                                                                        Language Is Not All You Need: Aligning Perception with Language Models   \n",
       "72                                                        Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought   \n",
       "73                                                                                                         Language Models are Few-Shot Learners   \n",
       "74                                                                                                   Large Language Model Guided Tree-of-Thought   \n",
       "75                                                                                        Large Language Models Are Human-Level Prompt Engineers   \n",
       "76                                                                          Large Language Models Can Be Easily Distracted by Irrelevant Context   \n",
       "77                                                                                                 Large Language Models are Zero-Shot Reasoners   \n",
       "78                                                                                    Large Language Models are reasoners with Self-Verification   \n",
       "79                                                                                     Larger language models do in-context learning differently   \n",
       "80                                                                                                               Learning from Task Descriptions   \n",
       "81                                                                             Learning to Generate Task-Specific Adapters from Task Description   \n",
       "82                                                                                              Learning to Transfer Prompts for Text Generation   \n",
       "83                                                                    Least-to-most promptingenables complex reasoning in large language models   \n",
       "84                                                           Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses   \n",
       "85                                                             Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning with LLMs   \n",
       "86                                                                                                                     Let's Verify Step by Step   \n",
       "87   MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning   \n",
       "88                                                         Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods   \n",
       "89                                                                Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations   \n",
       "90                                                                                   Making Pre-trained Language Models Better Few-shot Learners   \n",
       "91                                                                                             Meta-in-context learning in large language models   \n",
       "92                                                                                      Mixture of Soft Prompts for Controllable Data Generation   \n",
       "93                                                                                Model-tuning Via Prompts Makes NLP Models Adversarially Robust   \n",
       "94        More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models   \n",
       "95                                                          MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of Thought Prompting   \n",
       "96                                                                                      Multimodal Chain-of-Thought Reasoning in Language Models   \n",
       "97                                                                         Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning   \n",
       "98                                                                             Multitask Prompted Training Enables Zero-Shot Task Generalization   \n",
       "99                                                     On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning   \n",
       "100                                                                                    On the Advance of Making Language Models Better Reasoners   \n",
       "101                                                                                    OpenICL: An Open-Source Framework for In-context Learning   \n",
       "102                                                                                                           PAL: Program-aided Language Models   \n",
       "103                                                       PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents   \n",
       "104                                                                                         PPT: Pre-trained Prompt Tuning for Few-shot Learning   \n",
       "105                                            Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models   \n",
       "106                                                                         Post Hoc Explanations of Language Models Can Improve Language Models   \n",
       "107                                                                                                             Pre-Training to Learn in Context   \n",
       "108                                                                                  Prefix-Tuning: Optimizing Continuous Prompts for Generation   \n",
       "109                                                                                  Progressive Prompts: Continual Learning for Language Models   \n",
       "110                                                                                                 Prompt Engineering for Large Language Models   \n",
       "111                                                                   Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm   \n",
       "112                                                    Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners   \n",
       "113                                                              PromptChainer: Chaining Large Language Model Prompts through Visual Programming   \n",
       "114                                                                                       Promptagator: Few-shot Dense Retrieval From 8 Examples   \n",
       "115                                                                                                  Prompted Opinion Summarization with GPT-3.5   \n",
       "116                                                                                                               Prompting GPT-3 To Be Reliable   \n",
       "117                                                                   Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL   \n",
       "118                                                                                  REFINER: Reasoning Feedback on Intermediate Representations   \n",
       "119                                                                                             Re-Reading Improves Reasoning in Language Models   \n",
       "120                                                                                   ReAct: Synergizing Reasoning and Acting in Language Models   \n",
       "121                                                                                   Reasoning with Language Model is Planning with World Model   \n",
       "122                                                                                                         Recitation-Augmented Language Models   \n",
       "123                                                                       Reflexion: an autonomous agent with dynamic memory and self-reflection   \n",
       "124                                                                                           Reframing Instructional Prompts to GPTk's Language   \n",
       "125                                                              Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling   \n",
       "126                                                                  Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?   \n",
       "127                                                                             Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks   \n",
       "128                                                                                Revisiting Automated Prompting: Are We Actually Doing Better?   \n",
       "129                                                                             Satisfiability-Aided Language Models Using Declarative Prompting   \n",
       "130                                                                 Scalable Prompt Generation for Semi-supervised Learning with Language Models   \n",
       "131                                                                                                Scaling Instruction-Finetuned Language Models   \n",
       "132                                                                                                      Scaling Laws for Neural Language Models   \n",
       "133                                                                      Self-Consistency Improves Chain of Thought Reasoning in Language Models   \n",
       "134                                                                Self-Critique Prompting with Large Language Models for Inductive Instructions   \n",
       "135                                                                                         Self-Refine: Iterative Refinement with Self-Feedback   \n",
       "136                                                                Show Your Work: Scratchpads for Intermediate Computation with Language Models   \n",
       "137                                                                          Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding   \n",
       "138                                                                         Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer   \n",
       "139                                                                                       Successive Prompting for Decomposing Complex Questions   \n",
       "140                                         SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains   \n",
       "141                                                    Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models   \n",
       "142                                                                      TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks   \n",
       "143                                                                                       Teaching Algorithmic Reasoning via In-context Learning   \n",
       "144                                                                              The Capacity for Moral Self-Correction in Large Language Models   \n",
       "145                                                                                     The Power of Scale for Parameter-Efficient Prompt Tuning   \n",
       "146                                                                The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning   \n",
       "147                                                                               The Web Can Be Your Oyster for Improving Large Language Models   \n",
       "148                                                                                    Toxicity Detection with Generative Prompt-based Inference   \n",
       "149                                                                          Training language models to follow instructions with human feedback   \n",
       "150                                                                      Tree of Thoughts: Deliberate Problem Solving with Large Language Models   \n",
       "151                                                                TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding   \n",
       "152                                                                        UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation   \n",
       "153                                                                                                Universality and Limitations of Prompt Tuning   \n",
       "154                                                                     Visual-Language Prompt Tuning with Knowledge-guided Context Optimization   \n",
       "155                                               What In-Context Learning \"Learns\" In-Context: Disentangling Task Recognition and Task Learning   \n",
       "156                                                                    Why think step-by-step? Reasoning emerges from the locality of experience   \n",
       "157                                                                             ZeroPrompt: Streaming Acoustic Encoders are Zero-Shot Masked LMs   \n",
       "158                                                      kNN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference   \n",
       "159                                                             -la-carte Prompt Tuning (APT): Combining Distinct Data Via Composable Prompting   \n",
       "\n",
       "                                                                                                                          semantic scholar title  \\\n",
       "0                                                                            A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT   \n",
       "1                                                                                    A Taxonomy of Prompt Modifiers for Text-To-Image Generation   \n",
       "2                                          AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts   \n",
       "3                                                                     ART: Automatic multi-step reasoning and tool-use for large language models   \n",
       "4                                                                               Active Prompting with Chain-of-Thought for Large Language Models   \n",
       "5                                                   An automatically discovered chain-of-thought prompt generalizes to novel models and datasets   \n",
       "6                                                                               Ask Me Anything: A simple strategy for prompting language models   \n",
       "7                                                                                     Few-shot Learning with Retrieval Augmented Language Models   \n",
       "8                                                                 Eliciting Knowledge from Language Models Using Automatically Generated Prompts   \n",
       "9                                                                                  Automatic Chain of Thought Prompting in Large Language Models   \n",
       "10                                                           Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data   \n",
       "11                                                                                                            BERTese: Learning to Speak to BERT   \n",
       "12                                                                           Batch Prompting: Efficient Inference with Large Language Model APIs   \n",
       "13                                                                                       Better Zero-Shot Reasoning with Self-Adaptive Prompting   \n",
       "14                                                                                            Boosted Prompt Ensembles for Large Language Models   \n",
       "15                                            Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints   \n",
       "16                                                      CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale Language Model Society   \n",
       "17                                                                       Calibrate Before Use: Improving Few-Shot Performance of Language Models   \n",
       "18                                                                Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT   \n",
       "19                                                                                         Can We Edit Factual Knowledge by In-Context Learning?   \n",
       "20                                                                                       Chain of Hindsight Aligns Language Models with Feedback   \n",
       "21                                                                         Chain of Thought Prompting Elicits Reasoning in Large Language Models   \n",
       "22                                                                           Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models   \n",
       "23                                                                          Chain-of-Verification Reduces Hallucination in Large Language Models   \n",
       "24                                     ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer   \n",
       "25                                                           CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification   \n",
       "26                                                                                           Complexity-Based Prompting for Multi-Step Reasoning   \n",
       "27                                                                                               Compositional Exemplars for In-context Learning   \n",
       "28                                      Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt   \n",
       "29                                               Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers   \n",
       "30                                                                                              Constitutional AI: Harmlessness from AI Feedback   \n",
       "31                                                                                          Context-faithful Prompting for Large Language Models   \n",
       "32                                                                                                   SHARED OPEN VOCABULARIES AND SEMANTIC MEDIA   \n",
       "33                                                                            Decomposed Prompting: A Modular Approach for Solving Complex Tasks   \n",
       "34                                               Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP   \n",
       "35                                                                      Design Guidelines for Prompt Engineering Text-to-Image Generative Models   \n",
       "36                                                                           Discovering Language Model Behaviors with Model-Written Evaluations   \n",
       "37                                                                                          DocPrompting: Generating Code by Retrieving the Docs   \n",
       "38                                                        Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning   \n",
       "39                                                                                      Dynamic Prompting: A Unified Framework for Prompt Tuning   \n",
       "40                                                                        Effectiveness of Data Augmentation for Prefix Tuning with Limited Data   \n",
       "41                                                                                           Efficient Prompting via Dynamic In-Context Learning   \n",
       "42                                  Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference   \n",
       "43                                                                                              Evaluating Large Language Models Trained on Code   \n",
       "44                                                                                                 Evaluating the Robustness of Discrete Prompts   \n",
       "45                                             Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples   \n",
       "46                                                                       EvoPrompting: Language Models for Code-Level Neural Architecture Search   \n",
       "47                                                                                     Exploring Lottery Prompts for Pre-trained Language Models   \n",
       "48                                                                                  Fairness-guided Few-shot Prompting for Large Language Models   \n",
       "49                                            Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity   \n",
       "50                                                                       Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency   \n",
       "51                                              Flesch or Fumble? Evaluating Readability Standard Alignment of Instruction-Tuned Language Models   \n",
       "52                                                                                        Focused Prefix Tuning for Controllable Text Generation   \n",
       "53                                                                     From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting   \n",
       "54                                                                                                                        GPT-4 Technical Report   \n",
       "55                                                                                       Generated Knowledge Prompting for Commonsense Reasoning   \n",
       "56                                                                            Global Prompt Cell: A Portable Control Module for Effective Prompt   \n",
       "57                                                                      Graph of Thoughts: Solving Elaborate Problems with Large Language Models   \n",
       "58                                                             GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks   \n",
       "59                                                                              Guiding Large Language Models via Directional Stimulus Prompting   \n",
       "60                                                  Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery   \n",
       "61                                                                         Hierarchical Prompting Assists Large Language Model on Web Navigation   \n",
       "62                                                                                                    How Can We Know What Language Models Know?   \n",
       "63                                                                                              How Does In-Context Learning Help Prompt Tuning?   \n",
       "64                                                  How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks   \n",
       "65                                                                                 Ignore Previous Prompt: Attack Techniques For Language Models   \n",
       "66                                                                                                               In-Context Instruction Learning   \n",
       "67                                                                   In-Context Learning of Large Language Models Explained as Kernel Regression   \n",
       "68                                                                                                       Interactive Natural Language Processing   \n",
       "69                                                                                                                 Is GPT-4 a Good Data Analyst?   \n",
       "70                                                          Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning   \n",
       "71                                                                        Language Is Not All You Need: Aligning Perception with Language Models   \n",
       "72                                                        Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought   \n",
       "73                                                                                                         Language Models are Few-Shot Learners   \n",
       "74                                                                                                   Large Language Model Guided Tree-of-Thought   \n",
       "75                                                                                        Large Language Models Are Human-Level Prompt Engineers   \n",
       "76                                                                          Large Language Models Can Be Easily Distracted by Irrelevant Context   \n",
       "77                                                                                                 Large Language Models are Zero-Shot Reasoners   \n",
       "78                                                                                    Large Language Models are reasoners with Self-Verification   \n",
       "79                                                                                     Larger language models do in-context learning differently   \n",
       "80                                                                                                               Learning from Task Descriptions   \n",
       "81                                                                             Learning to Generate Task-Specific Adapters from Task Description   \n",
       "82                                                                                              Learning to Transfer Prompts for Text Generation   \n",
       "83                                                                    Least-to-Most Prompting Enables Complex Reasoning in Large Language Models   \n",
       "84                                                           Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses   \n",
       "85                                                             Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning with LLMs   \n",
       "86                                                                                                                     Let's Verify Step by Step   \n",
       "87   MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning   \n",
       "88                                                         Machine-Generated Text: A Comprehensive Survey of Threat Models and Detection Methods   \n",
       "89                                                                Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations   \n",
       "90                                                                                   Making Pre-trained Language Models Better Few-shot Learners   \n",
       "91                                                                                             Meta-in-context learning in large language models   \n",
       "92                                                                                      Mixture of Soft Prompts for Controllable Data Generation   \n",
       "93                                                                                Model-tuning Via Prompts Makes NLP Models Adversarially Robust   \n",
       "94        More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models   \n",
       "95                                                          MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of Thought Prompting   \n",
       "96                                                                                      Multimodal Chain-of-Thought Reasoning in Language Models   \n",
       "97                                                                         Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning   \n",
       "98                                                                             Multitask Prompted Training Enables Zero-Shot Task Generalization   \n",
       "99                                                     On Second Thought, Lets Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning   \n",
       "100                                                                                    On the Advance of Making Language Models Better Reasoners   \n",
       "101                                                                                    OpenICL: An Open-Source Framework for In-context Learning   \n",
       "102                                                                                                           PAL: Program-aided Language Models   \n",
       "103                                                       PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents   \n",
       "104                                                                                         PPT: Pre-trained Prompt Tuning for Few-shot Learning   \n",
       "105                                            Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models   \n",
       "106                                                                         Post Hoc Explanations of Language Models Can Improve Language Models   \n",
       "107                                                                                                             Pre-Training to Learn in Context   \n",
       "108                                                                                  Prefix-Tuning: Optimizing Continuous Prompts for Generation   \n",
       "109                                                                                  Progressive Prompts: Continual Learning for Language Models   \n",
       "110                                                                                                 Prompt Engineering for Large Language Models   \n",
       "111                                                                   Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm   \n",
       "112                                                    Prompt, Generate, Then Cache: Cascade of Foundation Models Makes Strong Few-Shot Learners   \n",
       "113                                                              PromptChainer: Chaining Large Language Model Prompts through Visual Programming   \n",
       "114                                                                                       Promptagator: Few-shot Dense Retrieval From 8 Examples   \n",
       "115                                                                                                  Prompted Opinion Summarization with GPT-3.5   \n",
       "116                                                                                                               Prompting GPT-3 To Be Reliable   \n",
       "117                                                                   Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL   \n",
       "118                                                                                  REFINER: Reasoning Feedback on Intermediate Representations   \n",
       "119                                                                                             Re-Reading Improves Reasoning in Language Models   \n",
       "120                                                                                   ReAct: Synergizing Reasoning and Acting in Language Models   \n",
       "121                                                                                   Reasoning with Language Model is Planning with World Model   \n",
       "122                                                                                                         Recitation-Augmented Language Models   \n",
       "123                                                                       Reflexion: an autonomous agent with dynamic memory and self-reflection   \n",
       "124                                                                                           Reframing Instructional Prompts to GPTks Language   \n",
       "125                                                              Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling   \n",
       "126                                                                  Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?   \n",
       "127                                                                             Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks   \n",
       "128                                                                                Revisiting Automated Prompting: Are We Actually Doing Better?   \n",
       "129                                                                             Satisfiability-Aided Language Models Using Declarative Prompting   \n",
       "130                                                                 Scalable Prompt Generation for Semi-supervised Learning with Language Models   \n",
       "131                                                                                                Scaling Instruction-Finetuned Language Models   \n",
       "132                                                                                                      Scaling Laws for Neural Language Models   \n",
       "133                                                                      Self-Consistency Improves Chain of Thought Reasoning in Language Models   \n",
       "134                                                                Self-Critique Prompting with Large Language Models for Inductive Instructions   \n",
       "135                                                                                         Self-Refine: Iterative Refinement with Self-Feedback   \n",
       "136                                                                Show Your Work: Scratchpads for Intermediate Computation with Language Models   \n",
       "137                                                                          Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding   \n",
       "138                                                                         Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer   \n",
       "139                                                                                       Successive Prompting for Decomposing Complex Questions   \n",
       "140                                         SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains   \n",
       "141                                                    Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models   \n",
       "142                                                                      TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks   \n",
       "143                                                                                       Teaching Algorithmic Reasoning via In-context Learning   \n",
       "144                                                                              The Capacity for Moral Self-Correction in Large Language Models   \n",
       "145                                                                                     The Power of Scale for Parameter-Efficient Prompt Tuning   \n",
       "146                                                                The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning   \n",
       "147                                                                               The Web Can Be Your Oyster for Improving Large Language Models   \n",
       "148                                                                                    Toxicity Detection with Generative Prompt-based Inference   \n",
       "149                                                                          Training language models to follow instructions with human feedback   \n",
       "150                                                                      Tree of Thoughts: Deliberate Problem Solving with Large Language Models   \n",
       "151                                                                TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding   \n",
       "152                                                                        UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation   \n",
       "153                                                                                                Universality and Limitations of Prompt Tuning   \n",
       "154                                                                     Visual-Language Prompt Tuning with Knowledge-Guided Context Optimization   \n",
       "155                                               What In-Context Learning \"Learns\" In-Context: Disentangling Task Recognition and Task Learning   \n",
       "156                                                                    Why think step-by-step? Reasoning emerges from the locality of experience   \n",
       "157                                                                             ZeroPrompt: Streaming Acoustic Encoders are Zero-Shot Masked LMs   \n",
       "158                                                      kNN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference   \n",
       "159                                                             -la-carte Prompt Tuning (APT): Combining Distinct Data Via Composable Prompting   \n",
       "\n",
       "    ss_publication_date  ss_year  citation_count  \\\n",
       "0            2023-02-21     2023             163   \n",
       "1            2022-04-20     2022              43   \n",
       "2            2021-10-04     2021             146   \n",
       "3            2023-03-16     2023              39   \n",
       "4            2023-02-23     2023              34   \n",
       "5            2023-05-04     2023               6   \n",
       "6            2022-10-05     2022              65   \n",
       "7            2022-08-05     2022             202   \n",
       "8            2020-10-29     2020             217   \n",
       "9            2022-10-07     2022             201   \n",
       "10           2023-02-24     2023              21   \n",
       "11           2021-03-09     2021              74   \n",
       "12           2023-01-19     2023              10   \n",
       "13           2023-05-23     2023               8   \n",
       "14           2023-04-12     2023              12   \n",
       "15           2023-02-17     2023               6   \n",
       "16           2023-03-31     2023              64   \n",
       "17           2021-02-19     2021             616   \n",
       "18           2023-02-19     2023              73   \n",
       "19           2023-05-22     2023              11   \n",
       "20           2023-02-06     2023              37   \n",
       "21           2022-01-28     2022            2105   \n",
       "22                  NaN     2023               2   \n",
       "23           2023-09-20     2023              10   \n",
       "24           2023-06-13     2023              12   \n",
       "25           2023-03-07     2023               3   \n",
       "26           2022-10-03     2022             116   \n",
       "27           2023-02-11     2023              24   \n",
       "28           2023-05-17     2023               3   \n",
       "29           2023-09-15     2023               8   \n",
       "30           2022-12-15     2022             307   \n",
       "31           2023-03-20     2023              21   \n",
       "32                  NaN     2012               1   \n",
       "33           2022-10-05     2022             104   \n",
       "34           2022-12-28     2022              57   \n",
       "35           2021-09-14     2021             142   \n",
       "36           2022-12-19     2022              87   \n",
       "37           2022-07-13     2022              27   \n",
       "38           2022-09-29     2022              51   \n",
       "39           2023-03-06     2023               4   \n",
       "40                  NaN     2023               0   \n",
       "41           2023-05-18     2023               5   \n",
       "42           2022-11-21     2022              16   \n",
       "43           2021-07-07     2021            1617   \n",
       "44           2023-02-11     2023               4   \n",
       "45           2022-09-05     2022              12   \n",
       "46           2023-02-28     2023              12   \n",
       "47           2023-05-31     2023               1   \n",
       "48           2023-03-23     2023               2   \n",
       "49           2021-04-18     2021             408   \n",
       "50           2023-05-18     2023               1   \n",
       "51           2023-09-11     2023               0   \n",
       "52           2023-06-01     2023               0   \n",
       "53           2023-09-08     2023               2   \n",
       "54           2023-03-15     2023            1529   \n",
       "55           2021-10-15     2021             114   \n",
       "56           2023-04-12     2023               0   \n",
       "57           2023-08-18     2023              35   \n",
       "58           2023-02-16     2023              16   \n",
       "59           2023-02-22     2023              10   \n",
       "60           2023-02-07     2023              38   \n",
       "61           2023-05-23     2023               3   \n",
       "62           2019-11-28     2019             763   \n",
       "63           2023-02-22     2023               5   \n",
       "64           2023-03-01     2023              24   \n",
       "65           2022-11-17     2022              68   \n",
       "66           2023-02-28     2023              19   \n",
       "67                  NaN     2023               4   \n",
       "68           2023-05-22     2023              14   \n",
       "69           2023-05-24     2023              17   \n",
       "70           2023-03-18     2023              16   \n",
       "71           2023-02-27     2023             154   \n",
       "72           2022-10-03     2022              58   \n",
       "73           2020-05-28     2020           16440   \n",
       "74           2023-05-15     2023              26   \n",
       "75           2022-11-03     2022             194   \n",
       "76           2023-01-31     2023              70   \n",
       "77           2022-05-24     2022             882   \n",
       "78                  NaN     2022              31   \n",
       "79           2023-03-07     2023              87   \n",
       "80           2020-11-01     2020              64   \n",
       "81           2021-01-02     2021              18   \n",
       "82           2022-05-03     2022              18   \n",
       "83           2022-05-21     2022             369   \n",
       "84           2023-05-30     2023               1   \n",
       "85           2023-05-19     2023               6   \n",
       "86           2023-05-31     2023              64   \n",
       "87           2022-05-01     2022              31   \n",
       "88           2022-10-13     2022              28   \n",
       "89           2022-05-24     2022              86   \n",
       "90           2021-01-01     2021            1082   \n",
       "91           2023-05-22     2023               5   \n",
       "92           2023-03-02     2023               3   \n",
       "93           2023-03-13     2023               3   \n",
       "94                  NaN     2023              37   \n",
       "95           2023-05-26     2023               1   \n",
       "96           2023-02-02     2023              91   \n",
       "97           2023-03-06     2023              21   \n",
       "98           2021-10-15     2021             924   \n",
       "99           2022-12-15     2022              23   \n",
       "100                 NaN     2022             106   \n",
       "101          2023-03-06     2023              12   \n",
       "102          2022-11-18     2022             198   \n",
       "103          2023-05-23     2023               4   \n",
       "104          2021-09-09     2021             234   \n",
       "105          2023-05-06     2023              46   \n",
       "106          2023-05-19     2023               5   \n",
       "107          2023-05-16     2023               5   \n",
       "108                 NaN     2021            1682   \n",
       "109          2023-01-29     2023              20   \n",
       "110                 NaN     2023               3   \n",
       "111          2021-02-15     2021             317   \n",
       "112          2023-03-03     2023              30   \n",
       "113          2022-03-13     2022              78   \n",
       "114          2022-09-23     2022              66   \n",
       "115          2022-11-29     2022               8   \n",
       "116          2022-10-17     2022              72   \n",
       "117          2023-09-13     2023               1   \n",
       "118          2023-04-04     2023              35   \n",
       "119          2023-09-12     2023               2   \n",
       "120          2022-10-06     2022             332   \n",
       "121          2023-05-24     2023              34   \n",
       "122          2022-10-04     2022              37   \n",
       "123                 NaN     2023             105   \n",
       "124          2021-09-16     2021             121   \n",
       "125          2023-05-17     2023               5   \n",
       "126          2022-02-25     2022             431   \n",
       "127          2020-05-22     2020             956   \n",
       "128          2023-04-07     2023               0   \n",
       "129          2023-05-16     2023               3   \n",
       "130          2023-02-18     2023               1   \n",
       "131          2022-10-20     2022             823   \n",
       "132          2020-01-23     2020            1530   \n",
       "133          2022-03-21     2022             659   \n",
       "134          2023-05-23     2023               3   \n",
       "135          2023-03-30     2023             201   \n",
       "136          2021-11-30     2021             315   \n",
       "137          2023-07-28     2023               5   \n",
       "138          2023-03-03     2023               4   \n",
       "139          2022-12-08     2022              40   \n",
       "140          2023-02-14     2023               2   \n",
       "141          2023-02-01     2023              26   \n",
       "142          2023-05-19     2023               4   \n",
       "143          2022-11-15     2022              52   \n",
       "144          2023-02-15     2023              49   \n",
       "145          2021-04-18     2021            1530   \n",
       "146          2022-05-06     2022              47   \n",
       "147          2023-05-18     2023               0   \n",
       "148          2022-05-24     2022              11   \n",
       "149          2022-03-04     2022            3009   \n",
       "150          2023-05-17     2023             227   \n",
       "151          2023-05-19     2023               0   \n",
       "152          2023-03-15     2023               8   \n",
       "153          2023-05-30     2023               0   \n",
       "154          2023-03-23     2023              11   \n",
       "155          2023-05-16     2023              19   \n",
       "156          2023-04-07     2023               8   \n",
       "157          2023-05-18     2023               1   \n",
       "158          2023-03-24     2023              16   \n",
       "159          2023-02-15     2023               4   \n",
       "\n",
       "                                                                                                                                           query  \\\n",
       "0                                                                            A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT   \n",
       "1                                                                                    A Taxonomy of Prompt Modifiers for Text To Image Generation   \n",
       "2                                          AI Chains: Transparent and Controllable Human AI Interaction by Chaining Large Language Model Prompts   \n",
       "3                                                                     ART: Automatic multi step reasoning and tool use for large language models   \n",
       "4                                                                               Active Prompting with Chain of Thought for Large Language Models   \n",
       "5                                                   An automatically discovered chain of thought prompt generalizes to novel models and datasets   \n",
       "6                                                                               Ask Me Anything: A simple strategy for prompting language models   \n",
       "7                                                                              Atlas: Few shot Learning with Retrieval Augmented Language Models   \n",
       "8                                                      AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts   \n",
       "9                                                                                  Automatic Chain of Thought Prompting in Large Language Models   \n",
       "10                                                           Automatic Prompt Augmentation and Selection with Chain of Thought from Labeled Data   \n",
       "11                                                                                                            BERTese: Learning to Speak to BERT   \n",
       "12                                                                                            Batch Prompting: Efficient Inference with LLM APIs   \n",
       "13                                                                                       Better Zero Shot Reasoning with Self Adaptive Prompting   \n",
       "14                                                                                            Boosted Prompt Ensembles for Large Language Models   \n",
       "15                                            Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints   \n",
       "16                                                      CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale Language Model Society   \n",
       "17                                                                       Calibrate Before Use: Improving Few Shot Performance of Language Models   \n",
       "18                                                                Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine tuned BERT   \n",
       "19                                                                                         Can We Edit Factual Knowledge by In Context Learning?   \n",
       "20                                                                                       Chain of Hindsight Aligns Language Models with Feedback   \n",
       "21                                                                         Chain of Thought Prompting Elicits Reasoning in Large Language Models   \n",
       "22                                                                           Chain of Symbol Prompting Elicits Planning in Large Langauge Models   \n",
       "23                                                                          Chain of Verification Reduces Hallucination in Large Language Models   \n",
       "24                                     ChatGPT vs Human authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer   \n",
       "25                                                           CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification   \n",
       "26                                                                                           Complexity Based Prompting for Multi Step Reasoning   \n",
       "27                                                                                               Compositional Exemplars for In context Learning   \n",
       "28                                      Compress, Then Prompt: Improving Accuracy Efficiency Trade off of LLM Inference with Transferable Prompt   \n",
       "29                                               Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers   \n",
       "30                                                                                              Constitutional AI: Harmlessness from AI Feedback   \n",
       "31                                                                                          Context faithful Prompting for Large Language Models   \n",
       "32                                                     CooK: Empowering General Purpose Language Models with Modular and Collaborative Knowledge   \n",
       "33                                                                            Decomposed Prompting: A Modular Approach for Solving Complex Tasks   \n",
       "34                                               Demonstrate Search Predict: Composing retrieval and language models for knowledge intensive NLP   \n",
       "35                                                                      Design Guidelines for Prompt Engineering Text to Image Generative Models   \n",
       "36                                                                           Discovering Language Model Behaviors with Model Written Evaluations   \n",
       "37                                                                                          DocPrompting: Generating Code by Retrieving the Docs   \n",
       "38                                                        Dynamic Prompt Learning via Policy Gradient for Semi structured Mathematical Reasoning   \n",
       "39                                                                                      Dynamic Prompting: A Unified Framework for Prompt Tuning   \n",
       "40                                                                        Effectiveness of Data Augmentation for Prefix Tuning with Limited Data   \n",
       "41                                                                                           Efficient Prompting via Dynamic In Context Learning   \n",
       "42                                  Enhancing Self Consistency and Performance of Pre Trained Language Models through Natural Language Inference   \n",
       "43                                                                                              Evaluating Large Language Models Trained on Code   \n",
       "44                                                                                                 Evaluating the Robustness of Discrete Prompts   \n",
       "45                                             Evaluating the Susceptibility of Pre Trained Language Models via Handcrafted Adversarial Examples   \n",
       "46                                                                       EvoPrompting: Language Models for Code Level Neural Architecture Search   \n",
       "47                                                                                     Exploring Lottery Prompts for Pre trained Language Models   \n",
       "48                                                                                  Fairness guided Few shot Prompting for Large Language Models   \n",
       "49                                            Fantastically Ordered Prompts and Where to Find Them: Overcoming Few Shot Prompt Order Sensitivity   \n",
       "50                                                                       Flatness Aware Prompt Selection Improves Accuracy and Sample Efficiency   \n",
       "51                                              Flesch or Fumble? Evaluating Readability Standard Alignment of Instruction Tuned Language Models   \n",
       "52                                                                                        Focused Prefix Tuning for Controllable Text Generation   \n",
       "53                                                                     From Sparse to Dense: GPT 4 Summarization with Chain of Density Prompting   \n",
       "54                                                                                                                        GPT 4 Technical Report   \n",
       "55                                                                                       Generated Knowledge Prompting for Commonsense Reasoning   \n",
       "56                                                                            Global Prompt Cell: A Portable Control Module for Effective Prompt   \n",
       "57                                                                      Graph of Thoughts: Solving Elaborate Problems with Large Language Models   \n",
       "58                                                             GraphPrompt: Unifying Pre Training and Downstream Tasks for Graph Neural Networks   \n",
       "59                                                                              Guiding Large Language Models via Directional Stimulus Prompting   \n",
       "60                                                  Hard Prompts Made Easy: Gradient Based Discrete Optimization for Prompt Tuning and Discovery   \n",
       "61                                                                         Hierarchical Prompting Assists Large Language Model on Web Navigation   \n",
       "62                                                                                                    How Can We Know What Language Models Know?   \n",
       "63                                                                                              How Does In Context Learning Help Prompt Tuning?   \n",
       "64                                                  How Robust is GPT 3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks   \n",
       "65                                                                                 Ignore Previous Prompt: Attack Techniques For Language Models   \n",
       "66                                                                                                               In Context Instruction Learning   \n",
       "67                                                                   In Context Learning of Large Language Models Explained as Kernel Regression   \n",
       "68                                                                                                       Interactive Natural Language Processing   \n",
       "69                                                                                                                 Is GPT 4 a Good Data Analyst?   \n",
       "70                                                          Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning   \n",
       "71                                                                        Language Is Not All You Need: Aligning Perception with Language Models   \n",
       "72                                                        Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain of Thought   \n",
       "73                                                                                                         Language Models are Few Shot Learners   \n",
       "74                                                                                                   Large Language Model Guided Tree of Thought   \n",
       "75                                                                                        Large Language Models Are Human Level Prompt Engineers   \n",
       "76                                                                          Large Language Models Can Be Easily Distracted by Irrelevant Context   \n",
       "77                                                                                                 Large Language Models are Zero Shot Reasoners   \n",
       "78                                                                                    Large Language Models are reasoners with Self Verification   \n",
       "79                                                                                     Larger language models do in context learning differently   \n",
       "80                                                                                                               Learning from Task Descriptions   \n",
       "81                                                                             Learning to Generate Task Specific Adapters from Task Description   \n",
       "82                                                                                              Learning to Transfer Prompts for Text Generation   \n",
       "83                                                                    Least to most promptingenables complex reasoning in large language models   \n",
       "84                                                           Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses   \n",
       "85                                                             Let's Sample Step by Step: Adaptive Consistency for Efficient Reasoning with LLMs   \n",
       "86                                                                                                                     Let's Verify Step by Step   \n",
       "87   MRKL Systems: A modular, neuro symbolic architecture that combines large language models, external knowledge sources and discrete reasoning   \n",
       "88                                                         Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods   \n",
       "89                                                                Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations   \n",
       "90                                                                                   Making Pre trained Language Models Better Few shot Learners   \n",
       "91                                                                                             Meta in context learning in large language models   \n",
       "92                                                                                      Mixture of Soft Prompts for Controllable Data Generation   \n",
       "93                                                                                Model tuning Via Prompts Makes NLP Models Adversarially Robust   \n",
       "94        More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application Integrated Large Language Models   \n",
       "95                                                          MultiTool CoT: GPT 3 Can Use Multiple External Tools with Chain of Thought Prompting   \n",
       "96                                                                                      Multimodal Chain of Thought Reasoning in Language Models   \n",
       "97                                                                         Multitask Prompt Tuning Enables Parameter Efficient Transfer Learning   \n",
       "98                                                                             Multitask Prompted Training Enables Zero Shot Task Generalization   \n",
       "99                                                     On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero Shot Reasoning   \n",
       "100                                                                                    On the Advance of Making Language Models Better Reasoners   \n",
       "101                                                                                    OpenICL: An Open Source Framework for In context Learning   \n",
       "102                                                                                                           PAL: Program aided Language Models   \n",
       "103                                                       PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents   \n",
       "104                                                                                         PPT: Pre trained Prompt Tuning for Few shot Learning   \n",
       "105                                            Plan and Solve Prompting: Improving Zero Shot Chain of Thought Reasoning by Large Language Models   \n",
       "106                                                                         Post Hoc Explanations of Language Models Can Improve Language Models   \n",
       "107                                                                                                             Pre Training to Learn in Context   \n",
       "108                                                                                  Prefix Tuning: Optimizing Continuous Prompts for Generation   \n",
       "109                                                                                  Progressive Prompts: Continual Learning for Language Models   \n",
       "110                                                                                                 Prompt Engineering for Large Language Models   \n",
       "111                                                                   Prompt Programming for Large Language Models: Beyond the Few Shot Paradigm   \n",
       "112                                                    Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few shot Learners   \n",
       "113                                                              PromptChainer: Chaining Large Language Model Prompts through Visual Programming   \n",
       "114                                                                                       Promptagator: Few shot Dense Retrieval From 8 Examples   \n",
       "115                                                                                                  Prompted Opinion Summarization with GPT 3.5   \n",
       "116                                                                                                               Prompting GPT 3 To Be Reliable   \n",
       "117                                                                   Query Dependent Prompt Evaluation and Optimization with Offline Inverse RL   \n",
       "118                                                                                  REFINER: Reasoning Feedback on Intermediate Representations   \n",
       "119                                                                                             Re Reading Improves Reasoning in Language Models   \n",
       "120                                                                                   ReAct: Synergizing Reasoning and Acting in Language Models   \n",
       "121                                                                                   Reasoning with Language Model is Planning with World Model   \n",
       "122                                                                                                         Recitation Augmented Language Models   \n",
       "123                                                                       Reflexion: an autonomous agent with dynamic memory and self reflection   \n",
       "124                                                                                           Reframing Instructional Prompts to GPTk's Language   \n",
       "125                                                              Reprompting: Automated Chain of Thought Prompt Inference Through Gibbs Sampling   \n",
       "126                                                                  Rethinking the Role of Demonstrations: What Makes In Context Learning Work?   \n",
       "127                                                                             Retrieval Augmented Generation for Knowledge Intensive NLP Tasks   \n",
       "128                                                                                Revisiting Automated Prompting: Are We Actually Doing Better?   \n",
       "129                                                                             Satisfiability Aided Language Models Using Declarative Prompting   \n",
       "130                                                                 Scalable Prompt Generation for Semi supervised Learning with Language Models   \n",
       "131                                                                                                Scaling Instruction Finetuned Language Models   \n",
       "132                                                                                                      Scaling Laws for Neural Language Models   \n",
       "133                                                                      Self Consistency Improves Chain of Thought Reasoning in Language Models   \n",
       "134                                                                Self Critique Prompting with Large Language Models for Inductive Instructions   \n",
       "135                                                                                         Self Refine: Iterative Refinement with Self Feedback   \n",
       "136                                                                Show Your Work: Scratchpads for Intermediate Computation with Language Models   \n",
       "137                                                                          Skeleton of Thought: Large Language Models Can Do Parallel Decoding   \n",
       "138                                                                         Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer   \n",
       "139                                                                                       Successive Prompting for Decomposing Complex Questions   \n",
       "140                                         SwitchPrompt: Learning Domain Specific Gated Soft Prompts for Classification in Low Resource Domains   \n",
       "141                                                    Synthetic Prompting: Generating Chain of Thought Demonstrations for Large Language Models   \n",
       "142                                                                      TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks   \n",
       "143                                                                                       Teaching Algorithmic Reasoning via In context Learning   \n",
       "144                                                                              The Capacity for Moral Self Correction in Large Language Models   \n",
       "145                                                                                     The Power of Scale for Parameter Efficient Prompt Tuning   \n",
       "146                                                                The Unreliability of Explanations in Few shot Prompting for Textual Reasoning   \n",
       "147                                                                               The Web Can Be Your Oyster for Improving Large Language Models   \n",
       "148                                                                                    Toxicity Detection with Generative Prompt based Inference   \n",
       "149                                                                          Training language models to follow instructions with human feedback   \n",
       "150                                                                      Tree of Thoughts: Deliberate Problem Solving with Large Language Models   \n",
       "151                                                                TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding   \n",
       "152                                                                        UPRISE: Universal Prompt Retrieval for Improving Zero Shot Evaluation   \n",
       "153                                                                                                Universality and Limitations of Prompt Tuning   \n",
       "154                                                                     Visual Language Prompt Tuning with Knowledge guided Context Optimization   \n",
       "155                                               What In Context Learning \"Learns\" In Context: Disentangling Task Recognition and Task Learning   \n",
       "156                                                                    Why think step by step? Reasoning emerges from the locality of experience   \n",
       "157                                                                             ZeroPrompt: Streaming Acoustic Encoders are Zero Shot Masked LMs   \n",
       "158                                                      kNN Prompting: Beyond Context Learning with Calibration Free Nearest Neighbor Inference   \n",
       "159                                                              la carte Prompt Tuning (APT): Combining Distinct Data Via Composable Prompting   \n",
       "\n",
       "    day_queried  \n",
       "0    2023-10-22  \n",
       "1    2023-10-22  \n",
       "2    2023-10-22  \n",
       "3    2023-10-22  \n",
       "4    2023-10-22  \n",
       "5    2023-10-22  \n",
       "6    2023-10-22  \n",
       "7    2023-10-22  \n",
       "8    2023-10-22  \n",
       "9    2023-10-22  \n",
       "10   2023-10-22  \n",
       "11   2023-10-22  \n",
       "12   2023-10-22  \n",
       "13   2023-10-22  \n",
       "14   2023-10-22  \n",
       "15   2023-10-22  \n",
       "16   2023-10-22  \n",
       "17   2023-10-22  \n",
       "18   2023-10-22  \n",
       "19   2023-10-22  \n",
       "20   2023-10-22  \n",
       "21   2023-10-22  \n",
       "22   2023-10-22  \n",
       "23   2023-10-22  \n",
       "24   2023-10-22  \n",
       "25   2023-10-22  \n",
       "26   2023-10-22  \n",
       "27   2023-10-22  \n",
       "28   2023-10-22  \n",
       "29   2023-10-22  \n",
       "30   2023-10-22  \n",
       "31   2023-10-22  \n",
       "32   2023-10-22  \n",
       "33   2023-10-22  \n",
       "34   2023-10-22  \n",
       "35   2023-10-22  \n",
       "36   2023-10-22  \n",
       "37   2023-10-22  \n",
       "38   2023-10-22  \n",
       "39   2023-10-22  \n",
       "40   2023-10-22  \n",
       "41   2023-10-22  \n",
       "42   2023-10-22  \n",
       "43   2023-10-22  \n",
       "44   2023-10-22  \n",
       "45   2023-10-22  \n",
       "46   2023-10-22  \n",
       "47   2023-10-22  \n",
       "48   2023-10-22  \n",
       "49   2023-10-22  \n",
       "50   2023-10-22  \n",
       "51   2023-10-22  \n",
       "52   2023-10-22  \n",
       "53   2023-10-22  \n",
       "54   2023-10-22  \n",
       "55   2023-10-22  \n",
       "56   2023-10-22  \n",
       "57   2023-10-22  \n",
       "58   2023-10-22  \n",
       "59   2023-10-22  \n",
       "60   2023-10-22  \n",
       "61   2023-10-22  \n",
       "62   2023-10-22  \n",
       "63   2023-10-22  \n",
       "64   2023-10-22  \n",
       "65   2023-10-22  \n",
       "66   2023-10-22  \n",
       "67   2023-10-22  \n",
       "68   2023-10-22  \n",
       "69   2023-10-22  \n",
       "70   2023-10-22  \n",
       "71   2023-10-22  \n",
       "72   2023-10-22  \n",
       "73   2023-10-22  \n",
       "74   2023-10-22  \n",
       "75   2023-10-22  \n",
       "76   2023-10-22  \n",
       "77   2023-10-22  \n",
       "78   2023-10-22  \n",
       "79   2023-10-22  \n",
       "80   2023-10-22  \n",
       "81   2023-10-22  \n",
       "82   2023-10-22  \n",
       "83   2023-10-22  \n",
       "84   2023-10-22  \n",
       "85   2023-10-22  \n",
       "86   2023-10-22  \n",
       "87   2023-10-22  \n",
       "88   2023-10-22  \n",
       "89   2023-10-22  \n",
       "90   2023-10-22  \n",
       "91   2023-10-22  \n",
       "92   2023-10-22  \n",
       "93   2023-10-22  \n",
       "94   2023-10-22  \n",
       "95   2023-10-22  \n",
       "96   2023-10-22  \n",
       "97   2023-10-22  \n",
       "98   2023-10-22  \n",
       "99   2023-10-22  \n",
       "100  2023-10-22  \n",
       "101  2023-10-22  \n",
       "102  2023-10-22  \n",
       "103  2023-10-22  \n",
       "104  2023-10-22  \n",
       "105  2023-10-22  \n",
       "106  2023-10-22  \n",
       "107  2023-10-22  \n",
       "108  2023-10-22  \n",
       "109  2023-10-22  \n",
       "110  2023-10-22  \n",
       "111  2023-10-22  \n",
       "112  2023-10-22  \n",
       "113  2023-10-22  \n",
       "114  2023-10-22  \n",
       "115  2023-10-22  \n",
       "116  2023-10-22  \n",
       "117  2023-10-22  \n",
       "118  2023-10-22  \n",
       "119  2023-10-22  \n",
       "120  2023-10-22  \n",
       "121  2023-10-22  \n",
       "122  2023-10-22  \n",
       "123  2023-10-22  \n",
       "124  2023-10-22  \n",
       "125  2023-10-22  \n",
       "126  2023-10-22  \n",
       "127  2023-10-22  \n",
       "128  2023-10-22  \n",
       "129  2023-10-22  \n",
       "130  2023-10-22  \n",
       "131  2023-10-22  \n",
       "132  2023-10-22  \n",
       "133  2023-10-22  \n",
       "134  2023-10-22  \n",
       "135  2023-10-22  \n",
       "136  2023-10-22  \n",
       "137  2023-10-22  \n",
       "138  2023-10-22  \n",
       "139  2023-10-22  \n",
       "140  2023-10-22  \n",
       "141  2023-10-22  \n",
       "142  2023-10-22  \n",
       "143  2023-10-22  \n",
       "144  2023-10-22  \n",
       "145  2023-10-22  \n",
       "146  2023-10-22  \n",
       "147  2023-10-22  \n",
       "148  2023-10-22  \n",
       "149  2023-10-22  \n",
       "150  2023-10-22  \n",
       "151  2023-10-22  \n",
       "152  2023-10-22  \n",
       "153  2023-10-22  \n",
       "154  2023-10-22  \n",
       "155  2023-10-22  \n",
       "156  2023-10-22  \n",
       "157  2023-10-22  \n",
       "158  2023-10-22  \n",
       "159  2023-10-22  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data \"Semantic Scholar Citations - First Pass.xlsx\"\n",
    "semantic_scholar_df_first_pass = pd.read_excel('Semantic Scholar Citations - First Pass.xlsx', sheet_name='Sheet1')\n",
    "\n",
    "semantic_scholar_df_first_pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper title</th>\n",
       "      <th>semantic scholar title</th>\n",
       "      <th>ss_publication_date</th>\n",
       "      <th>ss_year</th>\n",
       "      <th>citation_count</th>\n",
       "      <th>query</th>\n",
       "      <th>day_queried</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT</td>\n",
       "      <td>A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT</td>\n",
       "      <td>2023-02-21</td>\n",
       "      <td>2023</td>\n",
       "      <td>163</td>\n",
       "      <td>A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Taxonomy of Prompt Modifiers for Text-To-Image Generation</td>\n",
       "      <td>A Taxonomy of Prompt Modifiers for Text-To-Image Generation</td>\n",
       "      <td>2022-04-20</td>\n",
       "      <td>2022</td>\n",
       "      <td>43</td>\n",
       "      <td>A Taxonomy of Prompt Modifiers for Text To Image Generation</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts</td>\n",
       "      <td>AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts</td>\n",
       "      <td>2021-10-04</td>\n",
       "      <td>2021</td>\n",
       "      <td>146</td>\n",
       "      <td>AI Chains: Transparent and Controllable Human AI Interaction by Chaining Large Language Model Prompts</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ART: Automatic multi-step reasoning and tool-use for large language models</td>\n",
       "      <td>ART: Automatic multi-step reasoning and tool-use for large language models</td>\n",
       "      <td>2023-03-16</td>\n",
       "      <td>2023</td>\n",
       "      <td>39</td>\n",
       "      <td>ART: Automatic multi step reasoning and tool use for large language models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Active Prompting with Chain-of-Thought for Large Language Models</td>\n",
       "      <td>Active Prompting with Chain-of-Thought for Large Language Models</td>\n",
       "      <td>2023-02-23</td>\n",
       "      <td>2023</td>\n",
       "      <td>34</td>\n",
       "      <td>Active Prompting with Chain of Thought for Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>An automatically discovered chain-of-thought prompt generalizes to novel models and datasets</td>\n",
       "      <td>An automatically discovered chain-of-thought prompt generalizes to novel models and datasets</td>\n",
       "      <td>2023-05-04</td>\n",
       "      <td>2023</td>\n",
       "      <td>6</td>\n",
       "      <td>An automatically discovered chain of thought prompt generalizes to novel models and datasets</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ask Me Anything: A simple strategy for prompting language models</td>\n",
       "      <td>Ask Me Anything: A simple strategy for prompting language models</td>\n",
       "      <td>2022-10-05</td>\n",
       "      <td>2022</td>\n",
       "      <td>65</td>\n",
       "      <td>Ask Me Anything: A simple strategy for prompting language models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Atlas: Few-shot Learning with Retrieval Augmented Language Models</td>\n",
       "      <td>Few-shot Learning with Retrieval Augmented Language Models</td>\n",
       "      <td>2022-08-05</td>\n",
       "      <td>2022</td>\n",
       "      <td>202</td>\n",
       "      <td>Atlas: Few shot Learning with Retrieval Augmented Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts</td>\n",
       "      <td>Eliciting Knowledge from Language Models Using Automatically Generated Prompts</td>\n",
       "      <td>2020-10-29</td>\n",
       "      <td>2020</td>\n",
       "      <td>217</td>\n",
       "      <td>AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Automatic Chain of Thought Prompting in Large Language Models</td>\n",
       "      <td>Automatic Chain of Thought Prompting in Large Language Models</td>\n",
       "      <td>2022-10-07</td>\n",
       "      <td>2022</td>\n",
       "      <td>201</td>\n",
       "      <td>Automatic Chain of Thought Prompting in Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data</td>\n",
       "      <td>Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data</td>\n",
       "      <td>2023-02-24</td>\n",
       "      <td>2023</td>\n",
       "      <td>21</td>\n",
       "      <td>Automatic Prompt Augmentation and Selection with Chain of Thought from Labeled Data</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BERTese: Learning to Speak to BERT</td>\n",
       "      <td>BERTese: Learning to Speak to BERT</td>\n",
       "      <td>2021-03-09</td>\n",
       "      <td>2021</td>\n",
       "      <td>74</td>\n",
       "      <td>BERTese: Learning to Speak to BERT</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Batch Prompting: Efficient Inference with LLM APIs</td>\n",
       "      <td>Batch Prompting: Efficient Inference with Large Language Model APIs</td>\n",
       "      <td>2023-01-19</td>\n",
       "      <td>2023</td>\n",
       "      <td>10</td>\n",
       "      <td>Batch Prompting: Efficient Inference with LLM APIs</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Better Zero-Shot Reasoning with Self-Adaptive Prompting</td>\n",
       "      <td>Better Zero-Shot Reasoning with Self-Adaptive Prompting</td>\n",
       "      <td>2023-05-23</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>Better Zero Shot Reasoning with Self Adaptive Prompting</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Boosted Prompt Ensembles for Large Language Models</td>\n",
       "      <td>Boosted Prompt Ensembles for Large Language Models</td>\n",
       "      <td>2023-04-12</td>\n",
       "      <td>2023</td>\n",
       "      <td>12</td>\n",
       "      <td>Boosted Prompt Ensembles for Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints</td>\n",
       "      <td>Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints</td>\n",
       "      <td>2023-02-17</td>\n",
       "      <td>2023</td>\n",
       "      <td>6</td>\n",
       "      <td>Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale Language Model Society</td>\n",
       "      <td>CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale Language Model Society</td>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>2023</td>\n",
       "      <td>64</td>\n",
       "      <td>CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale Language Model Society</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Calibrate Before Use: Improving Few-Shot Performance of Language Models</td>\n",
       "      <td>Calibrate Before Use: Improving Few-Shot Performance of Language Models</td>\n",
       "      <td>2021-02-19</td>\n",
       "      <td>2021</td>\n",
       "      <td>616</td>\n",
       "      <td>Calibrate Before Use: Improving Few Shot Performance of Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT</td>\n",
       "      <td>Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT</td>\n",
       "      <td>2023-02-19</td>\n",
       "      <td>2023</td>\n",
       "      <td>73</td>\n",
       "      <td>Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine tuned BERT</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Can We Edit Factual Knowledge by In-Context Learning?</td>\n",
       "      <td>Can We Edit Factual Knowledge by In-Context Learning?</td>\n",
       "      <td>2023-05-22</td>\n",
       "      <td>2023</td>\n",
       "      <td>11</td>\n",
       "      <td>Can We Edit Factual Knowledge by In Context Learning?</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Chain of Hindsight Aligns Language Models with Feedback</td>\n",
       "      <td>Chain of Hindsight Aligns Language Models with Feedback</td>\n",
       "      <td>2023-02-06</td>\n",
       "      <td>2023</td>\n",
       "      <td>37</td>\n",
       "      <td>Chain of Hindsight Aligns Language Models with Feedback</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Chain of Thought Prompting Elicits Reasoning in Large Language Models</td>\n",
       "      <td>Chain of Thought Prompting Elicits Reasoning in Large Language Models</td>\n",
       "      <td>2022-01-28</td>\n",
       "      <td>2022</td>\n",
       "      <td>2105</td>\n",
       "      <td>Chain of Thought Prompting Elicits Reasoning in Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models</td>\n",
       "      <td>Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>Chain of Symbol Prompting Elicits Planning in Large Langauge Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Chain-of-Verification Reduces Hallucination in Large Language Models</td>\n",
       "      <td>Chain-of-Verification Reduces Hallucination in Large Language Models</td>\n",
       "      <td>2023-09-20</td>\n",
       "      <td>2023</td>\n",
       "      <td>10</td>\n",
       "      <td>Chain of Verification Reduces Hallucination in Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer</td>\n",
       "      <td>ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer</td>\n",
       "      <td>2023-06-13</td>\n",
       "      <td>2023</td>\n",
       "      <td>12</td>\n",
       "      <td>ChatGPT vs Human authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification</td>\n",
       "      <td>CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification</td>\n",
       "      <td>2023-03-07</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Complexity-Based Prompting for Multi-Step Reasoning</td>\n",
       "      <td>Complexity-Based Prompting for Multi-Step Reasoning</td>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>2022</td>\n",
       "      <td>116</td>\n",
       "      <td>Complexity Based Prompting for Multi Step Reasoning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Compositional Exemplars for In-context Learning</td>\n",
       "      <td>Compositional Exemplars for In-context Learning</td>\n",
       "      <td>2023-02-11</td>\n",
       "      <td>2023</td>\n",
       "      <td>24</td>\n",
       "      <td>Compositional Exemplars for In context Learning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt</td>\n",
       "      <td>Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt</td>\n",
       "      <td>2023-05-17</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>Compress, Then Prompt: Improving Accuracy Efficiency Trade off of LLM Inference with Transferable Prompt</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers</td>\n",
       "      <td>Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers</td>\n",
       "      <td>2023-09-15</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Constitutional AI: Harmlessness from AI Feedback</td>\n",
       "      <td>Constitutional AI: Harmlessness from AI Feedback</td>\n",
       "      <td>2022-12-15</td>\n",
       "      <td>2022</td>\n",
       "      <td>307</td>\n",
       "      <td>Constitutional AI: Harmlessness from AI Feedback</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Context-faithful Prompting for Large Language Models</td>\n",
       "      <td>Context-faithful Prompting for Large Language Models</td>\n",
       "      <td>2023-03-20</td>\n",
       "      <td>2023</td>\n",
       "      <td>21</td>\n",
       "      <td>Context faithful Prompting for Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>CooK: Empowering General-Purpose Language Models with Modular and Collaborative Knowledge</td>\n",
       "      <td>SHARED OPEN VOCABULARIES AND SEMANTIC MEDIA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>CooK: Empowering General Purpose Language Models with Modular and Collaborative Knowledge</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Decomposed Prompting: A Modular Approach for Solving Complex Tasks</td>\n",
       "      <td>Decomposed Prompting: A Modular Approach for Solving Complex Tasks</td>\n",
       "      <td>2022-10-05</td>\n",
       "      <td>2022</td>\n",
       "      <td>104</td>\n",
       "      <td>Decomposed Prompting: A Modular Approach for Solving Complex Tasks</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP</td>\n",
       "      <td>Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP</td>\n",
       "      <td>2022-12-28</td>\n",
       "      <td>2022</td>\n",
       "      <td>57</td>\n",
       "      <td>Demonstrate Search Predict: Composing retrieval and language models for knowledge intensive NLP</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Design Guidelines for Prompt Engineering Text-to-Image Generative Models</td>\n",
       "      <td>Design Guidelines for Prompt Engineering Text-to-Image Generative Models</td>\n",
       "      <td>2021-09-14</td>\n",
       "      <td>2021</td>\n",
       "      <td>142</td>\n",
       "      <td>Design Guidelines for Prompt Engineering Text to Image Generative Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Discovering Language Model Behaviors with Model-Written Evaluations</td>\n",
       "      <td>Discovering Language Model Behaviors with Model-Written Evaluations</td>\n",
       "      <td>2022-12-19</td>\n",
       "      <td>2022</td>\n",
       "      <td>87</td>\n",
       "      <td>Discovering Language Model Behaviors with Model Written Evaluations</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>DocPrompting: Generating Code by Retrieving the Docs</td>\n",
       "      <td>DocPrompting: Generating Code by Retrieving the Docs</td>\n",
       "      <td>2022-07-13</td>\n",
       "      <td>2022</td>\n",
       "      <td>27</td>\n",
       "      <td>DocPrompting: Generating Code by Retrieving the Docs</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning</td>\n",
       "      <td>Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning</td>\n",
       "      <td>2022-09-29</td>\n",
       "      <td>2022</td>\n",
       "      <td>51</td>\n",
       "      <td>Dynamic Prompt Learning via Policy Gradient for Semi structured Mathematical Reasoning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Dynamic Prompting: A Unified Framework for Prompt Tuning</td>\n",
       "      <td>Dynamic Prompting: A Unified Framework for Prompt Tuning</td>\n",
       "      <td>2023-03-06</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>Dynamic Prompting: A Unified Framework for Prompt Tuning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Effectiveness of Data Augmentation for Prefix Tuning with Limited Data</td>\n",
       "      <td>Effectiveness of Data Augmentation for Prefix Tuning with Limited Data</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>Effectiveness of Data Augmentation for Prefix Tuning with Limited Data</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Efficient Prompting via Dynamic In-Context Learning</td>\n",
       "      <td>Efficient Prompting via Dynamic In-Context Learning</td>\n",
       "      <td>2023-05-18</td>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>Efficient Prompting via Dynamic In Context Learning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference</td>\n",
       "      <td>Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference</td>\n",
       "      <td>2022-11-21</td>\n",
       "      <td>2022</td>\n",
       "      <td>16</td>\n",
       "      <td>Enhancing Self Consistency and Performance of Pre Trained Language Models through Natural Language Inference</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Evaluating Large Language Models Trained on Code</td>\n",
       "      <td>Evaluating Large Language Models Trained on Code</td>\n",
       "      <td>2021-07-07</td>\n",
       "      <td>2021</td>\n",
       "      <td>1617</td>\n",
       "      <td>Evaluating Large Language Models Trained on Code</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Evaluating the Robustness of Discrete Prompts</td>\n",
       "      <td>Evaluating the Robustness of Discrete Prompts</td>\n",
       "      <td>2023-02-11</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>Evaluating the Robustness of Discrete Prompts</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples</td>\n",
       "      <td>Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples</td>\n",
       "      <td>2022-09-05</td>\n",
       "      <td>2022</td>\n",
       "      <td>12</td>\n",
       "      <td>Evaluating the Susceptibility of Pre Trained Language Models via Handcrafted Adversarial Examples</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>EvoPrompting: Language Models for Code-Level Neural Architecture Search</td>\n",
       "      <td>EvoPrompting: Language Models for Code-Level Neural Architecture Search</td>\n",
       "      <td>2023-02-28</td>\n",
       "      <td>2023</td>\n",
       "      <td>12</td>\n",
       "      <td>EvoPrompting: Language Models for Code Level Neural Architecture Search</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Exploring Lottery Prompts for Pre-trained Language Models</td>\n",
       "      <td>Exploring Lottery Prompts for Pre-trained Language Models</td>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>Exploring Lottery Prompts for Pre trained Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Fairness-guided Few-shot Prompting for Large Language Models</td>\n",
       "      <td>Fairness-guided Few-shot Prompting for Large Language Models</td>\n",
       "      <td>2023-03-23</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>Fairness guided Few shot Prompting for Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity</td>\n",
       "      <td>Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity</td>\n",
       "      <td>2021-04-18</td>\n",
       "      <td>2021</td>\n",
       "      <td>408</td>\n",
       "      <td>Fantastically Ordered Prompts and Where to Find Them: Overcoming Few Shot Prompt Order Sensitivity</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency</td>\n",
       "      <td>Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency</td>\n",
       "      <td>2023-05-18</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>Flatness Aware Prompt Selection Improves Accuracy and Sample Efficiency</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Flesch or Fumble? Evaluating Readability Standard Alignment of Instruction-Tuned Language Models</td>\n",
       "      <td>Flesch or Fumble? Evaluating Readability Standard Alignment of Instruction-Tuned Language Models</td>\n",
       "      <td>2023-09-11</td>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>Flesch or Fumble? Evaluating Readability Standard Alignment of Instruction Tuned Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Focused Prefix Tuning for Controllable Text Generation</td>\n",
       "      <td>Focused Prefix Tuning for Controllable Text Generation</td>\n",
       "      <td>2023-06-01</td>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>Focused Prefix Tuning for Controllable Text Generation</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting</td>\n",
       "      <td>From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting</td>\n",
       "      <td>2023-09-08</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>From Sparse to Dense: GPT 4 Summarization with Chain of Density Prompting</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>GPT-4 Technical Report</td>\n",
       "      <td>GPT-4 Technical Report</td>\n",
       "      <td>2023-03-15</td>\n",
       "      <td>2023</td>\n",
       "      <td>1529</td>\n",
       "      <td>GPT 4 Technical Report</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Generated Knowledge Prompting for Commonsense Reasoning</td>\n",
       "      <td>Generated Knowledge Prompting for Commonsense Reasoning</td>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>2021</td>\n",
       "      <td>114</td>\n",
       "      <td>Generated Knowledge Prompting for Commonsense Reasoning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Global Prompt Cell: A Portable Control Module for Effective Prompt</td>\n",
       "      <td>Global Prompt Cell: A Portable Control Module for Effective Prompt</td>\n",
       "      <td>2023-04-12</td>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>Global Prompt Cell: A Portable Control Module for Effective Prompt</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Graph of Thoughts: Solving Elaborate Problems with Large Language Models</td>\n",
       "      <td>Graph of Thoughts: Solving Elaborate Problems with Large Language Models</td>\n",
       "      <td>2023-08-18</td>\n",
       "      <td>2023</td>\n",
       "      <td>35</td>\n",
       "      <td>Graph of Thoughts: Solving Elaborate Problems with Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks</td>\n",
       "      <td>GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks</td>\n",
       "      <td>2023-02-16</td>\n",
       "      <td>2023</td>\n",
       "      <td>16</td>\n",
       "      <td>GraphPrompt: Unifying Pre Training and Downstream Tasks for Graph Neural Networks</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Guiding Large Language Models via Directional Stimulus Prompting</td>\n",
       "      <td>Guiding Large Language Models via Directional Stimulus Prompting</td>\n",
       "      <td>2023-02-22</td>\n",
       "      <td>2023</td>\n",
       "      <td>10</td>\n",
       "      <td>Guiding Large Language Models via Directional Stimulus Prompting</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery</td>\n",
       "      <td>Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery</td>\n",
       "      <td>2023-02-07</td>\n",
       "      <td>2023</td>\n",
       "      <td>38</td>\n",
       "      <td>Hard Prompts Made Easy: Gradient Based Discrete Optimization for Prompt Tuning and Discovery</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Hierarchical Prompting Assists Large Language Model on Web Navigation</td>\n",
       "      <td>Hierarchical Prompting Assists Large Language Model on Web Navigation</td>\n",
       "      <td>2023-05-23</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>Hierarchical Prompting Assists Large Language Model on Web Navigation</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>How Can We Know What Language Models Know?</td>\n",
       "      <td>How Can We Know What Language Models Know?</td>\n",
       "      <td>2019-11-28</td>\n",
       "      <td>2019</td>\n",
       "      <td>763</td>\n",
       "      <td>How Can We Know What Language Models Know?</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>How Does In-Context Learning Help Prompt Tuning?</td>\n",
       "      <td>How Does In-Context Learning Help Prompt Tuning?</td>\n",
       "      <td>2023-02-22</td>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>How Does In Context Learning Help Prompt Tuning?</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks</td>\n",
       "      <td>How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>2023</td>\n",
       "      <td>24</td>\n",
       "      <td>How Robust is GPT 3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Ignore Previous Prompt: Attack Techniques For Language Models</td>\n",
       "      <td>Ignore Previous Prompt: Attack Techniques For Language Models</td>\n",
       "      <td>2022-11-17</td>\n",
       "      <td>2022</td>\n",
       "      <td>68</td>\n",
       "      <td>Ignore Previous Prompt: Attack Techniques For Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>In-Context Instruction Learning</td>\n",
       "      <td>In-Context Instruction Learning</td>\n",
       "      <td>2023-02-28</td>\n",
       "      <td>2023</td>\n",
       "      <td>19</td>\n",
       "      <td>In Context Instruction Learning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>In-Context Learning of Large Language Models Explained as Kernel Regression</td>\n",
       "      <td>In-Context Learning of Large Language Models Explained as Kernel Regression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>In Context Learning of Large Language Models Explained as Kernel Regression</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Interactive Natural Language Processing</td>\n",
       "      <td>Interactive Natural Language Processing</td>\n",
       "      <td>2023-05-22</td>\n",
       "      <td>2023</td>\n",
       "      <td>14</td>\n",
       "      <td>Interactive Natural Language Processing</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Is GPT-4 a Good Data Analyst?</td>\n",
       "      <td>Is GPT-4 a Good Data Analyst?</td>\n",
       "      <td>2023-05-24</td>\n",
       "      <td>2023</td>\n",
       "      <td>17</td>\n",
       "      <td>Is GPT 4 a Good Data Analyst?</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning</td>\n",
       "      <td>Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning</td>\n",
       "      <td>2023-03-18</td>\n",
       "      <td>2023</td>\n",
       "      <td>16</td>\n",
       "      <td>Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Language Is Not All You Need: Aligning Perception with Language Models</td>\n",
       "      <td>Language Is Not All You Need: Aligning Perception with Language Models</td>\n",
       "      <td>2023-02-27</td>\n",
       "      <td>2023</td>\n",
       "      <td>154</td>\n",
       "      <td>Language Is Not All You Need: Aligning Perception with Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought</td>\n",
       "      <td>Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought</td>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>2022</td>\n",
       "      <td>58</td>\n",
       "      <td>Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain of Thought</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Language Models are Few-Shot Learners</td>\n",
       "      <td>Language Models are Few-Shot Learners</td>\n",
       "      <td>2020-05-28</td>\n",
       "      <td>2020</td>\n",
       "      <td>16440</td>\n",
       "      <td>Language Models are Few Shot Learners</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Large Language Model Guided Tree-of-Thought</td>\n",
       "      <td>Large Language Model Guided Tree-of-Thought</td>\n",
       "      <td>2023-05-15</td>\n",
       "      <td>2023</td>\n",
       "      <td>26</td>\n",
       "      <td>Large Language Model Guided Tree of Thought</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Large Language Models Are Human-Level Prompt Engineers</td>\n",
       "      <td>Large Language Models Are Human-Level Prompt Engineers</td>\n",
       "      <td>2022-11-03</td>\n",
       "      <td>2022</td>\n",
       "      <td>194</td>\n",
       "      <td>Large Language Models Are Human Level Prompt Engineers</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Large Language Models Can Be Easily Distracted by Irrelevant Context</td>\n",
       "      <td>Large Language Models Can Be Easily Distracted by Irrelevant Context</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>2023</td>\n",
       "      <td>70</td>\n",
       "      <td>Large Language Models Can Be Easily Distracted by Irrelevant Context</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Large Language Models are Zero-Shot Reasoners</td>\n",
       "      <td>Large Language Models are Zero-Shot Reasoners</td>\n",
       "      <td>2022-05-24</td>\n",
       "      <td>2022</td>\n",
       "      <td>882</td>\n",
       "      <td>Large Language Models are Zero Shot Reasoners</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Large Language Models are reasoners with Self-Verification</td>\n",
       "      <td>Large Language Models are reasoners with Self-Verification</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022</td>\n",
       "      <td>31</td>\n",
       "      <td>Large Language Models are reasoners with Self Verification</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Larger language models do in-context learning differently</td>\n",
       "      <td>Larger language models do in-context learning differently</td>\n",
       "      <td>2023-03-07</td>\n",
       "      <td>2023</td>\n",
       "      <td>87</td>\n",
       "      <td>Larger language models do in context learning differently</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Learning from Task Descriptions</td>\n",
       "      <td>Learning from Task Descriptions</td>\n",
       "      <td>2020-11-01</td>\n",
       "      <td>2020</td>\n",
       "      <td>64</td>\n",
       "      <td>Learning from Task Descriptions</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Learning to Generate Task-Specific Adapters from Task Description</td>\n",
       "      <td>Learning to Generate Task-Specific Adapters from Task Description</td>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>2021</td>\n",
       "      <td>18</td>\n",
       "      <td>Learning to Generate Task Specific Adapters from Task Description</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Learning to Transfer Prompts for Text Generation</td>\n",
       "      <td>Learning to Transfer Prompts for Text Generation</td>\n",
       "      <td>2022-05-03</td>\n",
       "      <td>2022</td>\n",
       "      <td>18</td>\n",
       "      <td>Learning to Transfer Prompts for Text Generation</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Least-to-most promptingenables complex reasoning in large language models</td>\n",
       "      <td>Least-to-Most Prompting Enables Complex Reasoning in Large Language Models</td>\n",
       "      <td>2022-05-21</td>\n",
       "      <td>2022</td>\n",
       "      <td>369</td>\n",
       "      <td>Least to most promptingenables complex reasoning in large language models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses</td>\n",
       "      <td>Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses</td>\n",
       "      <td>2023-05-30</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning with LLMs</td>\n",
       "      <td>Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning with LLMs</td>\n",
       "      <td>2023-05-19</td>\n",
       "      <td>2023</td>\n",
       "      <td>6</td>\n",
       "      <td>Let's Sample Step by Step: Adaptive Consistency for Efficient Reasoning with LLMs</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Let's Verify Step by Step</td>\n",
       "      <td>Let's Verify Step by Step</td>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>2023</td>\n",
       "      <td>64</td>\n",
       "      <td>Let's Verify Step by Step</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning</td>\n",
       "      <td>MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning</td>\n",
       "      <td>2022-05-01</td>\n",
       "      <td>2022</td>\n",
       "      <td>31</td>\n",
       "      <td>MRKL Systems: A modular, neuro symbolic architecture that combines large language models, external knowledge sources and discrete reasoning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods</td>\n",
       "      <td>Machine-Generated Text: A Comprehensive Survey of Threat Models and Detection Methods</td>\n",
       "      <td>2022-10-13</td>\n",
       "      <td>2022</td>\n",
       "      <td>28</td>\n",
       "      <td>Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations</td>\n",
       "      <td>Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations</td>\n",
       "      <td>2022-05-24</td>\n",
       "      <td>2022</td>\n",
       "      <td>86</td>\n",
       "      <td>Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Making Pre-trained Language Models Better Few-shot Learners</td>\n",
       "      <td>Making Pre-trained Language Models Better Few-shot Learners</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>2021</td>\n",
       "      <td>1082</td>\n",
       "      <td>Making Pre trained Language Models Better Few shot Learners</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Meta-in-context learning in large language models</td>\n",
       "      <td>Meta-in-context learning in large language models</td>\n",
       "      <td>2023-05-22</td>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>Meta in context learning in large language models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Mixture of Soft Prompts for Controllable Data Generation</td>\n",
       "      <td>Mixture of Soft Prompts for Controllable Data Generation</td>\n",
       "      <td>2023-03-02</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>Mixture of Soft Prompts for Controllable Data Generation</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Model-tuning Via Prompts Makes NLP Models Adversarially Robust</td>\n",
       "      <td>Model-tuning Via Prompts Makes NLP Models Adversarially Robust</td>\n",
       "      <td>2023-03-13</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>Model tuning Via Prompts Makes NLP Models Adversarially Robust</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models</td>\n",
       "      <td>More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023</td>\n",
       "      <td>37</td>\n",
       "      <td>More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application Integrated Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of Thought Prompting</td>\n",
       "      <td>MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of Thought Prompting</td>\n",
       "      <td>2023-05-26</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>MultiTool CoT: GPT 3 Can Use Multiple External Tools with Chain of Thought Prompting</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Multimodal Chain-of-Thought Reasoning in Language Models</td>\n",
       "      <td>Multimodal Chain-of-Thought Reasoning in Language Models</td>\n",
       "      <td>2023-02-02</td>\n",
       "      <td>2023</td>\n",
       "      <td>91</td>\n",
       "      <td>Multimodal Chain of Thought Reasoning in Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning</td>\n",
       "      <td>Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning</td>\n",
       "      <td>2023-03-06</td>\n",
       "      <td>2023</td>\n",
       "      <td>21</td>\n",
       "      <td>Multitask Prompt Tuning Enables Parameter Efficient Transfer Learning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Multitask Prompted Training Enables Zero-Shot Task Generalization</td>\n",
       "      <td>Multitask Prompted Training Enables Zero-Shot Task Generalization</td>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>2021</td>\n",
       "      <td>924</td>\n",
       "      <td>Multitask Prompted Training Enables Zero Shot Task Generalization</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning</td>\n",
       "      <td>On Second Thought, Lets Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning</td>\n",
       "      <td>2022-12-15</td>\n",
       "      <td>2022</td>\n",
       "      <td>23</td>\n",
       "      <td>On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero Shot Reasoning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>On the Advance of Making Language Models Better Reasoners</td>\n",
       "      <td>On the Advance of Making Language Models Better Reasoners</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022</td>\n",
       "      <td>106</td>\n",
       "      <td>On the Advance of Making Language Models Better Reasoners</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>OpenICL: An Open-Source Framework for In-context Learning</td>\n",
       "      <td>OpenICL: An Open-Source Framework for In-context Learning</td>\n",
       "      <td>2023-03-06</td>\n",
       "      <td>2023</td>\n",
       "      <td>12</td>\n",
       "      <td>OpenICL: An Open Source Framework for In context Learning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>PAL: Program-aided Language Models</td>\n",
       "      <td>PAL: Program-aided Language Models</td>\n",
       "      <td>2022-11-18</td>\n",
       "      <td>2022</td>\n",
       "      <td>198</td>\n",
       "      <td>PAL: Program aided Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents</td>\n",
       "      <td>PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents</td>\n",
       "      <td>2023-05-23</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>PPT: Pre-trained Prompt Tuning for Few-shot Learning</td>\n",
       "      <td>PPT: Pre-trained Prompt Tuning for Few-shot Learning</td>\n",
       "      <td>2021-09-09</td>\n",
       "      <td>2021</td>\n",
       "      <td>234</td>\n",
       "      <td>PPT: Pre trained Prompt Tuning for Few shot Learning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models</td>\n",
       "      <td>Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models</td>\n",
       "      <td>2023-05-06</td>\n",
       "      <td>2023</td>\n",
       "      <td>46</td>\n",
       "      <td>Plan and Solve Prompting: Improving Zero Shot Chain of Thought Reasoning by Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Post Hoc Explanations of Language Models Can Improve Language Models</td>\n",
       "      <td>Post Hoc Explanations of Language Models Can Improve Language Models</td>\n",
       "      <td>2023-05-19</td>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>Post Hoc Explanations of Language Models Can Improve Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>Pre-Training to Learn in Context</td>\n",
       "      <td>Pre-Training to Learn in Context</td>\n",
       "      <td>2023-05-16</td>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>Pre Training to Learn in Context</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Prefix-Tuning: Optimizing Continuous Prompts for Generation</td>\n",
       "      <td>Prefix-Tuning: Optimizing Continuous Prompts for Generation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021</td>\n",
       "      <td>1682</td>\n",
       "      <td>Prefix Tuning: Optimizing Continuous Prompts for Generation</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Progressive Prompts: Continual Learning for Language Models</td>\n",
       "      <td>Progressive Prompts: Continual Learning for Language Models</td>\n",
       "      <td>2023-01-29</td>\n",
       "      <td>2023</td>\n",
       "      <td>20</td>\n",
       "      <td>Progressive Prompts: Continual Learning for Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Prompt Engineering for Large Language Models</td>\n",
       "      <td>Prompt Engineering for Large Language Models</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>Prompt Engineering for Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm</td>\n",
       "      <td>Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm</td>\n",
       "      <td>2021-02-15</td>\n",
       "      <td>2021</td>\n",
       "      <td>317</td>\n",
       "      <td>Prompt Programming for Large Language Models: Beyond the Few Shot Paradigm</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners</td>\n",
       "      <td>Prompt, Generate, Then Cache: Cascade of Foundation Models Makes Strong Few-Shot Learners</td>\n",
       "      <td>2023-03-03</td>\n",
       "      <td>2023</td>\n",
       "      <td>30</td>\n",
       "      <td>Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few shot Learners</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>PromptChainer: Chaining Large Language Model Prompts through Visual Programming</td>\n",
       "      <td>PromptChainer: Chaining Large Language Model Prompts through Visual Programming</td>\n",
       "      <td>2022-03-13</td>\n",
       "      <td>2022</td>\n",
       "      <td>78</td>\n",
       "      <td>PromptChainer: Chaining Large Language Model Prompts through Visual Programming</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Promptagator: Few-shot Dense Retrieval From 8 Examples</td>\n",
       "      <td>Promptagator: Few-shot Dense Retrieval From 8 Examples</td>\n",
       "      <td>2022-09-23</td>\n",
       "      <td>2022</td>\n",
       "      <td>66</td>\n",
       "      <td>Promptagator: Few shot Dense Retrieval From 8 Examples</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Prompted Opinion Summarization with GPT-3.5</td>\n",
       "      <td>Prompted Opinion Summarization with GPT-3.5</td>\n",
       "      <td>2022-11-29</td>\n",
       "      <td>2022</td>\n",
       "      <td>8</td>\n",
       "      <td>Prompted Opinion Summarization with GPT 3.5</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Prompting GPT-3 To Be Reliable</td>\n",
       "      <td>Prompting GPT-3 To Be Reliable</td>\n",
       "      <td>2022-10-17</td>\n",
       "      <td>2022</td>\n",
       "      <td>72</td>\n",
       "      <td>Prompting GPT 3 To Be Reliable</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL</td>\n",
       "      <td>Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL</td>\n",
       "      <td>2023-09-13</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>Query Dependent Prompt Evaluation and Optimization with Offline Inverse RL</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>REFINER: Reasoning Feedback on Intermediate Representations</td>\n",
       "      <td>REFINER: Reasoning Feedback on Intermediate Representations</td>\n",
       "      <td>2023-04-04</td>\n",
       "      <td>2023</td>\n",
       "      <td>35</td>\n",
       "      <td>REFINER: Reasoning Feedback on Intermediate Representations</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>Re-Reading Improves Reasoning in Language Models</td>\n",
       "      <td>Re-Reading Improves Reasoning in Language Models</td>\n",
       "      <td>2023-09-12</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>Re Reading Improves Reasoning in Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>ReAct: Synergizing Reasoning and Acting in Language Models</td>\n",
       "      <td>ReAct: Synergizing Reasoning and Acting in Language Models</td>\n",
       "      <td>2022-10-06</td>\n",
       "      <td>2022</td>\n",
       "      <td>332</td>\n",
       "      <td>ReAct: Synergizing Reasoning and Acting in Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Reasoning with Language Model is Planning with World Model</td>\n",
       "      <td>Reasoning with Language Model is Planning with World Model</td>\n",
       "      <td>2023-05-24</td>\n",
       "      <td>2023</td>\n",
       "      <td>34</td>\n",
       "      <td>Reasoning with Language Model is Planning with World Model</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>Recitation-Augmented Language Models</td>\n",
       "      <td>Recitation-Augmented Language Models</td>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>2022</td>\n",
       "      <td>37</td>\n",
       "      <td>Recitation Augmented Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Reflexion: an autonomous agent with dynamic memory and self-reflection</td>\n",
       "      <td>Reflexion: an autonomous agent with dynamic memory and self-reflection</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023</td>\n",
       "      <td>105</td>\n",
       "      <td>Reflexion: an autonomous agent with dynamic memory and self reflection</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Reframing Instructional Prompts to GPTk's Language</td>\n",
       "      <td>Reframing Instructional Prompts to GPTks Language</td>\n",
       "      <td>2021-09-16</td>\n",
       "      <td>2021</td>\n",
       "      <td>121</td>\n",
       "      <td>Reframing Instructional Prompts to GPTk's Language</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling</td>\n",
       "      <td>Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling</td>\n",
       "      <td>2023-05-17</td>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>Reprompting: Automated Chain of Thought Prompt Inference Through Gibbs Sampling</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?</td>\n",
       "      <td>Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?</td>\n",
       "      <td>2022-02-25</td>\n",
       "      <td>2022</td>\n",
       "      <td>431</td>\n",
       "      <td>Rethinking the Role of Demonstrations: What Makes In Context Learning Work?</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</td>\n",
       "      <td>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</td>\n",
       "      <td>2020-05-22</td>\n",
       "      <td>2020</td>\n",
       "      <td>956</td>\n",
       "      <td>Retrieval Augmented Generation for Knowledge Intensive NLP Tasks</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Revisiting Automated Prompting: Are We Actually Doing Better?</td>\n",
       "      <td>Revisiting Automated Prompting: Are We Actually Doing Better?</td>\n",
       "      <td>2023-04-07</td>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>Revisiting Automated Prompting: Are We Actually Doing Better?</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Satisfiability-Aided Language Models Using Declarative Prompting</td>\n",
       "      <td>Satisfiability-Aided Language Models Using Declarative Prompting</td>\n",
       "      <td>2023-05-16</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>Satisfiability Aided Language Models Using Declarative Prompting</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Scalable Prompt Generation for Semi-supervised Learning with Language Models</td>\n",
       "      <td>Scalable Prompt Generation for Semi-supervised Learning with Language Models</td>\n",
       "      <td>2023-02-18</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>Scalable Prompt Generation for Semi supervised Learning with Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>Scaling Instruction-Finetuned Language Models</td>\n",
       "      <td>Scaling Instruction-Finetuned Language Models</td>\n",
       "      <td>2022-10-20</td>\n",
       "      <td>2022</td>\n",
       "      <td>823</td>\n",
       "      <td>Scaling Instruction Finetuned Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>Scaling Laws for Neural Language Models</td>\n",
       "      <td>Scaling Laws for Neural Language Models</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>2020</td>\n",
       "      <td>1530</td>\n",
       "      <td>Scaling Laws for Neural Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Self-Consistency Improves Chain of Thought Reasoning in Language Models</td>\n",
       "      <td>Self-Consistency Improves Chain of Thought Reasoning in Language Models</td>\n",
       "      <td>2022-03-21</td>\n",
       "      <td>2022</td>\n",
       "      <td>659</td>\n",
       "      <td>Self Consistency Improves Chain of Thought Reasoning in Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Self-Critique Prompting with Large Language Models for Inductive Instructions</td>\n",
       "      <td>Self-Critique Prompting with Large Language Models for Inductive Instructions</td>\n",
       "      <td>2023-05-23</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>Self Critique Prompting with Large Language Models for Inductive Instructions</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Self-Refine: Iterative Refinement with Self-Feedback</td>\n",
       "      <td>Self-Refine: Iterative Refinement with Self-Feedback</td>\n",
       "      <td>2023-03-30</td>\n",
       "      <td>2023</td>\n",
       "      <td>201</td>\n",
       "      <td>Self Refine: Iterative Refinement with Self Feedback</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Show Your Work: Scratchpads for Intermediate Computation with Language Models</td>\n",
       "      <td>Show Your Work: Scratchpads for Intermediate Computation with Language Models</td>\n",
       "      <td>2021-11-30</td>\n",
       "      <td>2021</td>\n",
       "      <td>315</td>\n",
       "      <td>Show Your Work: Scratchpads for Intermediate Computation with Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding</td>\n",
       "      <td>Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding</td>\n",
       "      <td>2023-07-28</td>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>Skeleton of Thought: Large Language Models Can Do Parallel Decoding</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer</td>\n",
       "      <td>Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer</td>\n",
       "      <td>2023-03-03</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>Successive Prompting for Decomposing Complex Questions</td>\n",
       "      <td>Successive Prompting for Decomposing Complex Questions</td>\n",
       "      <td>2022-12-08</td>\n",
       "      <td>2022</td>\n",
       "      <td>40</td>\n",
       "      <td>Successive Prompting for Decomposing Complex Questions</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains</td>\n",
       "      <td>SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains</td>\n",
       "      <td>2023-02-14</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>SwitchPrompt: Learning Domain Specific Gated Soft Prompts for Classification in Low Resource Domains</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models</td>\n",
       "      <td>Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models</td>\n",
       "      <td>2023-02-01</td>\n",
       "      <td>2023</td>\n",
       "      <td>26</td>\n",
       "      <td>Synthetic Prompting: Generating Chain of Thought Demonstrations for Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks</td>\n",
       "      <td>TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks</td>\n",
       "      <td>2023-05-19</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Teaching Algorithmic Reasoning via In-context Learning</td>\n",
       "      <td>Teaching Algorithmic Reasoning via In-context Learning</td>\n",
       "      <td>2022-11-15</td>\n",
       "      <td>2022</td>\n",
       "      <td>52</td>\n",
       "      <td>Teaching Algorithmic Reasoning via In context Learning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>The Capacity for Moral Self-Correction in Large Language Models</td>\n",
       "      <td>The Capacity for Moral Self-Correction in Large Language Models</td>\n",
       "      <td>2023-02-15</td>\n",
       "      <td>2023</td>\n",
       "      <td>49</td>\n",
       "      <td>The Capacity for Moral Self Correction in Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>The Power of Scale for Parameter-Efficient Prompt Tuning</td>\n",
       "      <td>The Power of Scale for Parameter-Efficient Prompt Tuning</td>\n",
       "      <td>2021-04-18</td>\n",
       "      <td>2021</td>\n",
       "      <td>1530</td>\n",
       "      <td>The Power of Scale for Parameter Efficient Prompt Tuning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning</td>\n",
       "      <td>The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning</td>\n",
       "      <td>2022-05-06</td>\n",
       "      <td>2022</td>\n",
       "      <td>47</td>\n",
       "      <td>The Unreliability of Explanations in Few shot Prompting for Textual Reasoning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>The Web Can Be Your Oyster for Improving Large Language Models</td>\n",
       "      <td>The Web Can Be Your Oyster for Improving Large Language Models</td>\n",
       "      <td>2023-05-18</td>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>The Web Can Be Your Oyster for Improving Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Toxicity Detection with Generative Prompt-based Inference</td>\n",
       "      <td>Toxicity Detection with Generative Prompt-based Inference</td>\n",
       "      <td>2022-05-24</td>\n",
       "      <td>2022</td>\n",
       "      <td>11</td>\n",
       "      <td>Toxicity Detection with Generative Prompt based Inference</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Training language models to follow instructions with human feedback</td>\n",
       "      <td>Training language models to follow instructions with human feedback</td>\n",
       "      <td>2022-03-04</td>\n",
       "      <td>2022</td>\n",
       "      <td>3009</td>\n",
       "      <td>Training language models to follow instructions with human feedback</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Tree of Thoughts: Deliberate Problem Solving with Large Language Models</td>\n",
       "      <td>Tree of Thoughts: Deliberate Problem Solving with Large Language Models</td>\n",
       "      <td>2023-05-17</td>\n",
       "      <td>2023</td>\n",
       "      <td>227</td>\n",
       "      <td>Tree of Thoughts: Deliberate Problem Solving with Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding</td>\n",
       "      <td>TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding</td>\n",
       "      <td>2023-05-19</td>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation</td>\n",
       "      <td>UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation</td>\n",
       "      <td>2023-03-15</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>UPRISE: Universal Prompt Retrieval for Improving Zero Shot Evaluation</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Universality and Limitations of Prompt Tuning</td>\n",
       "      <td>Universality and Limitations of Prompt Tuning</td>\n",
       "      <td>2023-05-30</td>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>Universality and Limitations of Prompt Tuning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Visual-Language Prompt Tuning with Knowledge-guided Context Optimization</td>\n",
       "      <td>Visual-Language Prompt Tuning with Knowledge-Guided Context Optimization</td>\n",
       "      <td>2023-03-23</td>\n",
       "      <td>2023</td>\n",
       "      <td>11</td>\n",
       "      <td>Visual Language Prompt Tuning with Knowledge guided Context Optimization</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>What In-Context Learning \"Learns\" In-Context: Disentangling Task Recognition and Task Learning</td>\n",
       "      <td>What In-Context Learning \"Learns\" In-Context: Disentangling Task Recognition and Task Learning</td>\n",
       "      <td>2023-05-16</td>\n",
       "      <td>2023</td>\n",
       "      <td>19</td>\n",
       "      <td>What In Context Learning \"Learns\" In Context: Disentangling Task Recognition and Task Learning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>Why think step-by-step? Reasoning emerges from the locality of experience</td>\n",
       "      <td>Why think step-by-step? Reasoning emerges from the locality of experience</td>\n",
       "      <td>2023-04-07</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>Why think step by step? Reasoning emerges from the locality of experience</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>ZeroPrompt: Streaming Acoustic Encoders are Zero-Shot Masked LMs</td>\n",
       "      <td>ZeroPrompt: Streaming Acoustic Encoders are Zero-Shot Masked LMs</td>\n",
       "      <td>2023-05-18</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>ZeroPrompt: Streaming Acoustic Encoders are Zero Shot Masked LMs</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>kNN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference</td>\n",
       "      <td>kNN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference</td>\n",
       "      <td>2023-03-24</td>\n",
       "      <td>2023</td>\n",
       "      <td>16</td>\n",
       "      <td>kNN Prompting: Beyond Context Learning with Calibration Free Nearest Neighbor Inference</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>-la-carte Prompt Tuning (APT): Combining Distinct Data Via Composable Prompting</td>\n",
       "      <td>-la-carte Prompt Tuning (APT): Combining Distinct Data Via Composable Prompting</td>\n",
       "      <td>2023-02-15</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td> la carte Prompt Tuning (APT): Combining Distinct Data Via Composable Prompting</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Structured Prompting: Scaling In-Context Learning to 1, 000 Examples</td>\n",
       "      <td>Structured Prompting: Scaling In-Context Learning to 1, 000 Examples</td>\n",
       "      <td>2022-12-13</td>\n",
       "      <td>2022</td>\n",
       "      <td>20</td>\n",
       "      <td>Structured Prompting: Scaling In Context Learning to 1, 000 Examples</td>\n",
       "      <td>2023-10-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                     paper title  \\\n",
       "0                                                                            A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT   \n",
       "1                                                                                    A Taxonomy of Prompt Modifiers for Text-To-Image Generation   \n",
       "2                                          AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts   \n",
       "3                                                                     ART: Automatic multi-step reasoning and tool-use for large language models   \n",
       "4                                                                               Active Prompting with Chain-of-Thought for Large Language Models   \n",
       "5                                                   An automatically discovered chain-of-thought prompt generalizes to novel models and datasets   \n",
       "6                                                                               Ask Me Anything: A simple strategy for prompting language models   \n",
       "7                                                                              Atlas: Few-shot Learning with Retrieval Augmented Language Models   \n",
       "8                                                      AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts   \n",
       "9                                                                                  Automatic Chain of Thought Prompting in Large Language Models   \n",
       "10                                                           Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data   \n",
       "11                                                                                                            BERTese: Learning to Speak to BERT   \n",
       "12                                                                                            Batch Prompting: Efficient Inference with LLM APIs   \n",
       "13                                                                                       Better Zero-Shot Reasoning with Self-Adaptive Prompting   \n",
       "14                                                                                            Boosted Prompt Ensembles for Large Language Models   \n",
       "15                                            Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints   \n",
       "16                                                      CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale Language Model Society   \n",
       "17                                                                       Calibrate Before Use: Improving Few-Shot Performance of Language Models   \n",
       "18                                                                Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT   \n",
       "19                                                                                         Can We Edit Factual Knowledge by In-Context Learning?   \n",
       "20                                                                                       Chain of Hindsight Aligns Language Models with Feedback   \n",
       "21                                                                         Chain of Thought Prompting Elicits Reasoning in Large Language Models   \n",
       "22                                                                           Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models   \n",
       "23                                                                          Chain-of-Verification Reduces Hallucination in Large Language Models   \n",
       "24                                     ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer   \n",
       "25                                                           CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification   \n",
       "26                                                                                           Complexity-Based Prompting for Multi-Step Reasoning   \n",
       "27                                                                                               Compositional Exemplars for In-context Learning   \n",
       "28                                      Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt   \n",
       "29                                               Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers   \n",
       "30                                                                                              Constitutional AI: Harmlessness from AI Feedback   \n",
       "31                                                                                          Context-faithful Prompting for Large Language Models   \n",
       "32                                                     CooK: Empowering General-Purpose Language Models with Modular and Collaborative Knowledge   \n",
       "33                                                                            Decomposed Prompting: A Modular Approach for Solving Complex Tasks   \n",
       "34                                               Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP   \n",
       "35                                                                      Design Guidelines for Prompt Engineering Text-to-Image Generative Models   \n",
       "36                                                                           Discovering Language Model Behaviors with Model-Written Evaluations   \n",
       "37                                                                                          DocPrompting: Generating Code by Retrieving the Docs   \n",
       "38                                                        Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning   \n",
       "39                                                                                      Dynamic Prompting: A Unified Framework for Prompt Tuning   \n",
       "40                                                                        Effectiveness of Data Augmentation for Prefix Tuning with Limited Data   \n",
       "41                                                                                           Efficient Prompting via Dynamic In-Context Learning   \n",
       "42                                  Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference   \n",
       "43                                                                                              Evaluating Large Language Models Trained on Code   \n",
       "44                                                                                                 Evaluating the Robustness of Discrete Prompts   \n",
       "45                                             Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples   \n",
       "46                                                                       EvoPrompting: Language Models for Code-Level Neural Architecture Search   \n",
       "47                                                                                     Exploring Lottery Prompts for Pre-trained Language Models   \n",
       "48                                                                                  Fairness-guided Few-shot Prompting for Large Language Models   \n",
       "49                                            Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity   \n",
       "50                                                                       Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency   \n",
       "51                                              Flesch or Fumble? Evaluating Readability Standard Alignment of Instruction-Tuned Language Models   \n",
       "52                                                                                        Focused Prefix Tuning for Controllable Text Generation   \n",
       "53                                                                     From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting   \n",
       "54                                                                                                                        GPT-4 Technical Report   \n",
       "55                                                                                       Generated Knowledge Prompting for Commonsense Reasoning   \n",
       "56                                                                            Global Prompt Cell: A Portable Control Module for Effective Prompt   \n",
       "57                                                                      Graph of Thoughts: Solving Elaborate Problems with Large Language Models   \n",
       "58                                                             GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks   \n",
       "59                                                                              Guiding Large Language Models via Directional Stimulus Prompting   \n",
       "60                                                  Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery   \n",
       "61                                                                         Hierarchical Prompting Assists Large Language Model on Web Navigation   \n",
       "62                                                                                                    How Can We Know What Language Models Know?   \n",
       "63                                                                                              How Does In-Context Learning Help Prompt Tuning?   \n",
       "64                                                  How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks   \n",
       "65                                                                                 Ignore Previous Prompt: Attack Techniques For Language Models   \n",
       "66                                                                                                               In-Context Instruction Learning   \n",
       "67                                                                   In-Context Learning of Large Language Models Explained as Kernel Regression   \n",
       "68                                                                                                       Interactive Natural Language Processing   \n",
       "69                                                                                                                 Is GPT-4 a Good Data Analyst?   \n",
       "70                                                          Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning   \n",
       "71                                                                        Language Is Not All You Need: Aligning Perception with Language Models   \n",
       "72                                                        Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought   \n",
       "73                                                                                                         Language Models are Few-Shot Learners   \n",
       "74                                                                                                   Large Language Model Guided Tree-of-Thought   \n",
       "75                                                                                        Large Language Models Are Human-Level Prompt Engineers   \n",
       "76                                                                          Large Language Models Can Be Easily Distracted by Irrelevant Context   \n",
       "77                                                                                                 Large Language Models are Zero-Shot Reasoners   \n",
       "78                                                                                    Large Language Models are reasoners with Self-Verification   \n",
       "79                                                                                     Larger language models do in-context learning differently   \n",
       "80                                                                                                               Learning from Task Descriptions   \n",
       "81                                                                             Learning to Generate Task-Specific Adapters from Task Description   \n",
       "82                                                                                              Learning to Transfer Prompts for Text Generation   \n",
       "83                                                                    Least-to-most promptingenables complex reasoning in large language models   \n",
       "84                                                           Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses   \n",
       "85                                                             Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning with LLMs   \n",
       "86                                                                                                                     Let's Verify Step by Step   \n",
       "87   MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning   \n",
       "88                                                         Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods   \n",
       "89                                                                Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations   \n",
       "90                                                                                   Making Pre-trained Language Models Better Few-shot Learners   \n",
       "91                                                                                             Meta-in-context learning in large language models   \n",
       "92                                                                                      Mixture of Soft Prompts for Controllable Data Generation   \n",
       "93                                                                                Model-tuning Via Prompts Makes NLP Models Adversarially Robust   \n",
       "94        More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models   \n",
       "95                                                          MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of Thought Prompting   \n",
       "96                                                                                      Multimodal Chain-of-Thought Reasoning in Language Models   \n",
       "97                                                                         Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning   \n",
       "98                                                                             Multitask Prompted Training Enables Zero-Shot Task Generalization   \n",
       "99                                                     On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning   \n",
       "100                                                                                    On the Advance of Making Language Models Better Reasoners   \n",
       "101                                                                                    OpenICL: An Open-Source Framework for In-context Learning   \n",
       "102                                                                                                           PAL: Program-aided Language Models   \n",
       "103                                                       PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents   \n",
       "104                                                                                         PPT: Pre-trained Prompt Tuning for Few-shot Learning   \n",
       "105                                            Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models   \n",
       "106                                                                         Post Hoc Explanations of Language Models Can Improve Language Models   \n",
       "107                                                                                                             Pre-Training to Learn in Context   \n",
       "108                                                                                  Prefix-Tuning: Optimizing Continuous Prompts for Generation   \n",
       "109                                                                                  Progressive Prompts: Continual Learning for Language Models   \n",
       "110                                                                                                 Prompt Engineering for Large Language Models   \n",
       "111                                                                   Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm   \n",
       "112                                                    Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners   \n",
       "113                                                              PromptChainer: Chaining Large Language Model Prompts through Visual Programming   \n",
       "114                                                                                       Promptagator: Few-shot Dense Retrieval From 8 Examples   \n",
       "115                                                                                                  Prompted Opinion Summarization with GPT-3.5   \n",
       "116                                                                                                               Prompting GPT-3 To Be Reliable   \n",
       "117                                                                   Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL   \n",
       "118                                                                                  REFINER: Reasoning Feedback on Intermediate Representations   \n",
       "119                                                                                             Re-Reading Improves Reasoning in Language Models   \n",
       "120                                                                                   ReAct: Synergizing Reasoning and Acting in Language Models   \n",
       "121                                                                                   Reasoning with Language Model is Planning with World Model   \n",
       "122                                                                                                         Recitation-Augmented Language Models   \n",
       "123                                                                       Reflexion: an autonomous agent with dynamic memory and self-reflection   \n",
       "124                                                                                           Reframing Instructional Prompts to GPTk's Language   \n",
       "125                                                              Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling   \n",
       "126                                                                  Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?   \n",
       "127                                                                             Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks   \n",
       "128                                                                                Revisiting Automated Prompting: Are We Actually Doing Better?   \n",
       "129                                                                             Satisfiability-Aided Language Models Using Declarative Prompting   \n",
       "130                                                                 Scalable Prompt Generation for Semi-supervised Learning with Language Models   \n",
       "131                                                                                                Scaling Instruction-Finetuned Language Models   \n",
       "132                                                                                                      Scaling Laws for Neural Language Models   \n",
       "133                                                                      Self-Consistency Improves Chain of Thought Reasoning in Language Models   \n",
       "134                                                                Self-Critique Prompting with Large Language Models for Inductive Instructions   \n",
       "135                                                                                         Self-Refine: Iterative Refinement with Self-Feedback   \n",
       "136                                                                Show Your Work: Scratchpads for Intermediate Computation with Language Models   \n",
       "137                                                                          Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding   \n",
       "138                                                                         Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer   \n",
       "139                                                                                       Successive Prompting for Decomposing Complex Questions   \n",
       "140                                         SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains   \n",
       "141                                                    Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models   \n",
       "142                                                                      TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks   \n",
       "143                                                                                       Teaching Algorithmic Reasoning via In-context Learning   \n",
       "144                                                                              The Capacity for Moral Self-Correction in Large Language Models   \n",
       "145                                                                                     The Power of Scale for Parameter-Efficient Prompt Tuning   \n",
       "146                                                                The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning   \n",
       "147                                                                               The Web Can Be Your Oyster for Improving Large Language Models   \n",
       "148                                                                                    Toxicity Detection with Generative Prompt-based Inference   \n",
       "149                                                                          Training language models to follow instructions with human feedback   \n",
       "150                                                                      Tree of Thoughts: Deliberate Problem Solving with Large Language Models   \n",
       "151                                                                TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding   \n",
       "152                                                                        UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation   \n",
       "153                                                                                                Universality and Limitations of Prompt Tuning   \n",
       "154                                                                     Visual-Language Prompt Tuning with Knowledge-guided Context Optimization   \n",
       "155                                               What In-Context Learning \"Learns\" In-Context: Disentangling Task Recognition and Task Learning   \n",
       "156                                                                    Why think step-by-step? Reasoning emerges from the locality of experience   \n",
       "157                                                                             ZeroPrompt: Streaming Acoustic Encoders are Zero-Shot Masked LMs   \n",
       "158                                                      kNN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference   \n",
       "159                                                             -la-carte Prompt Tuning (APT): Combining Distinct Data Via Composable Prompting   \n",
       "160                                                                         Structured Prompting: Scaling In-Context Learning to 1, 000 Examples   \n",
       "\n",
       "                                                                                                                          semantic scholar title  \\\n",
       "0                                                                            A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT   \n",
       "1                                                                                    A Taxonomy of Prompt Modifiers for Text-To-Image Generation   \n",
       "2                                          AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts   \n",
       "3                                                                     ART: Automatic multi-step reasoning and tool-use for large language models   \n",
       "4                                                                               Active Prompting with Chain-of-Thought for Large Language Models   \n",
       "5                                                   An automatically discovered chain-of-thought prompt generalizes to novel models and datasets   \n",
       "6                                                                               Ask Me Anything: A simple strategy for prompting language models   \n",
       "7                                                                                     Few-shot Learning with Retrieval Augmented Language Models   \n",
       "8                                                                 Eliciting Knowledge from Language Models Using Automatically Generated Prompts   \n",
       "9                                                                                  Automatic Chain of Thought Prompting in Large Language Models   \n",
       "10                                                           Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data   \n",
       "11                                                                                                            BERTese: Learning to Speak to BERT   \n",
       "12                                                                           Batch Prompting: Efficient Inference with Large Language Model APIs   \n",
       "13                                                                                       Better Zero-Shot Reasoning with Self-Adaptive Prompting   \n",
       "14                                                                                            Boosted Prompt Ensembles for Large Language Models   \n",
       "15                                            Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints   \n",
       "16                                                      CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale Language Model Society   \n",
       "17                                                                       Calibrate Before Use: Improving Few-Shot Performance of Language Models   \n",
       "18                                                                Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT   \n",
       "19                                                                                         Can We Edit Factual Knowledge by In-Context Learning?   \n",
       "20                                                                                       Chain of Hindsight Aligns Language Models with Feedback   \n",
       "21                                                                         Chain of Thought Prompting Elicits Reasoning in Large Language Models   \n",
       "22                                                                           Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models   \n",
       "23                                                                          Chain-of-Verification Reduces Hallucination in Large Language Models   \n",
       "24                                     ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer   \n",
       "25                                                           CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification   \n",
       "26                                                                                           Complexity-Based Prompting for Multi-Step Reasoning   \n",
       "27                                                                                               Compositional Exemplars for In-context Learning   \n",
       "28                                      Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt   \n",
       "29                                               Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers   \n",
       "30                                                                                              Constitutional AI: Harmlessness from AI Feedback   \n",
       "31                                                                                          Context-faithful Prompting for Large Language Models   \n",
       "32                                                                                                   SHARED OPEN VOCABULARIES AND SEMANTIC MEDIA   \n",
       "33                                                                            Decomposed Prompting: A Modular Approach for Solving Complex Tasks   \n",
       "34                                               Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP   \n",
       "35                                                                      Design Guidelines for Prompt Engineering Text-to-Image Generative Models   \n",
       "36                                                                           Discovering Language Model Behaviors with Model-Written Evaluations   \n",
       "37                                                                                          DocPrompting: Generating Code by Retrieving the Docs   \n",
       "38                                                        Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning   \n",
       "39                                                                                      Dynamic Prompting: A Unified Framework for Prompt Tuning   \n",
       "40                                                                        Effectiveness of Data Augmentation for Prefix Tuning with Limited Data   \n",
       "41                                                                                           Efficient Prompting via Dynamic In-Context Learning   \n",
       "42                                  Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference   \n",
       "43                                                                                              Evaluating Large Language Models Trained on Code   \n",
       "44                                                                                                 Evaluating the Robustness of Discrete Prompts   \n",
       "45                                             Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples   \n",
       "46                                                                       EvoPrompting: Language Models for Code-Level Neural Architecture Search   \n",
       "47                                                                                     Exploring Lottery Prompts for Pre-trained Language Models   \n",
       "48                                                                                  Fairness-guided Few-shot Prompting for Large Language Models   \n",
       "49                                            Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity   \n",
       "50                                                                       Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency   \n",
       "51                                              Flesch or Fumble? Evaluating Readability Standard Alignment of Instruction-Tuned Language Models   \n",
       "52                                                                                        Focused Prefix Tuning for Controllable Text Generation   \n",
       "53                                                                     From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting   \n",
       "54                                                                                                                        GPT-4 Technical Report   \n",
       "55                                                                                       Generated Knowledge Prompting for Commonsense Reasoning   \n",
       "56                                                                            Global Prompt Cell: A Portable Control Module for Effective Prompt   \n",
       "57                                                                      Graph of Thoughts: Solving Elaborate Problems with Large Language Models   \n",
       "58                                                             GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks   \n",
       "59                                                                              Guiding Large Language Models via Directional Stimulus Prompting   \n",
       "60                                                  Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery   \n",
       "61                                                                         Hierarchical Prompting Assists Large Language Model on Web Navigation   \n",
       "62                                                                                                    How Can We Know What Language Models Know?   \n",
       "63                                                                                              How Does In-Context Learning Help Prompt Tuning?   \n",
       "64                                                  How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks   \n",
       "65                                                                                 Ignore Previous Prompt: Attack Techniques For Language Models   \n",
       "66                                                                                                               In-Context Instruction Learning   \n",
       "67                                                                   In-Context Learning of Large Language Models Explained as Kernel Regression   \n",
       "68                                                                                                       Interactive Natural Language Processing   \n",
       "69                                                                                                                 Is GPT-4 a Good Data Analyst?   \n",
       "70                                                          Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning   \n",
       "71                                                                        Language Is Not All You Need: Aligning Perception with Language Models   \n",
       "72                                                        Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought   \n",
       "73                                                                                                         Language Models are Few-Shot Learners   \n",
       "74                                                                                                   Large Language Model Guided Tree-of-Thought   \n",
       "75                                                                                        Large Language Models Are Human-Level Prompt Engineers   \n",
       "76                                                                          Large Language Models Can Be Easily Distracted by Irrelevant Context   \n",
       "77                                                                                                 Large Language Models are Zero-Shot Reasoners   \n",
       "78                                                                                    Large Language Models are reasoners with Self-Verification   \n",
       "79                                                                                     Larger language models do in-context learning differently   \n",
       "80                                                                                                               Learning from Task Descriptions   \n",
       "81                                                                             Learning to Generate Task-Specific Adapters from Task Description   \n",
       "82                                                                                              Learning to Transfer Prompts for Text Generation   \n",
       "83                                                                    Least-to-Most Prompting Enables Complex Reasoning in Large Language Models   \n",
       "84                                                           Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses   \n",
       "85                                                             Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning with LLMs   \n",
       "86                                                                                                                     Let's Verify Step by Step   \n",
       "87   MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning   \n",
       "88                                                         Machine-Generated Text: A Comprehensive Survey of Threat Models and Detection Methods   \n",
       "89                                                                Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations   \n",
       "90                                                                                   Making Pre-trained Language Models Better Few-shot Learners   \n",
       "91                                                                                             Meta-in-context learning in large language models   \n",
       "92                                                                                      Mixture of Soft Prompts for Controllable Data Generation   \n",
       "93                                                                                Model-tuning Via Prompts Makes NLP Models Adversarially Robust   \n",
       "94        More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models   \n",
       "95                                                          MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of Thought Prompting   \n",
       "96                                                                                      Multimodal Chain-of-Thought Reasoning in Language Models   \n",
       "97                                                                         Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning   \n",
       "98                                                                             Multitask Prompted Training Enables Zero-Shot Task Generalization   \n",
       "99                                                     On Second Thought, Lets Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning   \n",
       "100                                                                                    On the Advance of Making Language Models Better Reasoners   \n",
       "101                                                                                    OpenICL: An Open-Source Framework for In-context Learning   \n",
       "102                                                                                                           PAL: Program-aided Language Models   \n",
       "103                                                       PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents   \n",
       "104                                                                                         PPT: Pre-trained Prompt Tuning for Few-shot Learning   \n",
       "105                                            Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models   \n",
       "106                                                                         Post Hoc Explanations of Language Models Can Improve Language Models   \n",
       "107                                                                                                             Pre-Training to Learn in Context   \n",
       "108                                                                                  Prefix-Tuning: Optimizing Continuous Prompts for Generation   \n",
       "109                                                                                  Progressive Prompts: Continual Learning for Language Models   \n",
       "110                                                                                                 Prompt Engineering for Large Language Models   \n",
       "111                                                                   Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm   \n",
       "112                                                    Prompt, Generate, Then Cache: Cascade of Foundation Models Makes Strong Few-Shot Learners   \n",
       "113                                                              PromptChainer: Chaining Large Language Model Prompts through Visual Programming   \n",
       "114                                                                                       Promptagator: Few-shot Dense Retrieval From 8 Examples   \n",
       "115                                                                                                  Prompted Opinion Summarization with GPT-3.5   \n",
       "116                                                                                                               Prompting GPT-3 To Be Reliable   \n",
       "117                                                                   Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL   \n",
       "118                                                                                  REFINER: Reasoning Feedback on Intermediate Representations   \n",
       "119                                                                                             Re-Reading Improves Reasoning in Language Models   \n",
       "120                                                                                   ReAct: Synergizing Reasoning and Acting in Language Models   \n",
       "121                                                                                   Reasoning with Language Model is Planning with World Model   \n",
       "122                                                                                                         Recitation-Augmented Language Models   \n",
       "123                                                                       Reflexion: an autonomous agent with dynamic memory and self-reflection   \n",
       "124                                                                                           Reframing Instructional Prompts to GPTks Language   \n",
       "125                                                              Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling   \n",
       "126                                                                  Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?   \n",
       "127                                                                             Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks   \n",
       "128                                                                                Revisiting Automated Prompting: Are We Actually Doing Better?   \n",
       "129                                                                             Satisfiability-Aided Language Models Using Declarative Prompting   \n",
       "130                                                                 Scalable Prompt Generation for Semi-supervised Learning with Language Models   \n",
       "131                                                                                                Scaling Instruction-Finetuned Language Models   \n",
       "132                                                                                                      Scaling Laws for Neural Language Models   \n",
       "133                                                                      Self-Consistency Improves Chain of Thought Reasoning in Language Models   \n",
       "134                                                                Self-Critique Prompting with Large Language Models for Inductive Instructions   \n",
       "135                                                                                         Self-Refine: Iterative Refinement with Self-Feedback   \n",
       "136                                                                Show Your Work: Scratchpads for Intermediate Computation with Language Models   \n",
       "137                                                                          Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding   \n",
       "138                                                                         Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer   \n",
       "139                                                                                       Successive Prompting for Decomposing Complex Questions   \n",
       "140                                         SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains   \n",
       "141                                                    Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models   \n",
       "142                                                                      TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks   \n",
       "143                                                                                       Teaching Algorithmic Reasoning via In-context Learning   \n",
       "144                                                                              The Capacity for Moral Self-Correction in Large Language Models   \n",
       "145                                                                                     The Power of Scale for Parameter-Efficient Prompt Tuning   \n",
       "146                                                                The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning   \n",
       "147                                                                               The Web Can Be Your Oyster for Improving Large Language Models   \n",
       "148                                                                                    Toxicity Detection with Generative Prompt-based Inference   \n",
       "149                                                                          Training language models to follow instructions with human feedback   \n",
       "150                                                                      Tree of Thoughts: Deliberate Problem Solving with Large Language Models   \n",
       "151                                                                TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding   \n",
       "152                                                                        UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation   \n",
       "153                                                                                                Universality and Limitations of Prompt Tuning   \n",
       "154                                                                     Visual-Language Prompt Tuning with Knowledge-Guided Context Optimization   \n",
       "155                                               What In-Context Learning \"Learns\" In-Context: Disentangling Task Recognition and Task Learning   \n",
       "156                                                                    Why think step-by-step? Reasoning emerges from the locality of experience   \n",
       "157                                                                             ZeroPrompt: Streaming Acoustic Encoders are Zero-Shot Masked LMs   \n",
       "158                                                      kNN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference   \n",
       "159                                                             -la-carte Prompt Tuning (APT): Combining Distinct Data Via Composable Prompting   \n",
       "160                                                                         Structured Prompting: Scaling In-Context Learning to 1, 000 Examples   \n",
       "\n",
       "    ss_publication_date  ss_year  citation_count  \\\n",
       "0            2023-02-21     2023             163   \n",
       "1            2022-04-20     2022              43   \n",
       "2            2021-10-04     2021             146   \n",
       "3            2023-03-16     2023              39   \n",
       "4            2023-02-23     2023              34   \n",
       "5            2023-05-04     2023               6   \n",
       "6            2022-10-05     2022              65   \n",
       "7            2022-08-05     2022             202   \n",
       "8            2020-10-29     2020             217   \n",
       "9            2022-10-07     2022             201   \n",
       "10           2023-02-24     2023              21   \n",
       "11           2021-03-09     2021              74   \n",
       "12           2023-01-19     2023              10   \n",
       "13           2023-05-23     2023               8   \n",
       "14           2023-04-12     2023              12   \n",
       "15           2023-02-17     2023               6   \n",
       "16           2023-03-31     2023              64   \n",
       "17           2021-02-19     2021             616   \n",
       "18           2023-02-19     2023              73   \n",
       "19           2023-05-22     2023              11   \n",
       "20           2023-02-06     2023              37   \n",
       "21           2022-01-28     2022            2105   \n",
       "22                  NaN     2023               2   \n",
       "23           2023-09-20     2023              10   \n",
       "24           2023-06-13     2023              12   \n",
       "25           2023-03-07     2023               3   \n",
       "26           2022-10-03     2022             116   \n",
       "27           2023-02-11     2023              24   \n",
       "28           2023-05-17     2023               3   \n",
       "29           2023-09-15     2023               8   \n",
       "30           2022-12-15     2022             307   \n",
       "31           2023-03-20     2023              21   \n",
       "32                  NaN     2012               1   \n",
       "33           2022-10-05     2022             104   \n",
       "34           2022-12-28     2022              57   \n",
       "35           2021-09-14     2021             142   \n",
       "36           2022-12-19     2022              87   \n",
       "37           2022-07-13     2022              27   \n",
       "38           2022-09-29     2022              51   \n",
       "39           2023-03-06     2023               4   \n",
       "40                  NaN     2023               0   \n",
       "41           2023-05-18     2023               5   \n",
       "42           2022-11-21     2022              16   \n",
       "43           2021-07-07     2021            1617   \n",
       "44           2023-02-11     2023               4   \n",
       "45           2022-09-05     2022              12   \n",
       "46           2023-02-28     2023              12   \n",
       "47           2023-05-31     2023               1   \n",
       "48           2023-03-23     2023               2   \n",
       "49           2021-04-18     2021             408   \n",
       "50           2023-05-18     2023               1   \n",
       "51           2023-09-11     2023               0   \n",
       "52           2023-06-01     2023               0   \n",
       "53           2023-09-08     2023               2   \n",
       "54           2023-03-15     2023            1529   \n",
       "55           2021-10-15     2021             114   \n",
       "56           2023-04-12     2023               0   \n",
       "57           2023-08-18     2023              35   \n",
       "58           2023-02-16     2023              16   \n",
       "59           2023-02-22     2023              10   \n",
       "60           2023-02-07     2023              38   \n",
       "61           2023-05-23     2023               3   \n",
       "62           2019-11-28     2019             763   \n",
       "63           2023-02-22     2023               5   \n",
       "64           2023-03-01     2023              24   \n",
       "65           2022-11-17     2022              68   \n",
       "66           2023-02-28     2023              19   \n",
       "67                  NaN     2023               4   \n",
       "68           2023-05-22     2023              14   \n",
       "69           2023-05-24     2023              17   \n",
       "70           2023-03-18     2023              16   \n",
       "71           2023-02-27     2023             154   \n",
       "72           2022-10-03     2022              58   \n",
       "73           2020-05-28     2020           16440   \n",
       "74           2023-05-15     2023              26   \n",
       "75           2022-11-03     2022             194   \n",
       "76           2023-01-31     2023              70   \n",
       "77           2022-05-24     2022             882   \n",
       "78                  NaN     2022              31   \n",
       "79           2023-03-07     2023              87   \n",
       "80           2020-11-01     2020              64   \n",
       "81           2021-01-02     2021              18   \n",
       "82           2022-05-03     2022              18   \n",
       "83           2022-05-21     2022             369   \n",
       "84           2023-05-30     2023               1   \n",
       "85           2023-05-19     2023               6   \n",
       "86           2023-05-31     2023              64   \n",
       "87           2022-05-01     2022              31   \n",
       "88           2022-10-13     2022              28   \n",
       "89           2022-05-24     2022              86   \n",
       "90           2021-01-01     2021            1082   \n",
       "91           2023-05-22     2023               5   \n",
       "92           2023-03-02     2023               3   \n",
       "93           2023-03-13     2023               3   \n",
       "94                  NaN     2023              37   \n",
       "95           2023-05-26     2023               1   \n",
       "96           2023-02-02     2023              91   \n",
       "97           2023-03-06     2023              21   \n",
       "98           2021-10-15     2021             924   \n",
       "99           2022-12-15     2022              23   \n",
       "100                 NaN     2022             106   \n",
       "101          2023-03-06     2023              12   \n",
       "102          2022-11-18     2022             198   \n",
       "103          2023-05-23     2023               4   \n",
       "104          2021-09-09     2021             234   \n",
       "105          2023-05-06     2023              46   \n",
       "106          2023-05-19     2023               5   \n",
       "107          2023-05-16     2023               5   \n",
       "108                 NaN     2021            1682   \n",
       "109          2023-01-29     2023              20   \n",
       "110                 NaN     2023               3   \n",
       "111          2021-02-15     2021             317   \n",
       "112          2023-03-03     2023              30   \n",
       "113          2022-03-13     2022              78   \n",
       "114          2022-09-23     2022              66   \n",
       "115          2022-11-29     2022               8   \n",
       "116          2022-10-17     2022              72   \n",
       "117          2023-09-13     2023               1   \n",
       "118          2023-04-04     2023              35   \n",
       "119          2023-09-12     2023               2   \n",
       "120          2022-10-06     2022             332   \n",
       "121          2023-05-24     2023              34   \n",
       "122          2022-10-04     2022              37   \n",
       "123                 NaN     2023             105   \n",
       "124          2021-09-16     2021             121   \n",
       "125          2023-05-17     2023               5   \n",
       "126          2022-02-25     2022             431   \n",
       "127          2020-05-22     2020             956   \n",
       "128          2023-04-07     2023               0   \n",
       "129          2023-05-16     2023               3   \n",
       "130          2023-02-18     2023               1   \n",
       "131          2022-10-20     2022             823   \n",
       "132          2020-01-23     2020            1530   \n",
       "133          2022-03-21     2022             659   \n",
       "134          2023-05-23     2023               3   \n",
       "135          2023-03-30     2023             201   \n",
       "136          2021-11-30     2021             315   \n",
       "137          2023-07-28     2023               5   \n",
       "138          2023-03-03     2023               4   \n",
       "139          2022-12-08     2022              40   \n",
       "140          2023-02-14     2023               2   \n",
       "141          2023-02-01     2023              26   \n",
       "142          2023-05-19     2023               4   \n",
       "143          2022-11-15     2022              52   \n",
       "144          2023-02-15     2023              49   \n",
       "145          2021-04-18     2021            1530   \n",
       "146          2022-05-06     2022              47   \n",
       "147          2023-05-18     2023               0   \n",
       "148          2022-05-24     2022              11   \n",
       "149          2022-03-04     2022            3009   \n",
       "150          2023-05-17     2023             227   \n",
       "151          2023-05-19     2023               0   \n",
       "152          2023-03-15     2023               8   \n",
       "153          2023-05-30     2023               0   \n",
       "154          2023-03-23     2023              11   \n",
       "155          2023-05-16     2023              19   \n",
       "156          2023-04-07     2023               8   \n",
       "157          2023-05-18     2023               1   \n",
       "158          2023-03-24     2023              16   \n",
       "159          2023-02-15     2023               4   \n",
       "160          2022-12-13     2022              20   \n",
       "\n",
       "                                                                                                                                           query  \\\n",
       "0                                                                            A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT   \n",
       "1                                                                                    A Taxonomy of Prompt Modifiers for Text To Image Generation   \n",
       "2                                          AI Chains: Transparent and Controllable Human AI Interaction by Chaining Large Language Model Prompts   \n",
       "3                                                                     ART: Automatic multi step reasoning and tool use for large language models   \n",
       "4                                                                               Active Prompting with Chain of Thought for Large Language Models   \n",
       "5                                                   An automatically discovered chain of thought prompt generalizes to novel models and datasets   \n",
       "6                                                                               Ask Me Anything: A simple strategy for prompting language models   \n",
       "7                                                                              Atlas: Few shot Learning with Retrieval Augmented Language Models   \n",
       "8                                                      AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts   \n",
       "9                                                                                  Automatic Chain of Thought Prompting in Large Language Models   \n",
       "10                                                           Automatic Prompt Augmentation and Selection with Chain of Thought from Labeled Data   \n",
       "11                                                                                                            BERTese: Learning to Speak to BERT   \n",
       "12                                                                                            Batch Prompting: Efficient Inference with LLM APIs   \n",
       "13                                                                                       Better Zero Shot Reasoning with Self Adaptive Prompting   \n",
       "14                                                                                            Boosted Prompt Ensembles for Large Language Models   \n",
       "15                                            Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints   \n",
       "16                                                      CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale Language Model Society   \n",
       "17                                                                       Calibrate Before Use: Improving Few Shot Performance of Language Models   \n",
       "18                                                                Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine tuned BERT   \n",
       "19                                                                                         Can We Edit Factual Knowledge by In Context Learning?   \n",
       "20                                                                                       Chain of Hindsight Aligns Language Models with Feedback   \n",
       "21                                                                         Chain of Thought Prompting Elicits Reasoning in Large Language Models   \n",
       "22                                                                           Chain of Symbol Prompting Elicits Planning in Large Langauge Models   \n",
       "23                                                                          Chain of Verification Reduces Hallucination in Large Language Models   \n",
       "24                                     ChatGPT vs Human authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer   \n",
       "25                                                           CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification   \n",
       "26                                                                                           Complexity Based Prompting for Multi Step Reasoning   \n",
       "27                                                                                               Compositional Exemplars for In context Learning   \n",
       "28                                      Compress, Then Prompt: Improving Accuracy Efficiency Trade off of LLM Inference with Transferable Prompt   \n",
       "29                                               Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers   \n",
       "30                                                                                              Constitutional AI: Harmlessness from AI Feedback   \n",
       "31                                                                                          Context faithful Prompting for Large Language Models   \n",
       "32                                                     CooK: Empowering General Purpose Language Models with Modular and Collaborative Knowledge   \n",
       "33                                                                            Decomposed Prompting: A Modular Approach for Solving Complex Tasks   \n",
       "34                                               Demonstrate Search Predict: Composing retrieval and language models for knowledge intensive NLP   \n",
       "35                                                                      Design Guidelines for Prompt Engineering Text to Image Generative Models   \n",
       "36                                                                           Discovering Language Model Behaviors with Model Written Evaluations   \n",
       "37                                                                                          DocPrompting: Generating Code by Retrieving the Docs   \n",
       "38                                                        Dynamic Prompt Learning via Policy Gradient for Semi structured Mathematical Reasoning   \n",
       "39                                                                                      Dynamic Prompting: A Unified Framework for Prompt Tuning   \n",
       "40                                                                        Effectiveness of Data Augmentation for Prefix Tuning with Limited Data   \n",
       "41                                                                                           Efficient Prompting via Dynamic In Context Learning   \n",
       "42                                  Enhancing Self Consistency and Performance of Pre Trained Language Models through Natural Language Inference   \n",
       "43                                                                                              Evaluating Large Language Models Trained on Code   \n",
       "44                                                                                                 Evaluating the Robustness of Discrete Prompts   \n",
       "45                                             Evaluating the Susceptibility of Pre Trained Language Models via Handcrafted Adversarial Examples   \n",
       "46                                                                       EvoPrompting: Language Models for Code Level Neural Architecture Search   \n",
       "47                                                                                     Exploring Lottery Prompts for Pre trained Language Models   \n",
       "48                                                                                  Fairness guided Few shot Prompting for Large Language Models   \n",
       "49                                            Fantastically Ordered Prompts and Where to Find Them: Overcoming Few Shot Prompt Order Sensitivity   \n",
       "50                                                                       Flatness Aware Prompt Selection Improves Accuracy and Sample Efficiency   \n",
       "51                                              Flesch or Fumble? Evaluating Readability Standard Alignment of Instruction Tuned Language Models   \n",
       "52                                                                                        Focused Prefix Tuning for Controllable Text Generation   \n",
       "53                                                                     From Sparse to Dense: GPT 4 Summarization with Chain of Density Prompting   \n",
       "54                                                                                                                        GPT 4 Technical Report   \n",
       "55                                                                                       Generated Knowledge Prompting for Commonsense Reasoning   \n",
       "56                                                                            Global Prompt Cell: A Portable Control Module for Effective Prompt   \n",
       "57                                                                      Graph of Thoughts: Solving Elaborate Problems with Large Language Models   \n",
       "58                                                             GraphPrompt: Unifying Pre Training and Downstream Tasks for Graph Neural Networks   \n",
       "59                                                                              Guiding Large Language Models via Directional Stimulus Prompting   \n",
       "60                                                  Hard Prompts Made Easy: Gradient Based Discrete Optimization for Prompt Tuning and Discovery   \n",
       "61                                                                         Hierarchical Prompting Assists Large Language Model on Web Navigation   \n",
       "62                                                                                                    How Can We Know What Language Models Know?   \n",
       "63                                                                                              How Does In Context Learning Help Prompt Tuning?   \n",
       "64                                                  How Robust is GPT 3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks   \n",
       "65                                                                                 Ignore Previous Prompt: Attack Techniques For Language Models   \n",
       "66                                                                                                               In Context Instruction Learning   \n",
       "67                                                                   In Context Learning of Large Language Models Explained as Kernel Regression   \n",
       "68                                                                                                       Interactive Natural Language Processing   \n",
       "69                                                                                                                 Is GPT 4 a Good Data Analyst?   \n",
       "70                                                          Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning   \n",
       "71                                                                        Language Is Not All You Need: Aligning Perception with Language Models   \n",
       "72                                                        Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain of Thought   \n",
       "73                                                                                                         Language Models are Few Shot Learners   \n",
       "74                                                                                                   Large Language Model Guided Tree of Thought   \n",
       "75                                                                                        Large Language Models Are Human Level Prompt Engineers   \n",
       "76                                                                          Large Language Models Can Be Easily Distracted by Irrelevant Context   \n",
       "77                                                                                                 Large Language Models are Zero Shot Reasoners   \n",
       "78                                                                                    Large Language Models are reasoners with Self Verification   \n",
       "79                                                                                     Larger language models do in context learning differently   \n",
       "80                                                                                                               Learning from Task Descriptions   \n",
       "81                                                                             Learning to Generate Task Specific Adapters from Task Description   \n",
       "82                                                                                              Learning to Transfer Prompts for Text Generation   \n",
       "83                                                                    Least to most promptingenables complex reasoning in large language models   \n",
       "84                                                           Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses   \n",
       "85                                                             Let's Sample Step by Step: Adaptive Consistency for Efficient Reasoning with LLMs   \n",
       "86                                                                                                                     Let's Verify Step by Step   \n",
       "87   MRKL Systems: A modular, neuro symbolic architecture that combines large language models, external knowledge sources and discrete reasoning   \n",
       "88                                                         Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods   \n",
       "89                                                                Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations   \n",
       "90                                                                                   Making Pre trained Language Models Better Few shot Learners   \n",
       "91                                                                                             Meta in context learning in large language models   \n",
       "92                                                                                      Mixture of Soft Prompts for Controllable Data Generation   \n",
       "93                                                                                Model tuning Via Prompts Makes NLP Models Adversarially Robust   \n",
       "94        More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application Integrated Large Language Models   \n",
       "95                                                          MultiTool CoT: GPT 3 Can Use Multiple External Tools with Chain of Thought Prompting   \n",
       "96                                                                                      Multimodal Chain of Thought Reasoning in Language Models   \n",
       "97                                                                         Multitask Prompt Tuning Enables Parameter Efficient Transfer Learning   \n",
       "98                                                                             Multitask Prompted Training Enables Zero Shot Task Generalization   \n",
       "99                                                     On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero Shot Reasoning   \n",
       "100                                                                                    On the Advance of Making Language Models Better Reasoners   \n",
       "101                                                                                    OpenICL: An Open Source Framework for In context Learning   \n",
       "102                                                                                                           PAL: Program aided Language Models   \n",
       "103                                                       PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents   \n",
       "104                                                                                         PPT: Pre trained Prompt Tuning for Few shot Learning   \n",
       "105                                            Plan and Solve Prompting: Improving Zero Shot Chain of Thought Reasoning by Large Language Models   \n",
       "106                                                                         Post Hoc Explanations of Language Models Can Improve Language Models   \n",
       "107                                                                                                             Pre Training to Learn in Context   \n",
       "108                                                                                  Prefix Tuning: Optimizing Continuous Prompts for Generation   \n",
       "109                                                                                  Progressive Prompts: Continual Learning for Language Models   \n",
       "110                                                                                                 Prompt Engineering for Large Language Models   \n",
       "111                                                                   Prompt Programming for Large Language Models: Beyond the Few Shot Paradigm   \n",
       "112                                                    Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few shot Learners   \n",
       "113                                                              PromptChainer: Chaining Large Language Model Prompts through Visual Programming   \n",
       "114                                                                                       Promptagator: Few shot Dense Retrieval From 8 Examples   \n",
       "115                                                                                                  Prompted Opinion Summarization with GPT 3.5   \n",
       "116                                                                                                               Prompting GPT 3 To Be Reliable   \n",
       "117                                                                   Query Dependent Prompt Evaluation and Optimization with Offline Inverse RL   \n",
       "118                                                                                  REFINER: Reasoning Feedback on Intermediate Representations   \n",
       "119                                                                                             Re Reading Improves Reasoning in Language Models   \n",
       "120                                                                                   ReAct: Synergizing Reasoning and Acting in Language Models   \n",
       "121                                                                                   Reasoning with Language Model is Planning with World Model   \n",
       "122                                                                                                         Recitation Augmented Language Models   \n",
       "123                                                                       Reflexion: an autonomous agent with dynamic memory and self reflection   \n",
       "124                                                                                           Reframing Instructional Prompts to GPTk's Language   \n",
       "125                                                              Reprompting: Automated Chain of Thought Prompt Inference Through Gibbs Sampling   \n",
       "126                                                                  Rethinking the Role of Demonstrations: What Makes In Context Learning Work?   \n",
       "127                                                                             Retrieval Augmented Generation for Knowledge Intensive NLP Tasks   \n",
       "128                                                                                Revisiting Automated Prompting: Are We Actually Doing Better?   \n",
       "129                                                                             Satisfiability Aided Language Models Using Declarative Prompting   \n",
       "130                                                                 Scalable Prompt Generation for Semi supervised Learning with Language Models   \n",
       "131                                                                                                Scaling Instruction Finetuned Language Models   \n",
       "132                                                                                                      Scaling Laws for Neural Language Models   \n",
       "133                                                                      Self Consistency Improves Chain of Thought Reasoning in Language Models   \n",
       "134                                                                Self Critique Prompting with Large Language Models for Inductive Instructions   \n",
       "135                                                                                         Self Refine: Iterative Refinement with Self Feedback   \n",
       "136                                                                Show Your Work: Scratchpads for Intermediate Computation with Language Models   \n",
       "137                                                                          Skeleton of Thought: Large Language Models Can Do Parallel Decoding   \n",
       "138                                                                         Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer   \n",
       "139                                                                                       Successive Prompting for Decomposing Complex Questions   \n",
       "140                                         SwitchPrompt: Learning Domain Specific Gated Soft Prompts for Classification in Low Resource Domains   \n",
       "141                                                    Synthetic Prompting: Generating Chain of Thought Demonstrations for Large Language Models   \n",
       "142                                                                      TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks   \n",
       "143                                                                                       Teaching Algorithmic Reasoning via In context Learning   \n",
       "144                                                                              The Capacity for Moral Self Correction in Large Language Models   \n",
       "145                                                                                     The Power of Scale for Parameter Efficient Prompt Tuning   \n",
       "146                                                                The Unreliability of Explanations in Few shot Prompting for Textual Reasoning   \n",
       "147                                                                               The Web Can Be Your Oyster for Improving Large Language Models   \n",
       "148                                                                                    Toxicity Detection with Generative Prompt based Inference   \n",
       "149                                                                          Training language models to follow instructions with human feedback   \n",
       "150                                                                      Tree of Thoughts: Deliberate Problem Solving with Large Language Models   \n",
       "151                                                                TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding   \n",
       "152                                                                        UPRISE: Universal Prompt Retrieval for Improving Zero Shot Evaluation   \n",
       "153                                                                                                Universality and Limitations of Prompt Tuning   \n",
       "154                                                                     Visual Language Prompt Tuning with Knowledge guided Context Optimization   \n",
       "155                                               What In Context Learning \"Learns\" In Context: Disentangling Task Recognition and Task Learning   \n",
       "156                                                                    Why think step by step? Reasoning emerges from the locality of experience   \n",
       "157                                                                             ZeroPrompt: Streaming Acoustic Encoders are Zero Shot Masked LMs   \n",
       "158                                                      kNN Prompting: Beyond Context Learning with Calibration Free Nearest Neighbor Inference   \n",
       "159                                                              la carte Prompt Tuning (APT): Combining Distinct Data Via Composable Prompting   \n",
       "160                                                                         Structured Prompting: Scaling In Context Learning to 1, 000 Examples   \n",
       "\n",
       "             day_queried  \n",
       "0    2023-10-22 00:00:00  \n",
       "1    2023-10-22 00:00:00  \n",
       "2    2023-10-22 00:00:00  \n",
       "3    2023-10-22 00:00:00  \n",
       "4    2023-10-22 00:00:00  \n",
       "5    2023-10-22 00:00:00  \n",
       "6    2023-10-22 00:00:00  \n",
       "7    2023-10-22 00:00:00  \n",
       "8    2023-10-22 00:00:00  \n",
       "9    2023-10-22 00:00:00  \n",
       "10   2023-10-22 00:00:00  \n",
       "11   2023-10-22 00:00:00  \n",
       "12   2023-10-22 00:00:00  \n",
       "13   2023-10-22 00:00:00  \n",
       "14   2023-10-22 00:00:00  \n",
       "15   2023-10-22 00:00:00  \n",
       "16   2023-10-22 00:00:00  \n",
       "17   2023-10-22 00:00:00  \n",
       "18   2023-10-22 00:00:00  \n",
       "19   2023-10-22 00:00:00  \n",
       "20   2023-10-22 00:00:00  \n",
       "21   2023-10-22 00:00:00  \n",
       "22   2023-10-22 00:00:00  \n",
       "23   2023-10-22 00:00:00  \n",
       "24   2023-10-22 00:00:00  \n",
       "25   2023-10-22 00:00:00  \n",
       "26   2023-10-22 00:00:00  \n",
       "27   2023-10-22 00:00:00  \n",
       "28   2023-10-22 00:00:00  \n",
       "29   2023-10-22 00:00:00  \n",
       "30   2023-10-22 00:00:00  \n",
       "31   2023-10-22 00:00:00  \n",
       "32   2023-10-22 00:00:00  \n",
       "33   2023-10-22 00:00:00  \n",
       "34   2023-10-22 00:00:00  \n",
       "35   2023-10-22 00:00:00  \n",
       "36   2023-10-22 00:00:00  \n",
       "37   2023-10-22 00:00:00  \n",
       "38   2023-10-22 00:00:00  \n",
       "39   2023-10-22 00:00:00  \n",
       "40   2023-10-22 00:00:00  \n",
       "41   2023-10-22 00:00:00  \n",
       "42   2023-10-22 00:00:00  \n",
       "43   2023-10-22 00:00:00  \n",
       "44   2023-10-22 00:00:00  \n",
       "45   2023-10-22 00:00:00  \n",
       "46   2023-10-22 00:00:00  \n",
       "47   2023-10-22 00:00:00  \n",
       "48   2023-10-22 00:00:00  \n",
       "49   2023-10-22 00:00:00  \n",
       "50   2023-10-22 00:00:00  \n",
       "51   2023-10-22 00:00:00  \n",
       "52   2023-10-22 00:00:00  \n",
       "53   2023-10-22 00:00:00  \n",
       "54   2023-10-22 00:00:00  \n",
       "55   2023-10-22 00:00:00  \n",
       "56   2023-10-22 00:00:00  \n",
       "57   2023-10-22 00:00:00  \n",
       "58   2023-10-22 00:00:00  \n",
       "59   2023-10-22 00:00:00  \n",
       "60   2023-10-22 00:00:00  \n",
       "61   2023-10-22 00:00:00  \n",
       "62   2023-10-22 00:00:00  \n",
       "63   2023-10-22 00:00:00  \n",
       "64   2023-10-22 00:00:00  \n",
       "65   2023-10-22 00:00:00  \n",
       "66   2023-10-22 00:00:00  \n",
       "67   2023-10-22 00:00:00  \n",
       "68   2023-10-22 00:00:00  \n",
       "69   2023-10-22 00:00:00  \n",
       "70   2023-10-22 00:00:00  \n",
       "71   2023-10-22 00:00:00  \n",
       "72   2023-10-22 00:00:00  \n",
       "73   2023-10-22 00:00:00  \n",
       "74   2023-10-22 00:00:00  \n",
       "75   2023-10-22 00:00:00  \n",
       "76   2023-10-22 00:00:00  \n",
       "77   2023-10-22 00:00:00  \n",
       "78   2023-10-22 00:00:00  \n",
       "79   2023-10-22 00:00:00  \n",
       "80   2023-10-22 00:00:00  \n",
       "81   2023-10-22 00:00:00  \n",
       "82   2023-10-22 00:00:00  \n",
       "83   2023-10-22 00:00:00  \n",
       "84   2023-10-22 00:00:00  \n",
       "85   2023-10-22 00:00:00  \n",
       "86   2023-10-22 00:00:00  \n",
       "87   2023-10-22 00:00:00  \n",
       "88   2023-10-22 00:00:00  \n",
       "89   2023-10-22 00:00:00  \n",
       "90   2023-10-22 00:00:00  \n",
       "91   2023-10-22 00:00:00  \n",
       "92   2023-10-22 00:00:00  \n",
       "93   2023-10-22 00:00:00  \n",
       "94   2023-10-22 00:00:00  \n",
       "95   2023-10-22 00:00:00  \n",
       "96   2023-10-22 00:00:00  \n",
       "97   2023-10-22 00:00:00  \n",
       "98   2023-10-22 00:00:00  \n",
       "99   2023-10-22 00:00:00  \n",
       "100  2023-10-22 00:00:00  \n",
       "101  2023-10-22 00:00:00  \n",
       "102  2023-10-22 00:00:00  \n",
       "103  2023-10-22 00:00:00  \n",
       "104  2023-10-22 00:00:00  \n",
       "105  2023-10-22 00:00:00  \n",
       "106  2023-10-22 00:00:00  \n",
       "107  2023-10-22 00:00:00  \n",
       "108  2023-10-22 00:00:00  \n",
       "109  2023-10-22 00:00:00  \n",
       "110  2023-10-22 00:00:00  \n",
       "111  2023-10-22 00:00:00  \n",
       "112  2023-10-22 00:00:00  \n",
       "113  2023-10-22 00:00:00  \n",
       "114  2023-10-22 00:00:00  \n",
       "115  2023-10-22 00:00:00  \n",
       "116  2023-10-22 00:00:00  \n",
       "117  2023-10-22 00:00:00  \n",
       "118  2023-10-22 00:00:00  \n",
       "119  2023-10-22 00:00:00  \n",
       "120  2023-10-22 00:00:00  \n",
       "121  2023-10-22 00:00:00  \n",
       "122  2023-10-22 00:00:00  \n",
       "123  2023-10-22 00:00:00  \n",
       "124  2023-10-22 00:00:00  \n",
       "125  2023-10-22 00:00:00  \n",
       "126  2023-10-22 00:00:00  \n",
       "127  2023-10-22 00:00:00  \n",
       "128  2023-10-22 00:00:00  \n",
       "129  2023-10-22 00:00:00  \n",
       "130  2023-10-22 00:00:00  \n",
       "131  2023-10-22 00:00:00  \n",
       "132  2023-10-22 00:00:00  \n",
       "133  2023-10-22 00:00:00  \n",
       "134  2023-10-22 00:00:00  \n",
       "135  2023-10-22 00:00:00  \n",
       "136  2023-10-22 00:00:00  \n",
       "137  2023-10-22 00:00:00  \n",
       "138  2023-10-22 00:00:00  \n",
       "139  2023-10-22 00:00:00  \n",
       "140  2023-10-22 00:00:00  \n",
       "141  2023-10-22 00:00:00  \n",
       "142  2023-10-22 00:00:00  \n",
       "143  2023-10-22 00:00:00  \n",
       "144  2023-10-22 00:00:00  \n",
       "145  2023-10-22 00:00:00  \n",
       "146  2023-10-22 00:00:00  \n",
       "147  2023-10-22 00:00:00  \n",
       "148  2023-10-22 00:00:00  \n",
       "149  2023-10-22 00:00:00  \n",
       "150  2023-10-22 00:00:00  \n",
       "151  2023-10-22 00:00:00  \n",
       "152  2023-10-22 00:00:00  \n",
       "153  2023-10-22 00:00:00  \n",
       "154  2023-10-22 00:00:00  \n",
       "155  2023-10-22 00:00:00  \n",
       "156  2023-10-22 00:00:00  \n",
       "157  2023-10-22 00:00:00  \n",
       "158  2023-10-22 00:00:00  \n",
       "159  2023-10-22 00:00:00  \n",
       "160           2023-10-22  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create semantic_scholar_df by concatenating semantic_scholar_df_first_pass and semantic_scholar_df_corrections\n",
    "semantic_scholar_df = pd.concat([semantic_scholar_df_first_pass, semantic_scholar_df_corrections], ignore_index=True)\n",
    "\n",
    "semantic_scholar_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     2023-02-21\n",
      "1     2022-04-20\n",
      "2     2021-10-04\n",
      "3     2023-03-16\n",
      "4     2023-02-23\n",
      "5     2023-05-04\n",
      "6     2022-10-05\n",
      "7     2022-08-05\n",
      "8     2020-10-29\n",
      "9     2022-10-07\n",
      "10    2023-02-24\n",
      "11    2021-03-09\n",
      "12    2023-01-19\n",
      "13    2023-05-23\n",
      "14    2023-04-12\n",
      "15    2023-02-17\n",
      "16    2023-03-31\n",
      "17    2021-02-19\n",
      "18    2023-02-19\n",
      "19    2023-05-22\n",
      "20    2023-02-06\n",
      "21    2022-01-28\n",
      "22           NaT\n",
      "23    2023-09-20\n",
      "24    2023-06-13\n",
      "25    2023-03-07\n",
      "26    2022-10-03\n",
      "27    2023-02-11\n",
      "28    2023-05-17\n",
      "29    2023-09-15\n",
      "30    2022-12-15\n",
      "31    2023-03-20\n",
      "32           NaT\n",
      "33    2022-10-05\n",
      "34    2022-12-28\n",
      "35    2021-09-14\n",
      "36    2022-12-19\n",
      "37    2022-07-13\n",
      "38    2022-09-29\n",
      "39    2023-03-06\n",
      "40           NaT\n",
      "41    2023-05-18\n",
      "42    2022-11-21\n",
      "43    2021-07-07\n",
      "44    2023-02-11\n",
      "45    2022-09-05\n",
      "46    2023-02-28\n",
      "47    2023-05-31\n",
      "48    2023-03-23\n",
      "49    2021-04-18\n",
      "50    2023-05-18\n",
      "51    2023-09-11\n",
      "52    2023-06-01\n",
      "53    2023-09-08\n",
      "54    2023-03-15\n",
      "55    2021-10-15\n",
      "56    2023-04-12\n",
      "57    2023-08-18\n",
      "58    2023-02-16\n",
      "59    2023-02-22\n",
      "60    2023-02-07\n",
      "61    2023-05-23\n",
      "62    2019-11-28\n",
      "63    2023-02-22\n",
      "64    2023-03-01\n",
      "65    2022-11-17\n",
      "66    2023-02-28\n",
      "67           NaT\n",
      "68    2023-05-22\n",
      "69    2023-05-24\n",
      "70    2023-03-18\n",
      "71    2023-02-27\n",
      "72    2022-10-03\n",
      "73    2020-05-28\n",
      "74    2023-05-15\n",
      "75    2022-11-03\n",
      "76    2023-01-31\n",
      "77    2022-05-24\n",
      "78           NaT\n",
      "79    2023-03-07\n",
      "80    2020-11-01\n",
      "81    2021-01-02\n",
      "82    2022-05-03\n",
      "83    2022-05-21\n",
      "84    2023-05-30\n",
      "85    2023-05-19\n",
      "86    2023-05-31\n",
      "87    2022-05-01\n",
      "88    2022-10-13\n",
      "89    2022-05-24\n",
      "90    2021-01-01\n",
      "91    2023-05-22\n",
      "92    2023-03-02\n",
      "93    2023-03-13\n",
      "94           NaT\n",
      "95    2023-05-26\n",
      "96    2023-02-02\n",
      "97    2023-03-06\n",
      "98    2021-10-15\n",
      "99    2022-12-15\n",
      "100          NaT\n",
      "101   2023-03-06\n",
      "102   2022-11-18\n",
      "103   2023-05-23\n",
      "104   2021-09-09\n",
      "105   2023-05-06\n",
      "106   2023-05-19\n",
      "107   2023-05-16\n",
      "108          NaT\n",
      "109   2023-01-29\n",
      "110          NaT\n",
      "111   2021-02-15\n",
      "112   2023-03-03\n",
      "113   2022-03-13\n",
      "114   2022-09-23\n",
      "115   2022-11-29\n",
      "116   2022-10-17\n",
      "117   2023-09-13\n",
      "118   2023-04-04\n",
      "119   2023-09-12\n",
      "120   2022-10-06\n",
      "121   2023-05-24\n",
      "122   2022-10-04\n",
      "123          NaT\n",
      "124   2021-09-16\n",
      "125   2023-05-17\n",
      "126   2022-02-25\n",
      "127   2020-05-22\n",
      "128   2023-04-07\n",
      "129   2023-05-16\n",
      "130   2023-02-18\n",
      "131   2022-10-20\n",
      "132   2020-01-23\n",
      "133   2022-03-21\n",
      "134   2023-05-23\n",
      "135   2023-03-30\n",
      "136   2021-11-30\n",
      "137   2023-07-28\n",
      "138   2023-03-03\n",
      "139   2022-12-08\n",
      "140   2023-02-14\n",
      "141   2023-02-01\n",
      "142   2023-05-19\n",
      "143   2022-11-15\n",
      "144   2023-02-15\n",
      "145   2021-04-18\n",
      "146   2022-05-06\n",
      "147   2023-05-18\n",
      "148   2022-05-24\n",
      "149   2022-03-04\n",
      "150   2023-05-17\n",
      "151   2023-05-19\n",
      "152   2023-03-15\n",
      "153   2023-05-30\n",
      "154   2023-03-23\n",
      "155   2023-05-16\n",
      "156   2023-04-07\n",
      "157   2023-05-18\n",
      "158   2023-03-24\n",
      "159   2023-02-15\n",
      "160   2022-12-13\n",
      "Name: ss_publication_date, dtype: datetime64[ns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper title</th>\n",
       "      <th>semantic scholar title</th>\n",
       "      <th>ss_publication_date</th>\n",
       "      <th>ss_year</th>\n",
       "      <th>citation_count</th>\n",
       "      <th>query</th>\n",
       "      <th>day_queried</th>\n",
       "      <th>end_date</th>\n",
       "      <th>days_from_pub_to_end_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT</td>\n",
       "      <td>A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT</td>\n",
       "      <td>2023-02-21</td>\n",
       "      <td>2023</td>\n",
       "      <td>163</td>\n",
       "      <td>A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>243.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Taxonomy of Prompt Modifiers for Text-To-Image Generation</td>\n",
       "      <td>A Taxonomy of Prompt Modifiers for Text-To-Image Generation</td>\n",
       "      <td>2022-04-20</td>\n",
       "      <td>2022</td>\n",
       "      <td>43</td>\n",
       "      <td>A Taxonomy of Prompt Modifiers for Text To Image Generation</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>550.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts</td>\n",
       "      <td>AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts</td>\n",
       "      <td>2021-10-04</td>\n",
       "      <td>2021</td>\n",
       "      <td>146</td>\n",
       "      <td>AI Chains: Transparent and Controllable Human AI Interaction by Chaining Large Language Model Prompts</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>748.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ART: Automatic multi-step reasoning and tool-use for large language models</td>\n",
       "      <td>ART: Automatic multi-step reasoning and tool-use for large language models</td>\n",
       "      <td>2023-03-16</td>\n",
       "      <td>2023</td>\n",
       "      <td>39</td>\n",
       "      <td>ART: Automatic multi step reasoning and tool use for large language models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>220.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Active Prompting with Chain-of-Thought for Large Language Models</td>\n",
       "      <td>Active Prompting with Chain-of-Thought for Large Language Models</td>\n",
       "      <td>2023-02-23</td>\n",
       "      <td>2023</td>\n",
       "      <td>34</td>\n",
       "      <td>Active Prompting with Chain of Thought for Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>241.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>An automatically discovered chain-of-thought prompt generalizes to novel models and datasets</td>\n",
       "      <td>An automatically discovered chain-of-thought prompt generalizes to novel models and datasets</td>\n",
       "      <td>2023-05-04</td>\n",
       "      <td>2023</td>\n",
       "      <td>6</td>\n",
       "      <td>An automatically discovered chain of thought prompt generalizes to novel models and datasets</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>171.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ask Me Anything: A simple strategy for prompting language models</td>\n",
       "      <td>Ask Me Anything: A simple strategy for prompting language models</td>\n",
       "      <td>2022-10-05</td>\n",
       "      <td>2022</td>\n",
       "      <td>65</td>\n",
       "      <td>Ask Me Anything: A simple strategy for prompting language models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>382.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Atlas: Few-shot Learning with Retrieval Augmented Language Models</td>\n",
       "      <td>Few-shot Learning with Retrieval Augmented Language Models</td>\n",
       "      <td>2022-08-05</td>\n",
       "      <td>2022</td>\n",
       "      <td>202</td>\n",
       "      <td>Atlas: Few shot Learning with Retrieval Augmented Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>443.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts</td>\n",
       "      <td>Eliciting Knowledge from Language Models Using Automatically Generated Prompts</td>\n",
       "      <td>2020-10-29</td>\n",
       "      <td>2020</td>\n",
       "      <td>217</td>\n",
       "      <td>AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>1088.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Automatic Chain of Thought Prompting in Large Language Models</td>\n",
       "      <td>Automatic Chain of Thought Prompting in Large Language Models</td>\n",
       "      <td>2022-10-07</td>\n",
       "      <td>2022</td>\n",
       "      <td>201</td>\n",
       "      <td>Automatic Chain of Thought Prompting in Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>380.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data</td>\n",
       "      <td>Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data</td>\n",
       "      <td>2023-02-24</td>\n",
       "      <td>2023</td>\n",
       "      <td>21</td>\n",
       "      <td>Automatic Prompt Augmentation and Selection with Chain of Thought from Labeled Data</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>240.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BERTese: Learning to Speak to BERT</td>\n",
       "      <td>BERTese: Learning to Speak to BERT</td>\n",
       "      <td>2021-03-09</td>\n",
       "      <td>2021</td>\n",
       "      <td>74</td>\n",
       "      <td>BERTese: Learning to Speak to BERT</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>957.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Batch Prompting: Efficient Inference with LLM APIs</td>\n",
       "      <td>Batch Prompting: Efficient Inference with Large Language Model APIs</td>\n",
       "      <td>2023-01-19</td>\n",
       "      <td>2023</td>\n",
       "      <td>10</td>\n",
       "      <td>Batch Prompting: Efficient Inference with LLM APIs</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>276.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Better Zero-Shot Reasoning with Self-Adaptive Prompting</td>\n",
       "      <td>Better Zero-Shot Reasoning with Self-Adaptive Prompting</td>\n",
       "      <td>2023-05-23</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>Better Zero Shot Reasoning with Self Adaptive Prompting</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>152.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Boosted Prompt Ensembles for Large Language Models</td>\n",
       "      <td>Boosted Prompt Ensembles for Large Language Models</td>\n",
       "      <td>2023-04-12</td>\n",
       "      <td>2023</td>\n",
       "      <td>12</td>\n",
       "      <td>Boosted Prompt Ensembles for Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>193.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints</td>\n",
       "      <td>Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints</td>\n",
       "      <td>2023-02-17</td>\n",
       "      <td>2023</td>\n",
       "      <td>6</td>\n",
       "      <td>Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>247.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale Language Model Society</td>\n",
       "      <td>CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale Language Model Society</td>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>2023</td>\n",
       "      <td>64</td>\n",
       "      <td>CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale Language Model Society</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>205.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Calibrate Before Use: Improving Few-Shot Performance of Language Models</td>\n",
       "      <td>Calibrate Before Use: Improving Few-Shot Performance of Language Models</td>\n",
       "      <td>2021-02-19</td>\n",
       "      <td>2021</td>\n",
       "      <td>616</td>\n",
       "      <td>Calibrate Before Use: Improving Few Shot Performance of Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>975.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT</td>\n",
       "      <td>Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT</td>\n",
       "      <td>2023-02-19</td>\n",
       "      <td>2023</td>\n",
       "      <td>73</td>\n",
       "      <td>Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine tuned BERT</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>245.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Can We Edit Factual Knowledge by In-Context Learning?</td>\n",
       "      <td>Can We Edit Factual Knowledge by In-Context Learning?</td>\n",
       "      <td>2023-05-22</td>\n",
       "      <td>2023</td>\n",
       "      <td>11</td>\n",
       "      <td>Can We Edit Factual Knowledge by In Context Learning?</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>153.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Chain of Hindsight Aligns Language Models with Feedback</td>\n",
       "      <td>Chain of Hindsight Aligns Language Models with Feedback</td>\n",
       "      <td>2023-02-06</td>\n",
       "      <td>2023</td>\n",
       "      <td>37</td>\n",
       "      <td>Chain of Hindsight Aligns Language Models with Feedback</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>258.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Chain of Thought Prompting Elicits Reasoning in Large Language Models</td>\n",
       "      <td>Chain of Thought Prompting Elicits Reasoning in Large Language Models</td>\n",
       "      <td>2022-01-28</td>\n",
       "      <td>2022</td>\n",
       "      <td>2105</td>\n",
       "      <td>Chain of Thought Prompting Elicits Reasoning in Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>632.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models</td>\n",
       "      <td>Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>Chain of Symbol Prompting Elicits Planning in Large Langauge Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Chain-of-Verification Reduces Hallucination in Large Language Models</td>\n",
       "      <td>Chain-of-Verification Reduces Hallucination in Large Language Models</td>\n",
       "      <td>2023-09-20</td>\n",
       "      <td>2023</td>\n",
       "      <td>10</td>\n",
       "      <td>Chain of Verification Reduces Hallucination in Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>32.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer</td>\n",
       "      <td>ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer</td>\n",
       "      <td>2023-06-13</td>\n",
       "      <td>2023</td>\n",
       "      <td>12</td>\n",
       "      <td>ChatGPT vs Human authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>131.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification</td>\n",
       "      <td>CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification</td>\n",
       "      <td>2023-03-07</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>229.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Complexity-Based Prompting for Multi-Step Reasoning</td>\n",
       "      <td>Complexity-Based Prompting for Multi-Step Reasoning</td>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>2022</td>\n",
       "      <td>116</td>\n",
       "      <td>Complexity Based Prompting for Multi Step Reasoning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>384.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Compositional Exemplars for In-context Learning</td>\n",
       "      <td>Compositional Exemplars for In-context Learning</td>\n",
       "      <td>2023-02-11</td>\n",
       "      <td>2023</td>\n",
       "      <td>24</td>\n",
       "      <td>Compositional Exemplars for In context Learning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>253.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt</td>\n",
       "      <td>Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt</td>\n",
       "      <td>2023-05-17</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>Compress, Then Prompt: Improving Accuracy Efficiency Trade off of LLM Inference with Transferable Prompt</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>158.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers</td>\n",
       "      <td>Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers</td>\n",
       "      <td>2023-09-15</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>37.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Constitutional AI: Harmlessness from AI Feedback</td>\n",
       "      <td>Constitutional AI: Harmlessness from AI Feedback</td>\n",
       "      <td>2022-12-15</td>\n",
       "      <td>2022</td>\n",
       "      <td>307</td>\n",
       "      <td>Constitutional AI: Harmlessness from AI Feedback</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>311.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Context-faithful Prompting for Large Language Models</td>\n",
       "      <td>Context-faithful Prompting for Large Language Models</td>\n",
       "      <td>2023-03-20</td>\n",
       "      <td>2023</td>\n",
       "      <td>21</td>\n",
       "      <td>Context faithful Prompting for Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>216.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>CooK: Empowering General-Purpose Language Models with Modular and Collaborative Knowledge</td>\n",
       "      <td>SHARED OPEN VOCABULARIES AND SEMANTIC MEDIA</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>CooK: Empowering General Purpose Language Models with Modular and Collaborative Knowledge</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Decomposed Prompting: A Modular Approach for Solving Complex Tasks</td>\n",
       "      <td>Decomposed Prompting: A Modular Approach for Solving Complex Tasks</td>\n",
       "      <td>2022-10-05</td>\n",
       "      <td>2022</td>\n",
       "      <td>104</td>\n",
       "      <td>Decomposed Prompting: A Modular Approach for Solving Complex Tasks</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>382.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP</td>\n",
       "      <td>Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP</td>\n",
       "      <td>2022-12-28</td>\n",
       "      <td>2022</td>\n",
       "      <td>57</td>\n",
       "      <td>Demonstrate Search Predict: Composing retrieval and language models for knowledge intensive NLP</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>298.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Design Guidelines for Prompt Engineering Text-to-Image Generative Models</td>\n",
       "      <td>Design Guidelines for Prompt Engineering Text-to-Image Generative Models</td>\n",
       "      <td>2021-09-14</td>\n",
       "      <td>2021</td>\n",
       "      <td>142</td>\n",
       "      <td>Design Guidelines for Prompt Engineering Text to Image Generative Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>768.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Discovering Language Model Behaviors with Model-Written Evaluations</td>\n",
       "      <td>Discovering Language Model Behaviors with Model-Written Evaluations</td>\n",
       "      <td>2022-12-19</td>\n",
       "      <td>2022</td>\n",
       "      <td>87</td>\n",
       "      <td>Discovering Language Model Behaviors with Model Written Evaluations</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>307.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>DocPrompting: Generating Code by Retrieving the Docs</td>\n",
       "      <td>DocPrompting: Generating Code by Retrieving the Docs</td>\n",
       "      <td>2022-07-13</td>\n",
       "      <td>2022</td>\n",
       "      <td>27</td>\n",
       "      <td>DocPrompting: Generating Code by Retrieving the Docs</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>466.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning</td>\n",
       "      <td>Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning</td>\n",
       "      <td>2022-09-29</td>\n",
       "      <td>2022</td>\n",
       "      <td>51</td>\n",
       "      <td>Dynamic Prompt Learning via Policy Gradient for Semi structured Mathematical Reasoning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>388.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Dynamic Prompting: A Unified Framework for Prompt Tuning</td>\n",
       "      <td>Dynamic Prompting: A Unified Framework for Prompt Tuning</td>\n",
       "      <td>2023-03-06</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>Dynamic Prompting: A Unified Framework for Prompt Tuning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>230.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Effectiveness of Data Augmentation for Prefix Tuning with Limited Data</td>\n",
       "      <td>Effectiveness of Data Augmentation for Prefix Tuning with Limited Data</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>Effectiveness of Data Augmentation for Prefix Tuning with Limited Data</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Efficient Prompting via Dynamic In-Context Learning</td>\n",
       "      <td>Efficient Prompting via Dynamic In-Context Learning</td>\n",
       "      <td>2023-05-18</td>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>Efficient Prompting via Dynamic In Context Learning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>157.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference</td>\n",
       "      <td>Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference</td>\n",
       "      <td>2022-11-21</td>\n",
       "      <td>2022</td>\n",
       "      <td>16</td>\n",
       "      <td>Enhancing Self Consistency and Performance of Pre Trained Language Models through Natural Language Inference</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>335.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Evaluating Large Language Models Trained on Code</td>\n",
       "      <td>Evaluating Large Language Models Trained on Code</td>\n",
       "      <td>2021-07-07</td>\n",
       "      <td>2021</td>\n",
       "      <td>1617</td>\n",
       "      <td>Evaluating Large Language Models Trained on Code</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>837.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Evaluating the Robustness of Discrete Prompts</td>\n",
       "      <td>Evaluating the Robustness of Discrete Prompts</td>\n",
       "      <td>2023-02-11</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>Evaluating the Robustness of Discrete Prompts</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>253.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples</td>\n",
       "      <td>Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples</td>\n",
       "      <td>2022-09-05</td>\n",
       "      <td>2022</td>\n",
       "      <td>12</td>\n",
       "      <td>Evaluating the Susceptibility of Pre Trained Language Models via Handcrafted Adversarial Examples</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>412.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>EvoPrompting: Language Models for Code-Level Neural Architecture Search</td>\n",
       "      <td>EvoPrompting: Language Models for Code-Level Neural Architecture Search</td>\n",
       "      <td>2023-02-28</td>\n",
       "      <td>2023</td>\n",
       "      <td>12</td>\n",
       "      <td>EvoPrompting: Language Models for Code Level Neural Architecture Search</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>236.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Exploring Lottery Prompts for Pre-trained Language Models</td>\n",
       "      <td>Exploring Lottery Prompts for Pre-trained Language Models</td>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>Exploring Lottery Prompts for Pre trained Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>144.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Fairness-guided Few-shot Prompting for Large Language Models</td>\n",
       "      <td>Fairness-guided Few-shot Prompting for Large Language Models</td>\n",
       "      <td>2023-03-23</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>Fairness guided Few shot Prompting for Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>213.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity</td>\n",
       "      <td>Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity</td>\n",
       "      <td>2021-04-18</td>\n",
       "      <td>2021</td>\n",
       "      <td>408</td>\n",
       "      <td>Fantastically Ordered Prompts and Where to Find Them: Overcoming Few Shot Prompt Order Sensitivity</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>917.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency</td>\n",
       "      <td>Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency</td>\n",
       "      <td>2023-05-18</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>Flatness Aware Prompt Selection Improves Accuracy and Sample Efficiency</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>157.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Flesch or Fumble? Evaluating Readability Standard Alignment of Instruction-Tuned Language Models</td>\n",
       "      <td>Flesch or Fumble? Evaluating Readability Standard Alignment of Instruction-Tuned Language Models</td>\n",
       "      <td>2023-09-11</td>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>Flesch or Fumble? Evaluating Readability Standard Alignment of Instruction Tuned Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>41.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Focused Prefix Tuning for Controllable Text Generation</td>\n",
       "      <td>Focused Prefix Tuning for Controllable Text Generation</td>\n",
       "      <td>2023-06-01</td>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>Focused Prefix Tuning for Controllable Text Generation</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>143.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting</td>\n",
       "      <td>From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting</td>\n",
       "      <td>2023-09-08</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>From Sparse to Dense: GPT 4 Summarization with Chain of Density Prompting</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>44.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>GPT-4 Technical Report</td>\n",
       "      <td>GPT-4 Technical Report</td>\n",
       "      <td>2023-03-15</td>\n",
       "      <td>2023</td>\n",
       "      <td>1529</td>\n",
       "      <td>GPT 4 Technical Report</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>221.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Generated Knowledge Prompting for Commonsense Reasoning</td>\n",
       "      <td>Generated Knowledge Prompting for Commonsense Reasoning</td>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>2021</td>\n",
       "      <td>114</td>\n",
       "      <td>Generated Knowledge Prompting for Commonsense Reasoning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>737.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Global Prompt Cell: A Portable Control Module for Effective Prompt</td>\n",
       "      <td>Global Prompt Cell: A Portable Control Module for Effective Prompt</td>\n",
       "      <td>2023-04-12</td>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>Global Prompt Cell: A Portable Control Module for Effective Prompt</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>193.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Graph of Thoughts: Solving Elaborate Problems with Large Language Models</td>\n",
       "      <td>Graph of Thoughts: Solving Elaborate Problems with Large Language Models</td>\n",
       "      <td>2023-08-18</td>\n",
       "      <td>2023</td>\n",
       "      <td>35</td>\n",
       "      <td>Graph of Thoughts: Solving Elaborate Problems with Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>65.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks</td>\n",
       "      <td>GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks</td>\n",
       "      <td>2023-02-16</td>\n",
       "      <td>2023</td>\n",
       "      <td>16</td>\n",
       "      <td>GraphPrompt: Unifying Pre Training and Downstream Tasks for Graph Neural Networks</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>248.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Guiding Large Language Models via Directional Stimulus Prompting</td>\n",
       "      <td>Guiding Large Language Models via Directional Stimulus Prompting</td>\n",
       "      <td>2023-02-22</td>\n",
       "      <td>2023</td>\n",
       "      <td>10</td>\n",
       "      <td>Guiding Large Language Models via Directional Stimulus Prompting</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>242.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery</td>\n",
       "      <td>Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery</td>\n",
       "      <td>2023-02-07</td>\n",
       "      <td>2023</td>\n",
       "      <td>38</td>\n",
       "      <td>Hard Prompts Made Easy: Gradient Based Discrete Optimization for Prompt Tuning and Discovery</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>257.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Hierarchical Prompting Assists Large Language Model on Web Navigation</td>\n",
       "      <td>Hierarchical Prompting Assists Large Language Model on Web Navigation</td>\n",
       "      <td>2023-05-23</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>Hierarchical Prompting Assists Large Language Model on Web Navigation</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>152.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>How Can We Know What Language Models Know?</td>\n",
       "      <td>How Can We Know What Language Models Know?</td>\n",
       "      <td>2019-11-28</td>\n",
       "      <td>2019</td>\n",
       "      <td>763</td>\n",
       "      <td>How Can We Know What Language Models Know?</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>1424.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>How Does In-Context Learning Help Prompt Tuning?</td>\n",
       "      <td>How Does In-Context Learning Help Prompt Tuning?</td>\n",
       "      <td>2023-02-22</td>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>How Does In Context Learning Help Prompt Tuning?</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>242.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks</td>\n",
       "      <td>How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>2023</td>\n",
       "      <td>24</td>\n",
       "      <td>How Robust is GPT 3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>235.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Ignore Previous Prompt: Attack Techniques For Language Models</td>\n",
       "      <td>Ignore Previous Prompt: Attack Techniques For Language Models</td>\n",
       "      <td>2022-11-17</td>\n",
       "      <td>2022</td>\n",
       "      <td>68</td>\n",
       "      <td>Ignore Previous Prompt: Attack Techniques For Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>339.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>In-Context Instruction Learning</td>\n",
       "      <td>In-Context Instruction Learning</td>\n",
       "      <td>2023-02-28</td>\n",
       "      <td>2023</td>\n",
       "      <td>19</td>\n",
       "      <td>In Context Instruction Learning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>236.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>In-Context Learning of Large Language Models Explained as Kernel Regression</td>\n",
       "      <td>In-Context Learning of Large Language Models Explained as Kernel Regression</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>In Context Learning of Large Language Models Explained as Kernel Regression</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Interactive Natural Language Processing</td>\n",
       "      <td>Interactive Natural Language Processing</td>\n",
       "      <td>2023-05-22</td>\n",
       "      <td>2023</td>\n",
       "      <td>14</td>\n",
       "      <td>Interactive Natural Language Processing</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>153.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Is GPT-4 a Good Data Analyst?</td>\n",
       "      <td>Is GPT-4 a Good Data Analyst?</td>\n",
       "      <td>2023-05-24</td>\n",
       "      <td>2023</td>\n",
       "      <td>17</td>\n",
       "      <td>Is GPT 4 a Good Data Analyst?</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>151.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning</td>\n",
       "      <td>Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning</td>\n",
       "      <td>2023-03-18</td>\n",
       "      <td>2023</td>\n",
       "      <td>16</td>\n",
       "      <td>Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>218.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Language Is Not All You Need: Aligning Perception with Language Models</td>\n",
       "      <td>Language Is Not All You Need: Aligning Perception with Language Models</td>\n",
       "      <td>2023-02-27</td>\n",
       "      <td>2023</td>\n",
       "      <td>154</td>\n",
       "      <td>Language Is Not All You Need: Aligning Perception with Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>237.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought</td>\n",
       "      <td>Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought</td>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>2022</td>\n",
       "      <td>58</td>\n",
       "      <td>Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain of Thought</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>384.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Language Models are Few-Shot Learners</td>\n",
       "      <td>Language Models are Few-Shot Learners</td>\n",
       "      <td>2020-05-28</td>\n",
       "      <td>2020</td>\n",
       "      <td>16440</td>\n",
       "      <td>Language Models are Few Shot Learners</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>1242.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Large Language Model Guided Tree-of-Thought</td>\n",
       "      <td>Large Language Model Guided Tree-of-Thought</td>\n",
       "      <td>2023-05-15</td>\n",
       "      <td>2023</td>\n",
       "      <td>26</td>\n",
       "      <td>Large Language Model Guided Tree of Thought</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>160.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Large Language Models Are Human-Level Prompt Engineers</td>\n",
       "      <td>Large Language Models Are Human-Level Prompt Engineers</td>\n",
       "      <td>2022-11-03</td>\n",
       "      <td>2022</td>\n",
       "      <td>194</td>\n",
       "      <td>Large Language Models Are Human Level Prompt Engineers</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>353.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Large Language Models Can Be Easily Distracted by Irrelevant Context</td>\n",
       "      <td>Large Language Models Can Be Easily Distracted by Irrelevant Context</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>2023</td>\n",
       "      <td>70</td>\n",
       "      <td>Large Language Models Can Be Easily Distracted by Irrelevant Context</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>264.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Large Language Models are Zero-Shot Reasoners</td>\n",
       "      <td>Large Language Models are Zero-Shot Reasoners</td>\n",
       "      <td>2022-05-24</td>\n",
       "      <td>2022</td>\n",
       "      <td>882</td>\n",
       "      <td>Large Language Models are Zero Shot Reasoners</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>516.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Large Language Models are reasoners with Self-Verification</td>\n",
       "      <td>Large Language Models are reasoners with Self-Verification</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2022</td>\n",
       "      <td>31</td>\n",
       "      <td>Large Language Models are reasoners with Self Verification</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Larger language models do in-context learning differently</td>\n",
       "      <td>Larger language models do in-context learning differently</td>\n",
       "      <td>2023-03-07</td>\n",
       "      <td>2023</td>\n",
       "      <td>87</td>\n",
       "      <td>Larger language models do in context learning differently</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>229.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Learning from Task Descriptions</td>\n",
       "      <td>Learning from Task Descriptions</td>\n",
       "      <td>2020-11-01</td>\n",
       "      <td>2020</td>\n",
       "      <td>64</td>\n",
       "      <td>Learning from Task Descriptions</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>1085.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Learning to Generate Task-Specific Adapters from Task Description</td>\n",
       "      <td>Learning to Generate Task-Specific Adapters from Task Description</td>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>2021</td>\n",
       "      <td>18</td>\n",
       "      <td>Learning to Generate Task Specific Adapters from Task Description</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>1023.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Learning to Transfer Prompts for Text Generation</td>\n",
       "      <td>Learning to Transfer Prompts for Text Generation</td>\n",
       "      <td>2022-05-03</td>\n",
       "      <td>2022</td>\n",
       "      <td>18</td>\n",
       "      <td>Learning to Transfer Prompts for Text Generation</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>537.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Least-to-most promptingenables complex reasoning in large language models</td>\n",
       "      <td>Least-to-Most Prompting Enables Complex Reasoning in Large Language Models</td>\n",
       "      <td>2022-05-21</td>\n",
       "      <td>2022</td>\n",
       "      <td>369</td>\n",
       "      <td>Least to most promptingenables complex reasoning in large language models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>519.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses</td>\n",
       "      <td>Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses</td>\n",
       "      <td>2023-05-30</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>145.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning with LLMs</td>\n",
       "      <td>Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning with LLMs</td>\n",
       "      <td>2023-05-19</td>\n",
       "      <td>2023</td>\n",
       "      <td>6</td>\n",
       "      <td>Let's Sample Step by Step: Adaptive Consistency for Efficient Reasoning with LLMs</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>156.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Let's Verify Step by Step</td>\n",
       "      <td>Let's Verify Step by Step</td>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>2023</td>\n",
       "      <td>64</td>\n",
       "      <td>Let's Verify Step by Step</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>144.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning</td>\n",
       "      <td>MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning</td>\n",
       "      <td>2022-05-01</td>\n",
       "      <td>2022</td>\n",
       "      <td>31</td>\n",
       "      <td>MRKL Systems: A modular, neuro symbolic architecture that combines large language models, external knowledge sources and discrete reasoning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>539.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods</td>\n",
       "      <td>Machine-Generated Text: A Comprehensive Survey of Threat Models and Detection Methods</td>\n",
       "      <td>2022-10-13</td>\n",
       "      <td>2022</td>\n",
       "      <td>28</td>\n",
       "      <td>Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>374.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations</td>\n",
       "      <td>Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations</td>\n",
       "      <td>2022-05-24</td>\n",
       "      <td>2022</td>\n",
       "      <td>86</td>\n",
       "      <td>Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>516.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Making Pre-trained Language Models Better Few-shot Learners</td>\n",
       "      <td>Making Pre-trained Language Models Better Few-shot Learners</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>2021</td>\n",
       "      <td>1082</td>\n",
       "      <td>Making Pre trained Language Models Better Few shot Learners</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>1024.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Meta-in-context learning in large language models</td>\n",
       "      <td>Meta-in-context learning in large language models</td>\n",
       "      <td>2023-05-22</td>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>Meta in context learning in large language models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>153.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Mixture of Soft Prompts for Controllable Data Generation</td>\n",
       "      <td>Mixture of Soft Prompts for Controllable Data Generation</td>\n",
       "      <td>2023-03-02</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>Mixture of Soft Prompts for Controllable Data Generation</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>234.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Model-tuning Via Prompts Makes NLP Models Adversarially Robust</td>\n",
       "      <td>Model-tuning Via Prompts Makes NLP Models Adversarially Robust</td>\n",
       "      <td>2023-03-13</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>Model tuning Via Prompts Makes NLP Models Adversarially Robust</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>223.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models</td>\n",
       "      <td>More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2023</td>\n",
       "      <td>37</td>\n",
       "      <td>More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application Integrated Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of Thought Prompting</td>\n",
       "      <td>MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of Thought Prompting</td>\n",
       "      <td>2023-05-26</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>MultiTool CoT: GPT 3 Can Use Multiple External Tools with Chain of Thought Prompting</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>149.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Multimodal Chain-of-Thought Reasoning in Language Models</td>\n",
       "      <td>Multimodal Chain-of-Thought Reasoning in Language Models</td>\n",
       "      <td>2023-02-02</td>\n",
       "      <td>2023</td>\n",
       "      <td>91</td>\n",
       "      <td>Multimodal Chain of Thought Reasoning in Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>262.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning</td>\n",
       "      <td>Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning</td>\n",
       "      <td>2023-03-06</td>\n",
       "      <td>2023</td>\n",
       "      <td>21</td>\n",
       "      <td>Multitask Prompt Tuning Enables Parameter Efficient Transfer Learning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>230.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Multitask Prompted Training Enables Zero-Shot Task Generalization</td>\n",
       "      <td>Multitask Prompted Training Enables Zero-Shot Task Generalization</td>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>2021</td>\n",
       "      <td>924</td>\n",
       "      <td>Multitask Prompted Training Enables Zero Shot Task Generalization</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>737.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning</td>\n",
       "      <td>On Second Thought, Lets Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning</td>\n",
       "      <td>2022-12-15</td>\n",
       "      <td>2022</td>\n",
       "      <td>23</td>\n",
       "      <td>On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero Shot Reasoning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>311.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>On the Advance of Making Language Models Better Reasoners</td>\n",
       "      <td>On the Advance of Making Language Models Better Reasoners</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2022</td>\n",
       "      <td>106</td>\n",
       "      <td>On the Advance of Making Language Models Better Reasoners</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>OpenICL: An Open-Source Framework for In-context Learning</td>\n",
       "      <td>OpenICL: An Open-Source Framework for In-context Learning</td>\n",
       "      <td>2023-03-06</td>\n",
       "      <td>2023</td>\n",
       "      <td>12</td>\n",
       "      <td>OpenICL: An Open Source Framework for In context Learning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>230.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>PAL: Program-aided Language Models</td>\n",
       "      <td>PAL: Program-aided Language Models</td>\n",
       "      <td>2022-11-18</td>\n",
       "      <td>2022</td>\n",
       "      <td>198</td>\n",
       "      <td>PAL: Program aided Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>338.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents</td>\n",
       "      <td>PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents</td>\n",
       "      <td>2023-05-23</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>152.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>PPT: Pre-trained Prompt Tuning for Few-shot Learning</td>\n",
       "      <td>PPT: Pre-trained Prompt Tuning for Few-shot Learning</td>\n",
       "      <td>2021-09-09</td>\n",
       "      <td>2021</td>\n",
       "      <td>234</td>\n",
       "      <td>PPT: Pre trained Prompt Tuning for Few shot Learning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>773.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models</td>\n",
       "      <td>Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models</td>\n",
       "      <td>2023-05-06</td>\n",
       "      <td>2023</td>\n",
       "      <td>46</td>\n",
       "      <td>Plan and Solve Prompting: Improving Zero Shot Chain of Thought Reasoning by Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>169.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Post Hoc Explanations of Language Models Can Improve Language Models</td>\n",
       "      <td>Post Hoc Explanations of Language Models Can Improve Language Models</td>\n",
       "      <td>2023-05-19</td>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>Post Hoc Explanations of Language Models Can Improve Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>156.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>Pre-Training to Learn in Context</td>\n",
       "      <td>Pre-Training to Learn in Context</td>\n",
       "      <td>2023-05-16</td>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>Pre Training to Learn in Context</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>159.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Prefix-Tuning: Optimizing Continuous Prompts for Generation</td>\n",
       "      <td>Prefix-Tuning: Optimizing Continuous Prompts for Generation</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2021</td>\n",
       "      <td>1682</td>\n",
       "      <td>Prefix Tuning: Optimizing Continuous Prompts for Generation</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Progressive Prompts: Continual Learning for Language Models</td>\n",
       "      <td>Progressive Prompts: Continual Learning for Language Models</td>\n",
       "      <td>2023-01-29</td>\n",
       "      <td>2023</td>\n",
       "      <td>20</td>\n",
       "      <td>Progressive Prompts: Continual Learning for Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>266.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Prompt Engineering for Large Language Models</td>\n",
       "      <td>Prompt Engineering for Large Language Models</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>Prompt Engineering for Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm</td>\n",
       "      <td>Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm</td>\n",
       "      <td>2021-02-15</td>\n",
       "      <td>2021</td>\n",
       "      <td>317</td>\n",
       "      <td>Prompt Programming for Large Language Models: Beyond the Few Shot Paradigm</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>979.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners</td>\n",
       "      <td>Prompt, Generate, Then Cache: Cascade of Foundation Models Makes Strong Few-Shot Learners</td>\n",
       "      <td>2023-03-03</td>\n",
       "      <td>2023</td>\n",
       "      <td>30</td>\n",
       "      <td>Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few shot Learners</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>233.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>PromptChainer: Chaining Large Language Model Prompts through Visual Programming</td>\n",
       "      <td>PromptChainer: Chaining Large Language Model Prompts through Visual Programming</td>\n",
       "      <td>2022-03-13</td>\n",
       "      <td>2022</td>\n",
       "      <td>78</td>\n",
       "      <td>PromptChainer: Chaining Large Language Model Prompts through Visual Programming</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>588.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Promptagator: Few-shot Dense Retrieval From 8 Examples</td>\n",
       "      <td>Promptagator: Few-shot Dense Retrieval From 8 Examples</td>\n",
       "      <td>2022-09-23</td>\n",
       "      <td>2022</td>\n",
       "      <td>66</td>\n",
       "      <td>Promptagator: Few shot Dense Retrieval From 8 Examples</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>394.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Prompted Opinion Summarization with GPT-3.5</td>\n",
       "      <td>Prompted Opinion Summarization with GPT-3.5</td>\n",
       "      <td>2022-11-29</td>\n",
       "      <td>2022</td>\n",
       "      <td>8</td>\n",
       "      <td>Prompted Opinion Summarization with GPT 3.5</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>327.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Prompting GPT-3 To Be Reliable</td>\n",
       "      <td>Prompting GPT-3 To Be Reliable</td>\n",
       "      <td>2022-10-17</td>\n",
       "      <td>2022</td>\n",
       "      <td>72</td>\n",
       "      <td>Prompting GPT 3 To Be Reliable</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>370.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL</td>\n",
       "      <td>Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL</td>\n",
       "      <td>2023-09-13</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>Query Dependent Prompt Evaluation and Optimization with Offline Inverse RL</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>39.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>REFINER: Reasoning Feedback on Intermediate Representations</td>\n",
       "      <td>REFINER: Reasoning Feedback on Intermediate Representations</td>\n",
       "      <td>2023-04-04</td>\n",
       "      <td>2023</td>\n",
       "      <td>35</td>\n",
       "      <td>REFINER: Reasoning Feedback on Intermediate Representations</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>201.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>Re-Reading Improves Reasoning in Language Models</td>\n",
       "      <td>Re-Reading Improves Reasoning in Language Models</td>\n",
       "      <td>2023-09-12</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>Re Reading Improves Reasoning in Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>40.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>ReAct: Synergizing Reasoning and Acting in Language Models</td>\n",
       "      <td>ReAct: Synergizing Reasoning and Acting in Language Models</td>\n",
       "      <td>2022-10-06</td>\n",
       "      <td>2022</td>\n",
       "      <td>332</td>\n",
       "      <td>ReAct: Synergizing Reasoning and Acting in Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>381.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Reasoning with Language Model is Planning with World Model</td>\n",
       "      <td>Reasoning with Language Model is Planning with World Model</td>\n",
       "      <td>2023-05-24</td>\n",
       "      <td>2023</td>\n",
       "      <td>34</td>\n",
       "      <td>Reasoning with Language Model is Planning with World Model</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>151.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>Recitation-Augmented Language Models</td>\n",
       "      <td>Recitation-Augmented Language Models</td>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>2022</td>\n",
       "      <td>37</td>\n",
       "      <td>Recitation Augmented Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>383.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Reflexion: an autonomous agent with dynamic memory and self-reflection</td>\n",
       "      <td>Reflexion: an autonomous agent with dynamic memory and self-reflection</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2023</td>\n",
       "      <td>105</td>\n",
       "      <td>Reflexion: an autonomous agent with dynamic memory and self reflection</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Reframing Instructional Prompts to GPTk's Language</td>\n",
       "      <td>Reframing Instructional Prompts to GPTks Language</td>\n",
       "      <td>2021-09-16</td>\n",
       "      <td>2021</td>\n",
       "      <td>121</td>\n",
       "      <td>Reframing Instructional Prompts to GPTk's Language</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>766.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling</td>\n",
       "      <td>Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling</td>\n",
       "      <td>2023-05-17</td>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>Reprompting: Automated Chain of Thought Prompt Inference Through Gibbs Sampling</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>158.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?</td>\n",
       "      <td>Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?</td>\n",
       "      <td>2022-02-25</td>\n",
       "      <td>2022</td>\n",
       "      <td>431</td>\n",
       "      <td>Rethinking the Role of Demonstrations: What Makes In Context Learning Work?</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>604.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</td>\n",
       "      <td>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</td>\n",
       "      <td>2020-05-22</td>\n",
       "      <td>2020</td>\n",
       "      <td>956</td>\n",
       "      <td>Retrieval Augmented Generation for Knowledge Intensive NLP Tasks</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>1248.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Revisiting Automated Prompting: Are We Actually Doing Better?</td>\n",
       "      <td>Revisiting Automated Prompting: Are We Actually Doing Better?</td>\n",
       "      <td>2023-04-07</td>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>Revisiting Automated Prompting: Are We Actually Doing Better?</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>198.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Satisfiability-Aided Language Models Using Declarative Prompting</td>\n",
       "      <td>Satisfiability-Aided Language Models Using Declarative Prompting</td>\n",
       "      <td>2023-05-16</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>Satisfiability Aided Language Models Using Declarative Prompting</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>159.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Scalable Prompt Generation for Semi-supervised Learning with Language Models</td>\n",
       "      <td>Scalable Prompt Generation for Semi-supervised Learning with Language Models</td>\n",
       "      <td>2023-02-18</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>Scalable Prompt Generation for Semi supervised Learning with Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>246.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>Scaling Instruction-Finetuned Language Models</td>\n",
       "      <td>Scaling Instruction-Finetuned Language Models</td>\n",
       "      <td>2022-10-20</td>\n",
       "      <td>2022</td>\n",
       "      <td>823</td>\n",
       "      <td>Scaling Instruction Finetuned Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>367.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>Scaling Laws for Neural Language Models</td>\n",
       "      <td>Scaling Laws for Neural Language Models</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>2020</td>\n",
       "      <td>1530</td>\n",
       "      <td>Scaling Laws for Neural Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>1368.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Self-Consistency Improves Chain of Thought Reasoning in Language Models</td>\n",
       "      <td>Self-Consistency Improves Chain of Thought Reasoning in Language Models</td>\n",
       "      <td>2022-03-21</td>\n",
       "      <td>2022</td>\n",
       "      <td>659</td>\n",
       "      <td>Self Consistency Improves Chain of Thought Reasoning in Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>580.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Self-Critique Prompting with Large Language Models for Inductive Instructions</td>\n",
       "      <td>Self-Critique Prompting with Large Language Models for Inductive Instructions</td>\n",
       "      <td>2023-05-23</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>Self Critique Prompting with Large Language Models for Inductive Instructions</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>152.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Self-Refine: Iterative Refinement with Self-Feedback</td>\n",
       "      <td>Self-Refine: Iterative Refinement with Self-Feedback</td>\n",
       "      <td>2023-03-30</td>\n",
       "      <td>2023</td>\n",
       "      <td>201</td>\n",
       "      <td>Self Refine: Iterative Refinement with Self Feedback</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>206.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Show Your Work: Scratchpads for Intermediate Computation with Language Models</td>\n",
       "      <td>Show Your Work: Scratchpads for Intermediate Computation with Language Models</td>\n",
       "      <td>2021-11-30</td>\n",
       "      <td>2021</td>\n",
       "      <td>315</td>\n",
       "      <td>Show Your Work: Scratchpads for Intermediate Computation with Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>691.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding</td>\n",
       "      <td>Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding</td>\n",
       "      <td>2023-07-28</td>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>Skeleton of Thought: Large Language Models Can Do Parallel Decoding</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>86.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer</td>\n",
       "      <td>Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer</td>\n",
       "      <td>2023-03-03</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>233.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>Successive Prompting for Decomposing Complex Questions</td>\n",
       "      <td>Successive Prompting for Decomposing Complex Questions</td>\n",
       "      <td>2022-12-08</td>\n",
       "      <td>2022</td>\n",
       "      <td>40</td>\n",
       "      <td>Successive Prompting for Decomposing Complex Questions</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>318.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains</td>\n",
       "      <td>SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains</td>\n",
       "      <td>2023-02-14</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>SwitchPrompt: Learning Domain Specific Gated Soft Prompts for Classification in Low Resource Domains</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>250.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models</td>\n",
       "      <td>Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models</td>\n",
       "      <td>2023-02-01</td>\n",
       "      <td>2023</td>\n",
       "      <td>26</td>\n",
       "      <td>Synthetic Prompting: Generating Chain of Thought Demonstrations for Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>263.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks</td>\n",
       "      <td>TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks</td>\n",
       "      <td>2023-05-19</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>156.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Teaching Algorithmic Reasoning via In-context Learning</td>\n",
       "      <td>Teaching Algorithmic Reasoning via In-context Learning</td>\n",
       "      <td>2022-11-15</td>\n",
       "      <td>2022</td>\n",
       "      <td>52</td>\n",
       "      <td>Teaching Algorithmic Reasoning via In context Learning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>341.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>The Capacity for Moral Self-Correction in Large Language Models</td>\n",
       "      <td>The Capacity for Moral Self-Correction in Large Language Models</td>\n",
       "      <td>2023-02-15</td>\n",
       "      <td>2023</td>\n",
       "      <td>49</td>\n",
       "      <td>The Capacity for Moral Self Correction in Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>249.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>The Power of Scale for Parameter-Efficient Prompt Tuning</td>\n",
       "      <td>The Power of Scale for Parameter-Efficient Prompt Tuning</td>\n",
       "      <td>2021-04-18</td>\n",
       "      <td>2021</td>\n",
       "      <td>1530</td>\n",
       "      <td>The Power of Scale for Parameter Efficient Prompt Tuning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>917.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning</td>\n",
       "      <td>The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning</td>\n",
       "      <td>2022-05-06</td>\n",
       "      <td>2022</td>\n",
       "      <td>47</td>\n",
       "      <td>The Unreliability of Explanations in Few shot Prompting for Textual Reasoning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>534.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>The Web Can Be Your Oyster for Improving Large Language Models</td>\n",
       "      <td>The Web Can Be Your Oyster for Improving Large Language Models</td>\n",
       "      <td>2023-05-18</td>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>The Web Can Be Your Oyster for Improving Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>157.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Toxicity Detection with Generative Prompt-based Inference</td>\n",
       "      <td>Toxicity Detection with Generative Prompt-based Inference</td>\n",
       "      <td>2022-05-24</td>\n",
       "      <td>2022</td>\n",
       "      <td>11</td>\n",
       "      <td>Toxicity Detection with Generative Prompt based Inference</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>516.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Training language models to follow instructions with human feedback</td>\n",
       "      <td>Training language models to follow instructions with human feedback</td>\n",
       "      <td>2022-03-04</td>\n",
       "      <td>2022</td>\n",
       "      <td>3009</td>\n",
       "      <td>Training language models to follow instructions with human feedback</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>597.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Tree of Thoughts: Deliberate Problem Solving with Large Language Models</td>\n",
       "      <td>Tree of Thoughts: Deliberate Problem Solving with Large Language Models</td>\n",
       "      <td>2023-05-17</td>\n",
       "      <td>2023</td>\n",
       "      <td>227</td>\n",
       "      <td>Tree of Thoughts: Deliberate Problem Solving with Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>158.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding</td>\n",
       "      <td>TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding</td>\n",
       "      <td>2023-05-19</td>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>156.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation</td>\n",
       "      <td>UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation</td>\n",
       "      <td>2023-03-15</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>UPRISE: Universal Prompt Retrieval for Improving Zero Shot Evaluation</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>221.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Universality and Limitations of Prompt Tuning</td>\n",
       "      <td>Universality and Limitations of Prompt Tuning</td>\n",
       "      <td>2023-05-30</td>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>Universality and Limitations of Prompt Tuning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>145.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Visual-Language Prompt Tuning with Knowledge-guided Context Optimization</td>\n",
       "      <td>Visual-Language Prompt Tuning with Knowledge-Guided Context Optimization</td>\n",
       "      <td>2023-03-23</td>\n",
       "      <td>2023</td>\n",
       "      <td>11</td>\n",
       "      <td>Visual Language Prompt Tuning with Knowledge guided Context Optimization</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>213.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>What In-Context Learning \"Learns\" In-Context: Disentangling Task Recognition and Task Learning</td>\n",
       "      <td>What In-Context Learning \"Learns\" In-Context: Disentangling Task Recognition and Task Learning</td>\n",
       "      <td>2023-05-16</td>\n",
       "      <td>2023</td>\n",
       "      <td>19</td>\n",
       "      <td>What In Context Learning \"Learns\" In Context: Disentangling Task Recognition and Task Learning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>159.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>Why think step-by-step? Reasoning emerges from the locality of experience</td>\n",
       "      <td>Why think step-by-step? Reasoning emerges from the locality of experience</td>\n",
       "      <td>2023-04-07</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>Why think step by step? Reasoning emerges from the locality of experience</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>198.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>ZeroPrompt: Streaming Acoustic Encoders are Zero-Shot Masked LMs</td>\n",
       "      <td>ZeroPrompt: Streaming Acoustic Encoders are Zero-Shot Masked LMs</td>\n",
       "      <td>2023-05-18</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>ZeroPrompt: Streaming Acoustic Encoders are Zero Shot Masked LMs</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>157.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>kNN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference</td>\n",
       "      <td>kNN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference</td>\n",
       "      <td>2023-03-24</td>\n",
       "      <td>2023</td>\n",
       "      <td>16</td>\n",
       "      <td>kNN Prompting: Beyond Context Learning with Calibration Free Nearest Neighbor Inference</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>212.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>-la-carte Prompt Tuning (APT): Combining Distinct Data Via Composable Prompting</td>\n",
       "      <td>-la-carte Prompt Tuning (APT): Combining Distinct Data Via Composable Prompting</td>\n",
       "      <td>2023-02-15</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td> la carte Prompt Tuning (APT): Combining Distinct Data Via Composable Prompting</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>249.564615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Structured Prompting: Scaling In-Context Learning to 1, 000 Examples</td>\n",
       "      <td>Structured Prompting: Scaling In-Context Learning to 1, 000 Examples</td>\n",
       "      <td>2022-12-13</td>\n",
       "      <td>2022</td>\n",
       "      <td>20</td>\n",
       "      <td>Structured Prompting: Scaling In Context Learning to 1, 000 Examples</td>\n",
       "      <td>2023-10-22</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>313.564615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                     paper title  \\\n",
       "0                                                                            A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT   \n",
       "1                                                                                    A Taxonomy of Prompt Modifiers for Text-To-Image Generation   \n",
       "2                                          AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts   \n",
       "3                                                                     ART: Automatic multi-step reasoning and tool-use for large language models   \n",
       "4                                                                               Active Prompting with Chain-of-Thought for Large Language Models   \n",
       "5                                                   An automatically discovered chain-of-thought prompt generalizes to novel models and datasets   \n",
       "6                                                                               Ask Me Anything: A simple strategy for prompting language models   \n",
       "7                                                                              Atlas: Few-shot Learning with Retrieval Augmented Language Models   \n",
       "8                                                      AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts   \n",
       "9                                                                                  Automatic Chain of Thought Prompting in Large Language Models   \n",
       "10                                                           Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data   \n",
       "11                                                                                                            BERTese: Learning to Speak to BERT   \n",
       "12                                                                                            Batch Prompting: Efficient Inference with LLM APIs   \n",
       "13                                                                                       Better Zero-Shot Reasoning with Self-Adaptive Prompting   \n",
       "14                                                                                            Boosted Prompt Ensembles for Large Language Models   \n",
       "15                                            Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints   \n",
       "16                                                      CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale Language Model Society   \n",
       "17                                                                       Calibrate Before Use: Improving Few-Shot Performance of Language Models   \n",
       "18                                                                Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT   \n",
       "19                                                                                         Can We Edit Factual Knowledge by In-Context Learning?   \n",
       "20                                                                                       Chain of Hindsight Aligns Language Models with Feedback   \n",
       "21                                                                         Chain of Thought Prompting Elicits Reasoning in Large Language Models   \n",
       "22                                                                           Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models   \n",
       "23                                                                          Chain-of-Verification Reduces Hallucination in Large Language Models   \n",
       "24                                     ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer   \n",
       "25                                                           CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification   \n",
       "26                                                                                           Complexity-Based Prompting for Multi-Step Reasoning   \n",
       "27                                                                                               Compositional Exemplars for In-context Learning   \n",
       "28                                      Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt   \n",
       "29                                               Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers   \n",
       "30                                                                                              Constitutional AI: Harmlessness from AI Feedback   \n",
       "31                                                                                          Context-faithful Prompting for Large Language Models   \n",
       "32                                                     CooK: Empowering General-Purpose Language Models with Modular and Collaborative Knowledge   \n",
       "33                                                                            Decomposed Prompting: A Modular Approach for Solving Complex Tasks   \n",
       "34                                               Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP   \n",
       "35                                                                      Design Guidelines for Prompt Engineering Text-to-Image Generative Models   \n",
       "36                                                                           Discovering Language Model Behaviors with Model-Written Evaluations   \n",
       "37                                                                                          DocPrompting: Generating Code by Retrieving the Docs   \n",
       "38                                                        Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning   \n",
       "39                                                                                      Dynamic Prompting: A Unified Framework for Prompt Tuning   \n",
       "40                                                                        Effectiveness of Data Augmentation for Prefix Tuning with Limited Data   \n",
       "41                                                                                           Efficient Prompting via Dynamic In-Context Learning   \n",
       "42                                  Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference   \n",
       "43                                                                                              Evaluating Large Language Models Trained on Code   \n",
       "44                                                                                                 Evaluating the Robustness of Discrete Prompts   \n",
       "45                                             Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples   \n",
       "46                                                                       EvoPrompting: Language Models for Code-Level Neural Architecture Search   \n",
       "47                                                                                     Exploring Lottery Prompts for Pre-trained Language Models   \n",
       "48                                                                                  Fairness-guided Few-shot Prompting for Large Language Models   \n",
       "49                                            Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity   \n",
       "50                                                                       Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency   \n",
       "51                                              Flesch or Fumble? Evaluating Readability Standard Alignment of Instruction-Tuned Language Models   \n",
       "52                                                                                        Focused Prefix Tuning for Controllable Text Generation   \n",
       "53                                                                     From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting   \n",
       "54                                                                                                                        GPT-4 Technical Report   \n",
       "55                                                                                       Generated Knowledge Prompting for Commonsense Reasoning   \n",
       "56                                                                            Global Prompt Cell: A Portable Control Module for Effective Prompt   \n",
       "57                                                                      Graph of Thoughts: Solving Elaborate Problems with Large Language Models   \n",
       "58                                                             GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks   \n",
       "59                                                                              Guiding Large Language Models via Directional Stimulus Prompting   \n",
       "60                                                  Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery   \n",
       "61                                                                         Hierarchical Prompting Assists Large Language Model on Web Navigation   \n",
       "62                                                                                                    How Can We Know What Language Models Know?   \n",
       "63                                                                                              How Does In-Context Learning Help Prompt Tuning?   \n",
       "64                                                  How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks   \n",
       "65                                                                                 Ignore Previous Prompt: Attack Techniques For Language Models   \n",
       "66                                                                                                               In-Context Instruction Learning   \n",
       "67                                                                   In-Context Learning of Large Language Models Explained as Kernel Regression   \n",
       "68                                                                                                       Interactive Natural Language Processing   \n",
       "69                                                                                                                 Is GPT-4 a Good Data Analyst?   \n",
       "70                                                          Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning   \n",
       "71                                                                        Language Is Not All You Need: Aligning Perception with Language Models   \n",
       "72                                                        Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought   \n",
       "73                                                                                                         Language Models are Few-Shot Learners   \n",
       "74                                                                                                   Large Language Model Guided Tree-of-Thought   \n",
       "75                                                                                        Large Language Models Are Human-Level Prompt Engineers   \n",
       "76                                                                          Large Language Models Can Be Easily Distracted by Irrelevant Context   \n",
       "77                                                                                                 Large Language Models are Zero-Shot Reasoners   \n",
       "78                                                                                    Large Language Models are reasoners with Self-Verification   \n",
       "79                                                                                     Larger language models do in-context learning differently   \n",
       "80                                                                                                               Learning from Task Descriptions   \n",
       "81                                                                             Learning to Generate Task-Specific Adapters from Task Description   \n",
       "82                                                                                              Learning to Transfer Prompts for Text Generation   \n",
       "83                                                                    Least-to-most promptingenables complex reasoning in large language models   \n",
       "84                                                           Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses   \n",
       "85                                                             Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning with LLMs   \n",
       "86                                                                                                                     Let's Verify Step by Step   \n",
       "87   MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning   \n",
       "88                                                         Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods   \n",
       "89                                                                Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations   \n",
       "90                                                                                   Making Pre-trained Language Models Better Few-shot Learners   \n",
       "91                                                                                             Meta-in-context learning in large language models   \n",
       "92                                                                                      Mixture of Soft Prompts for Controllable Data Generation   \n",
       "93                                                                                Model-tuning Via Prompts Makes NLP Models Adversarially Robust   \n",
       "94        More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models   \n",
       "95                                                          MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of Thought Prompting   \n",
       "96                                                                                      Multimodal Chain-of-Thought Reasoning in Language Models   \n",
       "97                                                                         Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning   \n",
       "98                                                                             Multitask Prompted Training Enables Zero-Shot Task Generalization   \n",
       "99                                                     On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning   \n",
       "100                                                                                    On the Advance of Making Language Models Better Reasoners   \n",
       "101                                                                                    OpenICL: An Open-Source Framework for In-context Learning   \n",
       "102                                                                                                           PAL: Program-aided Language Models   \n",
       "103                                                       PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents   \n",
       "104                                                                                         PPT: Pre-trained Prompt Tuning for Few-shot Learning   \n",
       "105                                            Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models   \n",
       "106                                                                         Post Hoc Explanations of Language Models Can Improve Language Models   \n",
       "107                                                                                                             Pre-Training to Learn in Context   \n",
       "108                                                                                  Prefix-Tuning: Optimizing Continuous Prompts for Generation   \n",
       "109                                                                                  Progressive Prompts: Continual Learning for Language Models   \n",
       "110                                                                                                 Prompt Engineering for Large Language Models   \n",
       "111                                                                   Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm   \n",
       "112                                                    Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners   \n",
       "113                                                              PromptChainer: Chaining Large Language Model Prompts through Visual Programming   \n",
       "114                                                                                       Promptagator: Few-shot Dense Retrieval From 8 Examples   \n",
       "115                                                                                                  Prompted Opinion Summarization with GPT-3.5   \n",
       "116                                                                                                               Prompting GPT-3 To Be Reliable   \n",
       "117                                                                   Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL   \n",
       "118                                                                                  REFINER: Reasoning Feedback on Intermediate Representations   \n",
       "119                                                                                             Re-Reading Improves Reasoning in Language Models   \n",
       "120                                                                                   ReAct: Synergizing Reasoning and Acting in Language Models   \n",
       "121                                                                                   Reasoning with Language Model is Planning with World Model   \n",
       "122                                                                                                         Recitation-Augmented Language Models   \n",
       "123                                                                       Reflexion: an autonomous agent with dynamic memory and self-reflection   \n",
       "124                                                                                           Reframing Instructional Prompts to GPTk's Language   \n",
       "125                                                              Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling   \n",
       "126                                                                  Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?   \n",
       "127                                                                             Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks   \n",
       "128                                                                                Revisiting Automated Prompting: Are We Actually Doing Better?   \n",
       "129                                                                             Satisfiability-Aided Language Models Using Declarative Prompting   \n",
       "130                                                                 Scalable Prompt Generation for Semi-supervised Learning with Language Models   \n",
       "131                                                                                                Scaling Instruction-Finetuned Language Models   \n",
       "132                                                                                                      Scaling Laws for Neural Language Models   \n",
       "133                                                                      Self-Consistency Improves Chain of Thought Reasoning in Language Models   \n",
       "134                                                                Self-Critique Prompting with Large Language Models for Inductive Instructions   \n",
       "135                                                                                         Self-Refine: Iterative Refinement with Self-Feedback   \n",
       "136                                                                Show Your Work: Scratchpads for Intermediate Computation with Language Models   \n",
       "137                                                                          Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding   \n",
       "138                                                                         Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer   \n",
       "139                                                                                       Successive Prompting for Decomposing Complex Questions   \n",
       "140                                         SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains   \n",
       "141                                                    Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models   \n",
       "142                                                                      TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks   \n",
       "143                                                                                       Teaching Algorithmic Reasoning via In-context Learning   \n",
       "144                                                                              The Capacity for Moral Self-Correction in Large Language Models   \n",
       "145                                                                                     The Power of Scale for Parameter-Efficient Prompt Tuning   \n",
       "146                                                                The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning   \n",
       "147                                                                               The Web Can Be Your Oyster for Improving Large Language Models   \n",
       "148                                                                                    Toxicity Detection with Generative Prompt-based Inference   \n",
       "149                                                                          Training language models to follow instructions with human feedback   \n",
       "150                                                                      Tree of Thoughts: Deliberate Problem Solving with Large Language Models   \n",
       "151                                                                TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding   \n",
       "152                                                                        UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation   \n",
       "153                                                                                                Universality and Limitations of Prompt Tuning   \n",
       "154                                                                     Visual-Language Prompt Tuning with Knowledge-guided Context Optimization   \n",
       "155                                               What In-Context Learning \"Learns\" In-Context: Disentangling Task Recognition and Task Learning   \n",
       "156                                                                    Why think step-by-step? Reasoning emerges from the locality of experience   \n",
       "157                                                                             ZeroPrompt: Streaming Acoustic Encoders are Zero-Shot Masked LMs   \n",
       "158                                                      kNN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference   \n",
       "159                                                             -la-carte Prompt Tuning (APT): Combining Distinct Data Via Composable Prompting   \n",
       "160                                                                         Structured Prompting: Scaling In-Context Learning to 1, 000 Examples   \n",
       "\n",
       "                                                                                                                          semantic scholar title  \\\n",
       "0                                                                            A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT   \n",
       "1                                                                                    A Taxonomy of Prompt Modifiers for Text-To-Image Generation   \n",
       "2                                          AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts   \n",
       "3                                                                     ART: Automatic multi-step reasoning and tool-use for large language models   \n",
       "4                                                                               Active Prompting with Chain-of-Thought for Large Language Models   \n",
       "5                                                   An automatically discovered chain-of-thought prompt generalizes to novel models and datasets   \n",
       "6                                                                               Ask Me Anything: A simple strategy for prompting language models   \n",
       "7                                                                                     Few-shot Learning with Retrieval Augmented Language Models   \n",
       "8                                                                 Eliciting Knowledge from Language Models Using Automatically Generated Prompts   \n",
       "9                                                                                  Automatic Chain of Thought Prompting in Large Language Models   \n",
       "10                                                           Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data   \n",
       "11                                                                                                            BERTese: Learning to Speak to BERT   \n",
       "12                                                                           Batch Prompting: Efficient Inference with Large Language Model APIs   \n",
       "13                                                                                       Better Zero-Shot Reasoning with Self-Adaptive Prompting   \n",
       "14                                                                                            Boosted Prompt Ensembles for Large Language Models   \n",
       "15                                            Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints   \n",
       "16                                                      CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale Language Model Society   \n",
       "17                                                                       Calibrate Before Use: Improving Few-Shot Performance of Language Models   \n",
       "18                                                                Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT   \n",
       "19                                                                                         Can We Edit Factual Knowledge by In-Context Learning?   \n",
       "20                                                                                       Chain of Hindsight Aligns Language Models with Feedback   \n",
       "21                                                                         Chain of Thought Prompting Elicits Reasoning in Large Language Models   \n",
       "22                                                                           Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models   \n",
       "23                                                                          Chain-of-Verification Reduces Hallucination in Large Language Models   \n",
       "24                                     ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer   \n",
       "25                                                           CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification   \n",
       "26                                                                                           Complexity-Based Prompting for Multi-Step Reasoning   \n",
       "27                                                                                               Compositional Exemplars for In-context Learning   \n",
       "28                                      Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt   \n",
       "29                                               Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers   \n",
       "30                                                                                              Constitutional AI: Harmlessness from AI Feedback   \n",
       "31                                                                                          Context-faithful Prompting for Large Language Models   \n",
       "32                                                                                                   SHARED OPEN VOCABULARIES AND SEMANTIC MEDIA   \n",
       "33                                                                            Decomposed Prompting: A Modular Approach for Solving Complex Tasks   \n",
       "34                                               Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP   \n",
       "35                                                                      Design Guidelines for Prompt Engineering Text-to-Image Generative Models   \n",
       "36                                                                           Discovering Language Model Behaviors with Model-Written Evaluations   \n",
       "37                                                                                          DocPrompting: Generating Code by Retrieving the Docs   \n",
       "38                                                        Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning   \n",
       "39                                                                                      Dynamic Prompting: A Unified Framework for Prompt Tuning   \n",
       "40                                                                        Effectiveness of Data Augmentation for Prefix Tuning with Limited Data   \n",
       "41                                                                                           Efficient Prompting via Dynamic In-Context Learning   \n",
       "42                                  Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference   \n",
       "43                                                                                              Evaluating Large Language Models Trained on Code   \n",
       "44                                                                                                 Evaluating the Robustness of Discrete Prompts   \n",
       "45                                             Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples   \n",
       "46                                                                       EvoPrompting: Language Models for Code-Level Neural Architecture Search   \n",
       "47                                                                                     Exploring Lottery Prompts for Pre-trained Language Models   \n",
       "48                                                                                  Fairness-guided Few-shot Prompting for Large Language Models   \n",
       "49                                            Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity   \n",
       "50                                                                       Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency   \n",
       "51                                              Flesch or Fumble? Evaluating Readability Standard Alignment of Instruction-Tuned Language Models   \n",
       "52                                                                                        Focused Prefix Tuning for Controllable Text Generation   \n",
       "53                                                                     From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting   \n",
       "54                                                                                                                        GPT-4 Technical Report   \n",
       "55                                                                                       Generated Knowledge Prompting for Commonsense Reasoning   \n",
       "56                                                                            Global Prompt Cell: A Portable Control Module for Effective Prompt   \n",
       "57                                                                      Graph of Thoughts: Solving Elaborate Problems with Large Language Models   \n",
       "58                                                             GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks   \n",
       "59                                                                              Guiding Large Language Models via Directional Stimulus Prompting   \n",
       "60                                                  Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery   \n",
       "61                                                                         Hierarchical Prompting Assists Large Language Model on Web Navigation   \n",
       "62                                                                                                    How Can We Know What Language Models Know?   \n",
       "63                                                                                              How Does In-Context Learning Help Prompt Tuning?   \n",
       "64                                                  How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks   \n",
       "65                                                                                 Ignore Previous Prompt: Attack Techniques For Language Models   \n",
       "66                                                                                                               In-Context Instruction Learning   \n",
       "67                                                                   In-Context Learning of Large Language Models Explained as Kernel Regression   \n",
       "68                                                                                                       Interactive Natural Language Processing   \n",
       "69                                                                                                                 Is GPT-4 a Good Data Analyst?   \n",
       "70                                                          Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning   \n",
       "71                                                                        Language Is Not All You Need: Aligning Perception with Language Models   \n",
       "72                                                        Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought   \n",
       "73                                                                                                         Language Models are Few-Shot Learners   \n",
       "74                                                                                                   Large Language Model Guided Tree-of-Thought   \n",
       "75                                                                                        Large Language Models Are Human-Level Prompt Engineers   \n",
       "76                                                                          Large Language Models Can Be Easily Distracted by Irrelevant Context   \n",
       "77                                                                                                 Large Language Models are Zero-Shot Reasoners   \n",
       "78                                                                                    Large Language Models are reasoners with Self-Verification   \n",
       "79                                                                                     Larger language models do in-context learning differently   \n",
       "80                                                                                                               Learning from Task Descriptions   \n",
       "81                                                                             Learning to Generate Task-Specific Adapters from Task Description   \n",
       "82                                                                                              Learning to Transfer Prompts for Text Generation   \n",
       "83                                                                    Least-to-Most Prompting Enables Complex Reasoning in Large Language Models   \n",
       "84                                                           Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses   \n",
       "85                                                             Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning with LLMs   \n",
       "86                                                                                                                     Let's Verify Step by Step   \n",
       "87   MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning   \n",
       "88                                                         Machine-Generated Text: A Comprehensive Survey of Threat Models and Detection Methods   \n",
       "89                                                                Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations   \n",
       "90                                                                                   Making Pre-trained Language Models Better Few-shot Learners   \n",
       "91                                                                                             Meta-in-context learning in large language models   \n",
       "92                                                                                      Mixture of Soft Prompts for Controllable Data Generation   \n",
       "93                                                                                Model-tuning Via Prompts Makes NLP Models Adversarially Robust   \n",
       "94        More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models   \n",
       "95                                                          MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of Thought Prompting   \n",
       "96                                                                                      Multimodal Chain-of-Thought Reasoning in Language Models   \n",
       "97                                                                         Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning   \n",
       "98                                                                             Multitask Prompted Training Enables Zero-Shot Task Generalization   \n",
       "99                                                     On Second Thought, Lets Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning   \n",
       "100                                                                                    On the Advance of Making Language Models Better Reasoners   \n",
       "101                                                                                    OpenICL: An Open-Source Framework for In-context Learning   \n",
       "102                                                                                                           PAL: Program-aided Language Models   \n",
       "103                                                       PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents   \n",
       "104                                                                                         PPT: Pre-trained Prompt Tuning for Few-shot Learning   \n",
       "105                                            Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models   \n",
       "106                                                                         Post Hoc Explanations of Language Models Can Improve Language Models   \n",
       "107                                                                                                             Pre-Training to Learn in Context   \n",
       "108                                                                                  Prefix-Tuning: Optimizing Continuous Prompts for Generation   \n",
       "109                                                                                  Progressive Prompts: Continual Learning for Language Models   \n",
       "110                                                                                                 Prompt Engineering for Large Language Models   \n",
       "111                                                                   Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm   \n",
       "112                                                    Prompt, Generate, Then Cache: Cascade of Foundation Models Makes Strong Few-Shot Learners   \n",
       "113                                                              PromptChainer: Chaining Large Language Model Prompts through Visual Programming   \n",
       "114                                                                                       Promptagator: Few-shot Dense Retrieval From 8 Examples   \n",
       "115                                                                                                  Prompted Opinion Summarization with GPT-3.5   \n",
       "116                                                                                                               Prompting GPT-3 To Be Reliable   \n",
       "117                                                                   Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL   \n",
       "118                                                                                  REFINER: Reasoning Feedback on Intermediate Representations   \n",
       "119                                                                                             Re-Reading Improves Reasoning in Language Models   \n",
       "120                                                                                   ReAct: Synergizing Reasoning and Acting in Language Models   \n",
       "121                                                                                   Reasoning with Language Model is Planning with World Model   \n",
       "122                                                                                                         Recitation-Augmented Language Models   \n",
       "123                                                                       Reflexion: an autonomous agent with dynamic memory and self-reflection   \n",
       "124                                                                                           Reframing Instructional Prompts to GPTks Language   \n",
       "125                                                              Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling   \n",
       "126                                                                  Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?   \n",
       "127                                                                             Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks   \n",
       "128                                                                                Revisiting Automated Prompting: Are We Actually Doing Better?   \n",
       "129                                                                             Satisfiability-Aided Language Models Using Declarative Prompting   \n",
       "130                                                                 Scalable Prompt Generation for Semi-supervised Learning with Language Models   \n",
       "131                                                                                                Scaling Instruction-Finetuned Language Models   \n",
       "132                                                                                                      Scaling Laws for Neural Language Models   \n",
       "133                                                                      Self-Consistency Improves Chain of Thought Reasoning in Language Models   \n",
       "134                                                                Self-Critique Prompting with Large Language Models for Inductive Instructions   \n",
       "135                                                                                         Self-Refine: Iterative Refinement with Self-Feedback   \n",
       "136                                                                Show Your Work: Scratchpads for Intermediate Computation with Language Models   \n",
       "137                                                                          Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding   \n",
       "138                                                                         Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer   \n",
       "139                                                                                       Successive Prompting for Decomposing Complex Questions   \n",
       "140                                         SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains   \n",
       "141                                                    Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models   \n",
       "142                                                                      TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks   \n",
       "143                                                                                       Teaching Algorithmic Reasoning via In-context Learning   \n",
       "144                                                                              The Capacity for Moral Self-Correction in Large Language Models   \n",
       "145                                                                                     The Power of Scale for Parameter-Efficient Prompt Tuning   \n",
       "146                                                                The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning   \n",
       "147                                                                               The Web Can Be Your Oyster for Improving Large Language Models   \n",
       "148                                                                                    Toxicity Detection with Generative Prompt-based Inference   \n",
       "149                                                                          Training language models to follow instructions with human feedback   \n",
       "150                                                                      Tree of Thoughts: Deliberate Problem Solving with Large Language Models   \n",
       "151                                                                TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding   \n",
       "152                                                                        UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation   \n",
       "153                                                                                                Universality and Limitations of Prompt Tuning   \n",
       "154                                                                     Visual-Language Prompt Tuning with Knowledge-Guided Context Optimization   \n",
       "155                                               What In-Context Learning \"Learns\" In-Context: Disentangling Task Recognition and Task Learning   \n",
       "156                                                                    Why think step-by-step? Reasoning emerges from the locality of experience   \n",
       "157                                                                             ZeroPrompt: Streaming Acoustic Encoders are Zero-Shot Masked LMs   \n",
       "158                                                      kNN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference   \n",
       "159                                                             -la-carte Prompt Tuning (APT): Combining Distinct Data Via Composable Prompting   \n",
       "160                                                                         Structured Prompting: Scaling In-Context Learning to 1, 000 Examples   \n",
       "\n",
       "    ss_publication_date  ss_year  citation_count  \\\n",
       "0            2023-02-21     2023             163   \n",
       "1            2022-04-20     2022              43   \n",
       "2            2021-10-04     2021             146   \n",
       "3            2023-03-16     2023              39   \n",
       "4            2023-02-23     2023              34   \n",
       "5            2023-05-04     2023               6   \n",
       "6            2022-10-05     2022              65   \n",
       "7            2022-08-05     2022             202   \n",
       "8            2020-10-29     2020             217   \n",
       "9            2022-10-07     2022             201   \n",
       "10           2023-02-24     2023              21   \n",
       "11           2021-03-09     2021              74   \n",
       "12           2023-01-19     2023              10   \n",
       "13           2023-05-23     2023               8   \n",
       "14           2023-04-12     2023              12   \n",
       "15           2023-02-17     2023               6   \n",
       "16           2023-03-31     2023              64   \n",
       "17           2021-02-19     2021             616   \n",
       "18           2023-02-19     2023              73   \n",
       "19           2023-05-22     2023              11   \n",
       "20           2023-02-06     2023              37   \n",
       "21           2022-01-28     2022            2105   \n",
       "22                  NaT     2023               2   \n",
       "23           2023-09-20     2023              10   \n",
       "24           2023-06-13     2023              12   \n",
       "25           2023-03-07     2023               3   \n",
       "26           2022-10-03     2022             116   \n",
       "27           2023-02-11     2023              24   \n",
       "28           2023-05-17     2023               3   \n",
       "29           2023-09-15     2023               8   \n",
       "30           2022-12-15     2022             307   \n",
       "31           2023-03-20     2023              21   \n",
       "32                  NaT     2012               1   \n",
       "33           2022-10-05     2022             104   \n",
       "34           2022-12-28     2022              57   \n",
       "35           2021-09-14     2021             142   \n",
       "36           2022-12-19     2022              87   \n",
       "37           2022-07-13     2022              27   \n",
       "38           2022-09-29     2022              51   \n",
       "39           2023-03-06     2023               4   \n",
       "40                  NaT     2023               0   \n",
       "41           2023-05-18     2023               5   \n",
       "42           2022-11-21     2022              16   \n",
       "43           2021-07-07     2021            1617   \n",
       "44           2023-02-11     2023               4   \n",
       "45           2022-09-05     2022              12   \n",
       "46           2023-02-28     2023              12   \n",
       "47           2023-05-31     2023               1   \n",
       "48           2023-03-23     2023               2   \n",
       "49           2021-04-18     2021             408   \n",
       "50           2023-05-18     2023               1   \n",
       "51           2023-09-11     2023               0   \n",
       "52           2023-06-01     2023               0   \n",
       "53           2023-09-08     2023               2   \n",
       "54           2023-03-15     2023            1529   \n",
       "55           2021-10-15     2021             114   \n",
       "56           2023-04-12     2023               0   \n",
       "57           2023-08-18     2023              35   \n",
       "58           2023-02-16     2023              16   \n",
       "59           2023-02-22     2023              10   \n",
       "60           2023-02-07     2023              38   \n",
       "61           2023-05-23     2023               3   \n",
       "62           2019-11-28     2019             763   \n",
       "63           2023-02-22     2023               5   \n",
       "64           2023-03-01     2023              24   \n",
       "65           2022-11-17     2022              68   \n",
       "66           2023-02-28     2023              19   \n",
       "67                  NaT     2023               4   \n",
       "68           2023-05-22     2023              14   \n",
       "69           2023-05-24     2023              17   \n",
       "70           2023-03-18     2023              16   \n",
       "71           2023-02-27     2023             154   \n",
       "72           2022-10-03     2022              58   \n",
       "73           2020-05-28     2020           16440   \n",
       "74           2023-05-15     2023              26   \n",
       "75           2022-11-03     2022             194   \n",
       "76           2023-01-31     2023              70   \n",
       "77           2022-05-24     2022             882   \n",
       "78                  NaT     2022              31   \n",
       "79           2023-03-07     2023              87   \n",
       "80           2020-11-01     2020              64   \n",
       "81           2021-01-02     2021              18   \n",
       "82           2022-05-03     2022              18   \n",
       "83           2022-05-21     2022             369   \n",
       "84           2023-05-30     2023               1   \n",
       "85           2023-05-19     2023               6   \n",
       "86           2023-05-31     2023              64   \n",
       "87           2022-05-01     2022              31   \n",
       "88           2022-10-13     2022              28   \n",
       "89           2022-05-24     2022              86   \n",
       "90           2021-01-01     2021            1082   \n",
       "91           2023-05-22     2023               5   \n",
       "92           2023-03-02     2023               3   \n",
       "93           2023-03-13     2023               3   \n",
       "94                  NaT     2023              37   \n",
       "95           2023-05-26     2023               1   \n",
       "96           2023-02-02     2023              91   \n",
       "97           2023-03-06     2023              21   \n",
       "98           2021-10-15     2021             924   \n",
       "99           2022-12-15     2022              23   \n",
       "100                 NaT     2022             106   \n",
       "101          2023-03-06     2023              12   \n",
       "102          2022-11-18     2022             198   \n",
       "103          2023-05-23     2023               4   \n",
       "104          2021-09-09     2021             234   \n",
       "105          2023-05-06     2023              46   \n",
       "106          2023-05-19     2023               5   \n",
       "107          2023-05-16     2023               5   \n",
       "108                 NaT     2021            1682   \n",
       "109          2023-01-29     2023              20   \n",
       "110                 NaT     2023               3   \n",
       "111          2021-02-15     2021             317   \n",
       "112          2023-03-03     2023              30   \n",
       "113          2022-03-13     2022              78   \n",
       "114          2022-09-23     2022              66   \n",
       "115          2022-11-29     2022               8   \n",
       "116          2022-10-17     2022              72   \n",
       "117          2023-09-13     2023               1   \n",
       "118          2023-04-04     2023              35   \n",
       "119          2023-09-12     2023               2   \n",
       "120          2022-10-06     2022             332   \n",
       "121          2023-05-24     2023              34   \n",
       "122          2022-10-04     2022              37   \n",
       "123                 NaT     2023             105   \n",
       "124          2021-09-16     2021             121   \n",
       "125          2023-05-17     2023               5   \n",
       "126          2022-02-25     2022             431   \n",
       "127          2020-05-22     2020             956   \n",
       "128          2023-04-07     2023               0   \n",
       "129          2023-05-16     2023               3   \n",
       "130          2023-02-18     2023               1   \n",
       "131          2022-10-20     2022             823   \n",
       "132          2020-01-23     2020            1530   \n",
       "133          2022-03-21     2022             659   \n",
       "134          2023-05-23     2023               3   \n",
       "135          2023-03-30     2023             201   \n",
       "136          2021-11-30     2021             315   \n",
       "137          2023-07-28     2023               5   \n",
       "138          2023-03-03     2023               4   \n",
       "139          2022-12-08     2022              40   \n",
       "140          2023-02-14     2023               2   \n",
       "141          2023-02-01     2023              26   \n",
       "142          2023-05-19     2023               4   \n",
       "143          2022-11-15     2022              52   \n",
       "144          2023-02-15     2023              49   \n",
       "145          2021-04-18     2021            1530   \n",
       "146          2022-05-06     2022              47   \n",
       "147          2023-05-18     2023               0   \n",
       "148          2022-05-24     2022              11   \n",
       "149          2022-03-04     2022            3009   \n",
       "150          2023-05-17     2023             227   \n",
       "151          2023-05-19     2023               0   \n",
       "152          2023-03-15     2023               8   \n",
       "153          2023-05-30     2023               0   \n",
       "154          2023-03-23     2023              11   \n",
       "155          2023-05-16     2023              19   \n",
       "156          2023-04-07     2023               8   \n",
       "157          2023-05-18     2023               1   \n",
       "158          2023-03-24     2023              16   \n",
       "159          2023-02-15     2023               4   \n",
       "160          2022-12-13     2022              20   \n",
       "\n",
       "                                                                                                                                           query  \\\n",
       "0                                                                            A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT   \n",
       "1                                                                                    A Taxonomy of Prompt Modifiers for Text To Image Generation   \n",
       "2                                          AI Chains: Transparent and Controllable Human AI Interaction by Chaining Large Language Model Prompts   \n",
       "3                                                                     ART: Automatic multi step reasoning and tool use for large language models   \n",
       "4                                                                               Active Prompting with Chain of Thought for Large Language Models   \n",
       "5                                                   An automatically discovered chain of thought prompt generalizes to novel models and datasets   \n",
       "6                                                                               Ask Me Anything: A simple strategy for prompting language models   \n",
       "7                                                                              Atlas: Few shot Learning with Retrieval Augmented Language Models   \n",
       "8                                                      AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts   \n",
       "9                                                                                  Automatic Chain of Thought Prompting in Large Language Models   \n",
       "10                                                           Automatic Prompt Augmentation and Selection with Chain of Thought from Labeled Data   \n",
       "11                                                                                                            BERTese: Learning to Speak to BERT   \n",
       "12                                                                                            Batch Prompting: Efficient Inference with LLM APIs   \n",
       "13                                                                                       Better Zero Shot Reasoning with Self Adaptive Prompting   \n",
       "14                                                                                            Boosted Prompt Ensembles for Large Language Models   \n",
       "15                                            Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints   \n",
       "16                                                      CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale Language Model Society   \n",
       "17                                                                       Calibrate Before Use: Improving Few Shot Performance of Language Models   \n",
       "18                                                                Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine tuned BERT   \n",
       "19                                                                                         Can We Edit Factual Knowledge by In Context Learning?   \n",
       "20                                                                                       Chain of Hindsight Aligns Language Models with Feedback   \n",
       "21                                                                         Chain of Thought Prompting Elicits Reasoning in Large Language Models   \n",
       "22                                                                           Chain of Symbol Prompting Elicits Planning in Large Langauge Models   \n",
       "23                                                                          Chain of Verification Reduces Hallucination in Large Language Models   \n",
       "24                                     ChatGPT vs Human authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer   \n",
       "25                                                           CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification   \n",
       "26                                                                                           Complexity Based Prompting for Multi Step Reasoning   \n",
       "27                                                                                               Compositional Exemplars for In context Learning   \n",
       "28                                      Compress, Then Prompt: Improving Accuracy Efficiency Trade off of LLM Inference with Transferable Prompt   \n",
       "29                                               Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers   \n",
       "30                                                                                              Constitutional AI: Harmlessness from AI Feedback   \n",
       "31                                                                                          Context faithful Prompting for Large Language Models   \n",
       "32                                                     CooK: Empowering General Purpose Language Models with Modular and Collaborative Knowledge   \n",
       "33                                                                            Decomposed Prompting: A Modular Approach for Solving Complex Tasks   \n",
       "34                                               Demonstrate Search Predict: Composing retrieval and language models for knowledge intensive NLP   \n",
       "35                                                                      Design Guidelines for Prompt Engineering Text to Image Generative Models   \n",
       "36                                                                           Discovering Language Model Behaviors with Model Written Evaluations   \n",
       "37                                                                                          DocPrompting: Generating Code by Retrieving the Docs   \n",
       "38                                                        Dynamic Prompt Learning via Policy Gradient for Semi structured Mathematical Reasoning   \n",
       "39                                                                                      Dynamic Prompting: A Unified Framework for Prompt Tuning   \n",
       "40                                                                        Effectiveness of Data Augmentation for Prefix Tuning with Limited Data   \n",
       "41                                                                                           Efficient Prompting via Dynamic In Context Learning   \n",
       "42                                  Enhancing Self Consistency and Performance of Pre Trained Language Models through Natural Language Inference   \n",
       "43                                                                                              Evaluating Large Language Models Trained on Code   \n",
       "44                                                                                                 Evaluating the Robustness of Discrete Prompts   \n",
       "45                                             Evaluating the Susceptibility of Pre Trained Language Models via Handcrafted Adversarial Examples   \n",
       "46                                                                       EvoPrompting: Language Models for Code Level Neural Architecture Search   \n",
       "47                                                                                     Exploring Lottery Prompts for Pre trained Language Models   \n",
       "48                                                                                  Fairness guided Few shot Prompting for Large Language Models   \n",
       "49                                            Fantastically Ordered Prompts and Where to Find Them: Overcoming Few Shot Prompt Order Sensitivity   \n",
       "50                                                                       Flatness Aware Prompt Selection Improves Accuracy and Sample Efficiency   \n",
       "51                                              Flesch or Fumble? Evaluating Readability Standard Alignment of Instruction Tuned Language Models   \n",
       "52                                                                                        Focused Prefix Tuning for Controllable Text Generation   \n",
       "53                                                                     From Sparse to Dense: GPT 4 Summarization with Chain of Density Prompting   \n",
       "54                                                                                                                        GPT 4 Technical Report   \n",
       "55                                                                                       Generated Knowledge Prompting for Commonsense Reasoning   \n",
       "56                                                                            Global Prompt Cell: A Portable Control Module for Effective Prompt   \n",
       "57                                                                      Graph of Thoughts: Solving Elaborate Problems with Large Language Models   \n",
       "58                                                             GraphPrompt: Unifying Pre Training and Downstream Tasks for Graph Neural Networks   \n",
       "59                                                                              Guiding Large Language Models via Directional Stimulus Prompting   \n",
       "60                                                  Hard Prompts Made Easy: Gradient Based Discrete Optimization for Prompt Tuning and Discovery   \n",
       "61                                                                         Hierarchical Prompting Assists Large Language Model on Web Navigation   \n",
       "62                                                                                                    How Can We Know What Language Models Know?   \n",
       "63                                                                                              How Does In Context Learning Help Prompt Tuning?   \n",
       "64                                                  How Robust is GPT 3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks   \n",
       "65                                                                                 Ignore Previous Prompt: Attack Techniques For Language Models   \n",
       "66                                                                                                               In Context Instruction Learning   \n",
       "67                                                                   In Context Learning of Large Language Models Explained as Kernel Regression   \n",
       "68                                                                                                       Interactive Natural Language Processing   \n",
       "69                                                                                                                 Is GPT 4 a Good Data Analyst?   \n",
       "70                                                          Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning   \n",
       "71                                                                        Language Is Not All You Need: Aligning Perception with Language Models   \n",
       "72                                                        Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain of Thought   \n",
       "73                                                                                                         Language Models are Few Shot Learners   \n",
       "74                                                                                                   Large Language Model Guided Tree of Thought   \n",
       "75                                                                                        Large Language Models Are Human Level Prompt Engineers   \n",
       "76                                                                          Large Language Models Can Be Easily Distracted by Irrelevant Context   \n",
       "77                                                                                                 Large Language Models are Zero Shot Reasoners   \n",
       "78                                                                                    Large Language Models are reasoners with Self Verification   \n",
       "79                                                                                     Larger language models do in context learning differently   \n",
       "80                                                                                                               Learning from Task Descriptions   \n",
       "81                                                                             Learning to Generate Task Specific Adapters from Task Description   \n",
       "82                                                                                              Learning to Transfer Prompts for Text Generation   \n",
       "83                                                                    Least to most promptingenables complex reasoning in large language models   \n",
       "84                                                           Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses   \n",
       "85                                                             Let's Sample Step by Step: Adaptive Consistency for Efficient Reasoning with LLMs   \n",
       "86                                                                                                                     Let's Verify Step by Step   \n",
       "87   MRKL Systems: A modular, neuro symbolic architecture that combines large language models, external knowledge sources and discrete reasoning   \n",
       "88                                                         Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods   \n",
       "89                                                                Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations   \n",
       "90                                                                                   Making Pre trained Language Models Better Few shot Learners   \n",
       "91                                                                                             Meta in context learning in large language models   \n",
       "92                                                                                      Mixture of Soft Prompts for Controllable Data Generation   \n",
       "93                                                                                Model tuning Via Prompts Makes NLP Models Adversarially Robust   \n",
       "94        More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application Integrated Large Language Models   \n",
       "95                                                          MultiTool CoT: GPT 3 Can Use Multiple External Tools with Chain of Thought Prompting   \n",
       "96                                                                                      Multimodal Chain of Thought Reasoning in Language Models   \n",
       "97                                                                         Multitask Prompt Tuning Enables Parameter Efficient Transfer Learning   \n",
       "98                                                                             Multitask Prompted Training Enables Zero Shot Task Generalization   \n",
       "99                                                     On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero Shot Reasoning   \n",
       "100                                                                                    On the Advance of Making Language Models Better Reasoners   \n",
       "101                                                                                    OpenICL: An Open Source Framework for In context Learning   \n",
       "102                                                                                                           PAL: Program aided Language Models   \n",
       "103                                                       PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents   \n",
       "104                                                                                         PPT: Pre trained Prompt Tuning for Few shot Learning   \n",
       "105                                            Plan and Solve Prompting: Improving Zero Shot Chain of Thought Reasoning by Large Language Models   \n",
       "106                                                                         Post Hoc Explanations of Language Models Can Improve Language Models   \n",
       "107                                                                                                             Pre Training to Learn in Context   \n",
       "108                                                                                  Prefix Tuning: Optimizing Continuous Prompts for Generation   \n",
       "109                                                                                  Progressive Prompts: Continual Learning for Language Models   \n",
       "110                                                                                                 Prompt Engineering for Large Language Models   \n",
       "111                                                                   Prompt Programming for Large Language Models: Beyond the Few Shot Paradigm   \n",
       "112                                                    Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few shot Learners   \n",
       "113                                                              PromptChainer: Chaining Large Language Model Prompts through Visual Programming   \n",
       "114                                                                                       Promptagator: Few shot Dense Retrieval From 8 Examples   \n",
       "115                                                                                                  Prompted Opinion Summarization with GPT 3.5   \n",
       "116                                                                                                               Prompting GPT 3 To Be Reliable   \n",
       "117                                                                   Query Dependent Prompt Evaluation and Optimization with Offline Inverse RL   \n",
       "118                                                                                  REFINER: Reasoning Feedback on Intermediate Representations   \n",
       "119                                                                                             Re Reading Improves Reasoning in Language Models   \n",
       "120                                                                                   ReAct: Synergizing Reasoning and Acting in Language Models   \n",
       "121                                                                                   Reasoning with Language Model is Planning with World Model   \n",
       "122                                                                                                         Recitation Augmented Language Models   \n",
       "123                                                                       Reflexion: an autonomous agent with dynamic memory and self reflection   \n",
       "124                                                                                           Reframing Instructional Prompts to GPTk's Language   \n",
       "125                                                              Reprompting: Automated Chain of Thought Prompt Inference Through Gibbs Sampling   \n",
       "126                                                                  Rethinking the Role of Demonstrations: What Makes In Context Learning Work?   \n",
       "127                                                                             Retrieval Augmented Generation for Knowledge Intensive NLP Tasks   \n",
       "128                                                                                Revisiting Automated Prompting: Are We Actually Doing Better?   \n",
       "129                                                                             Satisfiability Aided Language Models Using Declarative Prompting   \n",
       "130                                                                 Scalable Prompt Generation for Semi supervised Learning with Language Models   \n",
       "131                                                                                                Scaling Instruction Finetuned Language Models   \n",
       "132                                                                                                      Scaling Laws for Neural Language Models   \n",
       "133                                                                      Self Consistency Improves Chain of Thought Reasoning in Language Models   \n",
       "134                                                                Self Critique Prompting with Large Language Models for Inductive Instructions   \n",
       "135                                                                                         Self Refine: Iterative Refinement with Self Feedback   \n",
       "136                                                                Show Your Work: Scratchpads for Intermediate Computation with Language Models   \n",
       "137                                                                          Skeleton of Thought: Large Language Models Can Do Parallel Decoding   \n",
       "138                                                                         Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer   \n",
       "139                                                                                       Successive Prompting for Decomposing Complex Questions   \n",
       "140                                         SwitchPrompt: Learning Domain Specific Gated Soft Prompts for Classification in Low Resource Domains   \n",
       "141                                                    Synthetic Prompting: Generating Chain of Thought Demonstrations for Large Language Models   \n",
       "142                                                                      TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks   \n",
       "143                                                                                       Teaching Algorithmic Reasoning via In context Learning   \n",
       "144                                                                              The Capacity for Moral Self Correction in Large Language Models   \n",
       "145                                                                                     The Power of Scale for Parameter Efficient Prompt Tuning   \n",
       "146                                                                The Unreliability of Explanations in Few shot Prompting for Textual Reasoning   \n",
       "147                                                                               The Web Can Be Your Oyster for Improving Large Language Models   \n",
       "148                                                                                    Toxicity Detection with Generative Prompt based Inference   \n",
       "149                                                                          Training language models to follow instructions with human feedback   \n",
       "150                                                                      Tree of Thoughts: Deliberate Problem Solving with Large Language Models   \n",
       "151                                                                TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding   \n",
       "152                                                                        UPRISE: Universal Prompt Retrieval for Improving Zero Shot Evaluation   \n",
       "153                                                                                                Universality and Limitations of Prompt Tuning   \n",
       "154                                                                     Visual Language Prompt Tuning with Knowledge guided Context Optimization   \n",
       "155                                               What In Context Learning \"Learns\" In Context: Disentangling Task Recognition and Task Learning   \n",
       "156                                                                    Why think step by step? Reasoning emerges from the locality of experience   \n",
       "157                                                                             ZeroPrompt: Streaming Acoustic Encoders are Zero Shot Masked LMs   \n",
       "158                                                      kNN Prompting: Beyond Context Learning with Calibration Free Nearest Neighbor Inference   \n",
       "159                                                              la carte Prompt Tuning (APT): Combining Distinct Data Via Composable Prompting   \n",
       "160                                                                         Structured Prompting: Scaling In Context Learning to 1, 000 Examples   \n",
       "\n",
       "             day_queried                   end_date  days_from_pub_to_end_date  \n",
       "0    2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 243.564615  \n",
       "1    2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 550.564615  \n",
       "2    2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 748.564615  \n",
       "3    2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 220.564615  \n",
       "4    2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 241.564615  \n",
       "5    2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 171.564615  \n",
       "6    2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 382.564615  \n",
       "7    2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 443.564615  \n",
       "8    2023-10-22 00:00:00 2023-10-22 13:33:02.768043                1088.564615  \n",
       "9    2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 380.564615  \n",
       "10   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 240.564615  \n",
       "11   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 957.564615  \n",
       "12   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 276.564615  \n",
       "13   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 152.564615  \n",
       "14   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 193.564615  \n",
       "15   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 247.564615  \n",
       "16   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 205.564615  \n",
       "17   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 975.564615  \n",
       "18   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 245.564615  \n",
       "19   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 153.564615  \n",
       "20   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 258.564615  \n",
       "21   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 632.564615  \n",
       "22   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                        NaN  \n",
       "23   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                  32.564615  \n",
       "24   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 131.564615  \n",
       "25   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 229.564615  \n",
       "26   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 384.564615  \n",
       "27   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 253.564615  \n",
       "28   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 158.564615  \n",
       "29   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                  37.564615  \n",
       "30   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 311.564615  \n",
       "31   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 216.564615  \n",
       "32   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                        NaN  \n",
       "33   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 382.564615  \n",
       "34   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 298.564615  \n",
       "35   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 768.564615  \n",
       "36   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 307.564615  \n",
       "37   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 466.564615  \n",
       "38   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 388.564615  \n",
       "39   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 230.564615  \n",
       "40   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                        NaN  \n",
       "41   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 157.564615  \n",
       "42   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 335.564615  \n",
       "43   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 837.564615  \n",
       "44   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 253.564615  \n",
       "45   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 412.564615  \n",
       "46   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 236.564615  \n",
       "47   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 144.564615  \n",
       "48   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 213.564615  \n",
       "49   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 917.564615  \n",
       "50   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 157.564615  \n",
       "51   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                  41.564615  \n",
       "52   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 143.564615  \n",
       "53   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                  44.564615  \n",
       "54   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 221.564615  \n",
       "55   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 737.564615  \n",
       "56   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 193.564615  \n",
       "57   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                  65.564615  \n",
       "58   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 248.564615  \n",
       "59   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 242.564615  \n",
       "60   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 257.564615  \n",
       "61   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 152.564615  \n",
       "62   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                1424.564615  \n",
       "63   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 242.564615  \n",
       "64   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 235.564615  \n",
       "65   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 339.564615  \n",
       "66   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 236.564615  \n",
       "67   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                        NaN  \n",
       "68   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 153.564615  \n",
       "69   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 151.564615  \n",
       "70   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 218.564615  \n",
       "71   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 237.564615  \n",
       "72   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 384.564615  \n",
       "73   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                1242.564615  \n",
       "74   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 160.564615  \n",
       "75   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 353.564615  \n",
       "76   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 264.564615  \n",
       "77   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 516.564615  \n",
       "78   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                        NaN  \n",
       "79   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 229.564615  \n",
       "80   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                1085.564615  \n",
       "81   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                1023.564615  \n",
       "82   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 537.564615  \n",
       "83   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 519.564615  \n",
       "84   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 145.564615  \n",
       "85   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 156.564615  \n",
       "86   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 144.564615  \n",
       "87   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 539.564615  \n",
       "88   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 374.564615  \n",
       "89   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 516.564615  \n",
       "90   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                1024.564615  \n",
       "91   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 153.564615  \n",
       "92   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 234.564615  \n",
       "93   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 223.564615  \n",
       "94   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                        NaN  \n",
       "95   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 149.564615  \n",
       "96   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 262.564615  \n",
       "97   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 230.564615  \n",
       "98   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 737.564615  \n",
       "99   2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 311.564615  \n",
       "100  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                        NaN  \n",
       "101  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 230.564615  \n",
       "102  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 338.564615  \n",
       "103  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 152.564615  \n",
       "104  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 773.564615  \n",
       "105  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 169.564615  \n",
       "106  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 156.564615  \n",
       "107  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 159.564615  \n",
       "108  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                        NaN  \n",
       "109  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 266.564615  \n",
       "110  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                        NaN  \n",
       "111  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 979.564615  \n",
       "112  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 233.564615  \n",
       "113  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 588.564615  \n",
       "114  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 394.564615  \n",
       "115  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 327.564615  \n",
       "116  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 370.564615  \n",
       "117  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                  39.564615  \n",
       "118  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 201.564615  \n",
       "119  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                  40.564615  \n",
       "120  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 381.564615  \n",
       "121  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 151.564615  \n",
       "122  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 383.564615  \n",
       "123  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                        NaN  \n",
       "124  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 766.564615  \n",
       "125  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 158.564615  \n",
       "126  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 604.564615  \n",
       "127  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                1248.564615  \n",
       "128  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 198.564615  \n",
       "129  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 159.564615  \n",
       "130  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 246.564615  \n",
       "131  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 367.564615  \n",
       "132  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                1368.564615  \n",
       "133  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 580.564615  \n",
       "134  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 152.564615  \n",
       "135  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 206.564615  \n",
       "136  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 691.564615  \n",
       "137  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                  86.564615  \n",
       "138  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 233.564615  \n",
       "139  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 318.564615  \n",
       "140  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 250.564615  \n",
       "141  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 263.564615  \n",
       "142  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 156.564615  \n",
       "143  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 341.564615  \n",
       "144  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 249.564615  \n",
       "145  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 917.564615  \n",
       "146  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 534.564615  \n",
       "147  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 157.564615  \n",
       "148  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 516.564615  \n",
       "149  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 597.564615  \n",
       "150  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 158.564615  \n",
       "151  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 156.564615  \n",
       "152  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 221.564615  \n",
       "153  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 145.564615  \n",
       "154  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 213.564615  \n",
       "155  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 159.564615  \n",
       "156  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 198.564615  \n",
       "157  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 157.564615  \n",
       "158  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 212.564615  \n",
       "159  2023-10-22 00:00:00 2023-10-22 13:33:02.768043                 249.564615  \n",
       "160           2023-10-22 2023-10-22 13:33:02.768043                 313.564615  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add days between publication and today as a column to semantic_scholar_df\n",
    "\n",
    "# Convert publication date to datetime object\n",
    "semantic_scholar_df[\"ss_publication_date\"] = pd.to_datetime(semantic_scholar_df[\"ss_publication_date\"])\n",
    "\n",
    "print(semantic_scholar_df[\"ss_publication_date\"])\n",
    "\n",
    "# Column for end_date of today\n",
    "semantic_scholar_df[\"end_date\"] = datetime.today()\n",
    "\n",
    "# Calculate days between publication and end_date\n",
    "semantic_scholar_df[\"days_from_pub_to_end_date\"] = (semantic_scholar_df['end_date'] - semantic_scholar_df['ss_publication_date']) / np.timedelta64(1, 'D')\n",
    "\n",
    "semantic_scholar_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper title</th>\n",
       "      <th>semantic scholar title</th>\n",
       "      <th>ss_publication_date</th>\n",
       "      <th>ss_year</th>\n",
       "      <th>citation_count</th>\n",
       "      <th>query</th>\n",
       "      <th>day_queried</th>\n",
       "      <th>end_date</th>\n",
       "      <th>days_from_pub_to_end_date</th>\n",
       "      <th>citations_per_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT</td>\n",
       "      <td>A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT</td>\n",
       "      <td>2023-02-21</td>\n",
       "      <td>2023</td>\n",
       "      <td>163</td>\n",
       "      <td>A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>243.564615</td>\n",
       "      <td>0.669227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Taxonomy of Prompt Modifiers for Text-To-Image Generation</td>\n",
       "      <td>A Taxonomy of Prompt Modifiers for Text-To-Image Generation</td>\n",
       "      <td>2022-04-20</td>\n",
       "      <td>2022</td>\n",
       "      <td>43</td>\n",
       "      <td>A Taxonomy of Prompt Modifiers for Text To Image Generation</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>550.564615</td>\n",
       "      <td>0.078102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts</td>\n",
       "      <td>AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts</td>\n",
       "      <td>2021-10-04</td>\n",
       "      <td>2021</td>\n",
       "      <td>146</td>\n",
       "      <td>AI Chains: Transparent and Controllable Human AI Interaction by Chaining Large Language Model Prompts</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>748.564615</td>\n",
       "      <td>0.195040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ART: Automatic multi-step reasoning and tool-use for large language models</td>\n",
       "      <td>ART: Automatic multi-step reasoning and tool-use for large language models</td>\n",
       "      <td>2023-03-16</td>\n",
       "      <td>2023</td>\n",
       "      <td>39</td>\n",
       "      <td>ART: Automatic multi step reasoning and tool use for large language models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>220.564615</td>\n",
       "      <td>0.176819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Active Prompting with Chain-of-Thought for Large Language Models</td>\n",
       "      <td>Active Prompting with Chain-of-Thought for Large Language Models</td>\n",
       "      <td>2023-02-23</td>\n",
       "      <td>2023</td>\n",
       "      <td>34</td>\n",
       "      <td>Active Prompting with Chain of Thought for Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>241.564615</td>\n",
       "      <td>0.140749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>An automatically discovered chain-of-thought prompt generalizes to novel models and datasets</td>\n",
       "      <td>An automatically discovered chain-of-thought prompt generalizes to novel models and datasets</td>\n",
       "      <td>2023-05-04</td>\n",
       "      <td>2023</td>\n",
       "      <td>6</td>\n",
       "      <td>An automatically discovered chain of thought prompt generalizes to novel models and datasets</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>171.564615</td>\n",
       "      <td>0.034972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ask Me Anything: A simple strategy for prompting language models</td>\n",
       "      <td>Ask Me Anything: A simple strategy for prompting language models</td>\n",
       "      <td>2022-10-05</td>\n",
       "      <td>2022</td>\n",
       "      <td>65</td>\n",
       "      <td>Ask Me Anything: A simple strategy for prompting language models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>382.564615</td>\n",
       "      <td>0.169906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Atlas: Few-shot Learning with Retrieval Augmented Language Models</td>\n",
       "      <td>Few-shot Learning with Retrieval Augmented Language Models</td>\n",
       "      <td>2022-08-05</td>\n",
       "      <td>2022</td>\n",
       "      <td>202</td>\n",
       "      <td>Atlas: Few shot Learning with Retrieval Augmented Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>443.564615</td>\n",
       "      <td>0.455402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts</td>\n",
       "      <td>Eliciting Knowledge from Language Models Using Automatically Generated Prompts</td>\n",
       "      <td>2020-10-29</td>\n",
       "      <td>2020</td>\n",
       "      <td>217</td>\n",
       "      <td>AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>1088.564615</td>\n",
       "      <td>0.199345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Automatic Chain of Thought Prompting in Large Language Models</td>\n",
       "      <td>Automatic Chain of Thought Prompting in Large Language Models</td>\n",
       "      <td>2022-10-07</td>\n",
       "      <td>2022</td>\n",
       "      <td>201</td>\n",
       "      <td>Automatic Chain of Thought Prompting in Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>380.564615</td>\n",
       "      <td>0.528163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data</td>\n",
       "      <td>Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data</td>\n",
       "      <td>2023-02-24</td>\n",
       "      <td>2023</td>\n",
       "      <td>21</td>\n",
       "      <td>Automatic Prompt Augmentation and Selection with Chain of Thought from Labeled Data</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>240.564615</td>\n",
       "      <td>0.087295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BERTese: Learning to Speak to BERT</td>\n",
       "      <td>BERTese: Learning to Speak to BERT</td>\n",
       "      <td>2021-03-09</td>\n",
       "      <td>2021</td>\n",
       "      <td>74</td>\n",
       "      <td>BERTese: Learning to Speak to BERT</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>957.564615</td>\n",
       "      <td>0.077279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Batch Prompting: Efficient Inference with LLM APIs</td>\n",
       "      <td>Batch Prompting: Efficient Inference with Large Language Model APIs</td>\n",
       "      <td>2023-01-19</td>\n",
       "      <td>2023</td>\n",
       "      <td>10</td>\n",
       "      <td>Batch Prompting: Efficient Inference with LLM APIs</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>276.564615</td>\n",
       "      <td>0.036158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Better Zero-Shot Reasoning with Self-Adaptive Prompting</td>\n",
       "      <td>Better Zero-Shot Reasoning with Self-Adaptive Prompting</td>\n",
       "      <td>2023-05-23</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>Better Zero Shot Reasoning with Self Adaptive Prompting</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>152.564615</td>\n",
       "      <td>0.052437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Boosted Prompt Ensembles for Large Language Models</td>\n",
       "      <td>Boosted Prompt Ensembles for Large Language Models</td>\n",
       "      <td>2023-04-12</td>\n",
       "      <td>2023</td>\n",
       "      <td>12</td>\n",
       "      <td>Boosted Prompt Ensembles for Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>193.564615</td>\n",
       "      <td>0.061995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints</td>\n",
       "      <td>Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints</td>\n",
       "      <td>2023-02-17</td>\n",
       "      <td>2023</td>\n",
       "      <td>6</td>\n",
       "      <td>Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>247.564615</td>\n",
       "      <td>0.024236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale Language Model Society</td>\n",
       "      <td>CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale Language Model Society</td>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>2023</td>\n",
       "      <td>64</td>\n",
       "      <td>CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale Language Model Society</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>205.564615</td>\n",
       "      <td>0.311338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Calibrate Before Use: Improving Few-Shot Performance of Language Models</td>\n",
       "      <td>Calibrate Before Use: Improving Few-Shot Performance of Language Models</td>\n",
       "      <td>2021-02-19</td>\n",
       "      <td>2021</td>\n",
       "      <td>616</td>\n",
       "      <td>Calibrate Before Use: Improving Few Shot Performance of Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>975.564615</td>\n",
       "      <td>0.631429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT</td>\n",
       "      <td>Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT</td>\n",
       "      <td>2023-02-19</td>\n",
       "      <td>2023</td>\n",
       "      <td>73</td>\n",
       "      <td>Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine tuned BERT</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>245.564615</td>\n",
       "      <td>0.297274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Can We Edit Factual Knowledge by In-Context Learning?</td>\n",
       "      <td>Can We Edit Factual Knowledge by In-Context Learning?</td>\n",
       "      <td>2023-05-22</td>\n",
       "      <td>2023</td>\n",
       "      <td>11</td>\n",
       "      <td>Can We Edit Factual Knowledge by In Context Learning?</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>153.564615</td>\n",
       "      <td>0.071631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Chain of Hindsight Aligns Language Models with Feedback</td>\n",
       "      <td>Chain of Hindsight Aligns Language Models with Feedback</td>\n",
       "      <td>2023-02-06</td>\n",
       "      <td>2023</td>\n",
       "      <td>37</td>\n",
       "      <td>Chain of Hindsight Aligns Language Models with Feedback</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>258.564615</td>\n",
       "      <td>0.143098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Chain of Thought Prompting Elicits Reasoning in Large Language Models</td>\n",
       "      <td>Chain of Thought Prompting Elicits Reasoning in Large Language Models</td>\n",
       "      <td>2022-01-28</td>\n",
       "      <td>2022</td>\n",
       "      <td>2105</td>\n",
       "      <td>Chain of Thought Prompting Elicits Reasoning in Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>632.564615</td>\n",
       "      <td>3.327723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models</td>\n",
       "      <td>Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>Chain of Symbol Prompting Elicits Planning in Large Langauge Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Chain-of-Verification Reduces Hallucination in Large Language Models</td>\n",
       "      <td>Chain-of-Verification Reduces Hallucination in Large Language Models</td>\n",
       "      <td>2023-09-20</td>\n",
       "      <td>2023</td>\n",
       "      <td>10</td>\n",
       "      <td>Chain of Verification Reduces Hallucination in Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>32.564615</td>\n",
       "      <td>0.307082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer</td>\n",
       "      <td>ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer</td>\n",
       "      <td>2023-06-13</td>\n",
       "      <td>2023</td>\n",
       "      <td>12</td>\n",
       "      <td>ChatGPT vs Human authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>131.564615</td>\n",
       "      <td>0.091210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification</td>\n",
       "      <td>CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification</td>\n",
       "      <td>2023-03-07</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>229.564615</td>\n",
       "      <td>0.013068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Complexity-Based Prompting for Multi-Step Reasoning</td>\n",
       "      <td>Complexity-Based Prompting for Multi-Step Reasoning</td>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>2022</td>\n",
       "      <td>116</td>\n",
       "      <td>Complexity Based Prompting for Multi Step Reasoning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>384.564615</td>\n",
       "      <td>0.301640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Compositional Exemplars for In-context Learning</td>\n",
       "      <td>Compositional Exemplars for In-context Learning</td>\n",
       "      <td>2023-02-11</td>\n",
       "      <td>2023</td>\n",
       "      <td>24</td>\n",
       "      <td>Compositional Exemplars for In context Learning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>253.564615</td>\n",
       "      <td>0.094650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt</td>\n",
       "      <td>Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt</td>\n",
       "      <td>2023-05-17</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>Compress, Then Prompt: Improving Accuracy Efficiency Trade off of LLM Inference with Transferable Prompt</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>158.564615</td>\n",
       "      <td>0.018920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers</td>\n",
       "      <td>Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers</td>\n",
       "      <td>2023-09-15</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>37.564615</td>\n",
       "      <td>0.212966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Constitutional AI: Harmlessness from AI Feedback</td>\n",
       "      <td>Constitutional AI: Harmlessness from AI Feedback</td>\n",
       "      <td>2022-12-15</td>\n",
       "      <td>2022</td>\n",
       "      <td>307</td>\n",
       "      <td>Constitutional AI: Harmlessness from AI Feedback</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>311.564615</td>\n",
       "      <td>0.985349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Context-faithful Prompting for Large Language Models</td>\n",
       "      <td>Context-faithful Prompting for Large Language Models</td>\n",
       "      <td>2023-03-20</td>\n",
       "      <td>2023</td>\n",
       "      <td>21</td>\n",
       "      <td>Context faithful Prompting for Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>216.564615</td>\n",
       "      <td>0.096969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>CooK: Empowering General-Purpose Language Models with Modular and Collaborative Knowledge</td>\n",
       "      <td>SHARED OPEN VOCABULARIES AND SEMANTIC MEDIA</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>CooK: Empowering General Purpose Language Models with Modular and Collaborative Knowledge</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Decomposed Prompting: A Modular Approach for Solving Complex Tasks</td>\n",
       "      <td>Decomposed Prompting: A Modular Approach for Solving Complex Tasks</td>\n",
       "      <td>2022-10-05</td>\n",
       "      <td>2022</td>\n",
       "      <td>104</td>\n",
       "      <td>Decomposed Prompting: A Modular Approach for Solving Complex Tasks</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>382.564615</td>\n",
       "      <td>0.271850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP</td>\n",
       "      <td>Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP</td>\n",
       "      <td>2022-12-28</td>\n",
       "      <td>2022</td>\n",
       "      <td>57</td>\n",
       "      <td>Demonstrate Search Predict: Composing retrieval and language models for knowledge intensive NLP</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>298.564615</td>\n",
       "      <td>0.190913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Design Guidelines for Prompt Engineering Text-to-Image Generative Models</td>\n",
       "      <td>Design Guidelines for Prompt Engineering Text-to-Image Generative Models</td>\n",
       "      <td>2021-09-14</td>\n",
       "      <td>2021</td>\n",
       "      <td>142</td>\n",
       "      <td>Design Guidelines for Prompt Engineering Text to Image Generative Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>768.564615</td>\n",
       "      <td>0.184760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Discovering Language Model Behaviors with Model-Written Evaluations</td>\n",
       "      <td>Discovering Language Model Behaviors with Model-Written Evaluations</td>\n",
       "      <td>2022-12-19</td>\n",
       "      <td>2022</td>\n",
       "      <td>87</td>\n",
       "      <td>Discovering Language Model Behaviors with Model Written Evaluations</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>307.564615</td>\n",
       "      <td>0.282867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>DocPrompting: Generating Code by Retrieving the Docs</td>\n",
       "      <td>DocPrompting: Generating Code by Retrieving the Docs</td>\n",
       "      <td>2022-07-13</td>\n",
       "      <td>2022</td>\n",
       "      <td>27</td>\n",
       "      <td>DocPrompting: Generating Code by Retrieving the Docs</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>466.564615</td>\n",
       "      <td>0.057870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning</td>\n",
       "      <td>Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning</td>\n",
       "      <td>2022-09-29</td>\n",
       "      <td>2022</td>\n",
       "      <td>51</td>\n",
       "      <td>Dynamic Prompt Learning via Policy Gradient for Semi structured Mathematical Reasoning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>388.564615</td>\n",
       "      <td>0.131252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Dynamic Prompting: A Unified Framework for Prompt Tuning</td>\n",
       "      <td>Dynamic Prompting: A Unified Framework for Prompt Tuning</td>\n",
       "      <td>2023-03-06</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>Dynamic Prompting: A Unified Framework for Prompt Tuning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>230.564615</td>\n",
       "      <td>0.017349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Effectiveness of Data Augmentation for Prefix Tuning with Limited Data</td>\n",
       "      <td>Effectiveness of Data Augmentation for Prefix Tuning with Limited Data</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>Effectiveness of Data Augmentation for Prefix Tuning with Limited Data</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Efficient Prompting via Dynamic In-Context Learning</td>\n",
       "      <td>Efficient Prompting via Dynamic In-Context Learning</td>\n",
       "      <td>2023-05-18</td>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>Efficient Prompting via Dynamic In Context Learning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>157.564615</td>\n",
       "      <td>0.031733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference</td>\n",
       "      <td>Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference</td>\n",
       "      <td>2022-11-21</td>\n",
       "      <td>2022</td>\n",
       "      <td>16</td>\n",
       "      <td>Enhancing Self Consistency and Performance of Pre Trained Language Models through Natural Language Inference</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>335.564615</td>\n",
       "      <td>0.047681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Evaluating Large Language Models Trained on Code</td>\n",
       "      <td>Evaluating Large Language Models Trained on Code</td>\n",
       "      <td>2021-07-07</td>\n",
       "      <td>2021</td>\n",
       "      <td>1617</td>\n",
       "      <td>Evaluating Large Language Models Trained on Code</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>837.564615</td>\n",
       "      <td>1.930597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Evaluating the Robustness of Discrete Prompts</td>\n",
       "      <td>Evaluating the Robustness of Discrete Prompts</td>\n",
       "      <td>2023-02-11</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>Evaluating the Robustness of Discrete Prompts</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>253.564615</td>\n",
       "      <td>0.015775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples</td>\n",
       "      <td>Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples</td>\n",
       "      <td>2022-09-05</td>\n",
       "      <td>2022</td>\n",
       "      <td>12</td>\n",
       "      <td>Evaluating the Susceptibility of Pre Trained Language Models via Handcrafted Adversarial Examples</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>412.564615</td>\n",
       "      <td>0.029086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>EvoPrompting: Language Models for Code-Level Neural Architecture Search</td>\n",
       "      <td>EvoPrompting: Language Models for Code-Level Neural Architecture Search</td>\n",
       "      <td>2023-02-28</td>\n",
       "      <td>2023</td>\n",
       "      <td>12</td>\n",
       "      <td>EvoPrompting: Language Models for Code Level Neural Architecture Search</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>236.564615</td>\n",
       "      <td>0.050726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Exploring Lottery Prompts for Pre-trained Language Models</td>\n",
       "      <td>Exploring Lottery Prompts for Pre-trained Language Models</td>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>Exploring Lottery Prompts for Pre trained Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>144.564615</td>\n",
       "      <td>0.006917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Fairness-guided Few-shot Prompting for Large Language Models</td>\n",
       "      <td>Fairness-guided Few-shot Prompting for Large Language Models</td>\n",
       "      <td>2023-03-23</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>Fairness guided Few shot Prompting for Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>213.564615</td>\n",
       "      <td>0.009365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity</td>\n",
       "      <td>Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity</td>\n",
       "      <td>2021-04-18</td>\n",
       "      <td>2021</td>\n",
       "      <td>408</td>\n",
       "      <td>Fantastically Ordered Prompts and Where to Find Them: Overcoming Few Shot Prompt Order Sensitivity</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>917.564615</td>\n",
       "      <td>0.444655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency</td>\n",
       "      <td>Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency</td>\n",
       "      <td>2023-05-18</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>Flatness Aware Prompt Selection Improves Accuracy and Sample Efficiency</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>157.564615</td>\n",
       "      <td>0.006347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Flesch or Fumble? Evaluating Readability Standard Alignment of Instruction-Tuned Language Models</td>\n",
       "      <td>Flesch or Fumble? Evaluating Readability Standard Alignment of Instruction-Tuned Language Models</td>\n",
       "      <td>2023-09-11</td>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>Flesch or Fumble? Evaluating Readability Standard Alignment of Instruction Tuned Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>41.564615</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Focused Prefix Tuning for Controllable Text Generation</td>\n",
       "      <td>Focused Prefix Tuning for Controllable Text Generation</td>\n",
       "      <td>2023-06-01</td>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>Focused Prefix Tuning for Controllable Text Generation</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>143.564615</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting</td>\n",
       "      <td>From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting</td>\n",
       "      <td>2023-09-08</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>From Sparse to Dense: GPT 4 Summarization with Chain of Density Prompting</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>44.564615</td>\n",
       "      <td>0.044879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>GPT-4 Technical Report</td>\n",
       "      <td>GPT-4 Technical Report</td>\n",
       "      <td>2023-03-15</td>\n",
       "      <td>2023</td>\n",
       "      <td>1529</td>\n",
       "      <td>GPT 4 Technical Report</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>221.564615</td>\n",
       "      <td>6.900921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Generated Knowledge Prompting for Commonsense Reasoning</td>\n",
       "      <td>Generated Knowledge Prompting for Commonsense Reasoning</td>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>2021</td>\n",
       "      <td>114</td>\n",
       "      <td>Generated Knowledge Prompting for Commonsense Reasoning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>737.564615</td>\n",
       "      <td>0.154563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Global Prompt Cell: A Portable Control Module for Effective Prompt</td>\n",
       "      <td>Global Prompt Cell: A Portable Control Module for Effective Prompt</td>\n",
       "      <td>2023-04-12</td>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>Global Prompt Cell: A Portable Control Module for Effective Prompt</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>193.564615</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Graph of Thoughts: Solving Elaborate Problems with Large Language Models</td>\n",
       "      <td>Graph of Thoughts: Solving Elaborate Problems with Large Language Models</td>\n",
       "      <td>2023-08-18</td>\n",
       "      <td>2023</td>\n",
       "      <td>35</td>\n",
       "      <td>Graph of Thoughts: Solving Elaborate Problems with Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>65.564615</td>\n",
       "      <td>0.533825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks</td>\n",
       "      <td>GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks</td>\n",
       "      <td>2023-02-16</td>\n",
       "      <td>2023</td>\n",
       "      <td>16</td>\n",
       "      <td>GraphPrompt: Unifying Pre Training and Downstream Tasks for Graph Neural Networks</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>248.564615</td>\n",
       "      <td>0.064370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Guiding Large Language Models via Directional Stimulus Prompting</td>\n",
       "      <td>Guiding Large Language Models via Directional Stimulus Prompting</td>\n",
       "      <td>2023-02-22</td>\n",
       "      <td>2023</td>\n",
       "      <td>10</td>\n",
       "      <td>Guiding Large Language Models via Directional Stimulus Prompting</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>242.564615</td>\n",
       "      <td>0.041226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery</td>\n",
       "      <td>Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery</td>\n",
       "      <td>2023-02-07</td>\n",
       "      <td>2023</td>\n",
       "      <td>38</td>\n",
       "      <td>Hard Prompts Made Easy: Gradient Based Discrete Optimization for Prompt Tuning and Discovery</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>257.564615</td>\n",
       "      <td>0.147536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Hierarchical Prompting Assists Large Language Model on Web Navigation</td>\n",
       "      <td>Hierarchical Prompting Assists Large Language Model on Web Navigation</td>\n",
       "      <td>2023-05-23</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>Hierarchical Prompting Assists Large Language Model on Web Navigation</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>152.564615</td>\n",
       "      <td>0.019664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>How Can We Know What Language Models Know?</td>\n",
       "      <td>How Can We Know What Language Models Know?</td>\n",
       "      <td>2019-11-28</td>\n",
       "      <td>2019</td>\n",
       "      <td>763</td>\n",
       "      <td>How Can We Know What Language Models Know?</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>1424.564615</td>\n",
       "      <td>0.535602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>How Does In-Context Learning Help Prompt Tuning?</td>\n",
       "      <td>How Does In-Context Learning Help Prompt Tuning?</td>\n",
       "      <td>2023-02-22</td>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>How Does In Context Learning Help Prompt Tuning?</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>242.564615</td>\n",
       "      <td>0.020613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks</td>\n",
       "      <td>How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>2023</td>\n",
       "      <td>24</td>\n",
       "      <td>How Robust is GPT 3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>235.564615</td>\n",
       "      <td>0.101883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Ignore Previous Prompt: Attack Techniques For Language Models</td>\n",
       "      <td>Ignore Previous Prompt: Attack Techniques For Language Models</td>\n",
       "      <td>2022-11-17</td>\n",
       "      <td>2022</td>\n",
       "      <td>68</td>\n",
       "      <td>Ignore Previous Prompt: Attack Techniques For Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>339.564615</td>\n",
       "      <td>0.200256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>In-Context Instruction Learning</td>\n",
       "      <td>In-Context Instruction Learning</td>\n",
       "      <td>2023-02-28</td>\n",
       "      <td>2023</td>\n",
       "      <td>19</td>\n",
       "      <td>In Context Instruction Learning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>236.564615</td>\n",
       "      <td>0.080316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>In-Context Learning of Large Language Models Explained as Kernel Regression</td>\n",
       "      <td>In-Context Learning of Large Language Models Explained as Kernel Regression</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>In Context Learning of Large Language Models Explained as Kernel Regression</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Interactive Natural Language Processing</td>\n",
       "      <td>Interactive Natural Language Processing</td>\n",
       "      <td>2023-05-22</td>\n",
       "      <td>2023</td>\n",
       "      <td>14</td>\n",
       "      <td>Interactive Natural Language Processing</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>153.564615</td>\n",
       "      <td>0.091167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Is GPT-4 a Good Data Analyst?</td>\n",
       "      <td>Is GPT-4 a Good Data Analyst?</td>\n",
       "      <td>2023-05-24</td>\n",
       "      <td>2023</td>\n",
       "      <td>17</td>\n",
       "      <td>Is GPT 4 a Good Data Analyst?</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>151.564615</td>\n",
       "      <td>0.112163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning</td>\n",
       "      <td>Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning</td>\n",
       "      <td>2023-03-18</td>\n",
       "      <td>2023</td>\n",
       "      <td>16</td>\n",
       "      <td>Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>218.564615</td>\n",
       "      <td>0.073205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Language Is Not All You Need: Aligning Perception with Language Models</td>\n",
       "      <td>Language Is Not All You Need: Aligning Perception with Language Models</td>\n",
       "      <td>2023-02-27</td>\n",
       "      <td>2023</td>\n",
       "      <td>154</td>\n",
       "      <td>Language Is Not All You Need: Aligning Perception with Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>237.564615</td>\n",
       "      <td>0.648245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought</td>\n",
       "      <td>Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought</td>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>2022</td>\n",
       "      <td>58</td>\n",
       "      <td>Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain of Thought</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>384.564615</td>\n",
       "      <td>0.150820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Language Models are Few-Shot Learners</td>\n",
       "      <td>Language Models are Few-Shot Learners</td>\n",
       "      <td>2020-05-28</td>\n",
       "      <td>2020</td>\n",
       "      <td>16440</td>\n",
       "      <td>Language Models are Few Shot Learners</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>1242.564615</td>\n",
       "      <td>13.230700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Large Language Model Guided Tree-of-Thought</td>\n",
       "      <td>Large Language Model Guided Tree-of-Thought</td>\n",
       "      <td>2023-05-15</td>\n",
       "      <td>2023</td>\n",
       "      <td>26</td>\n",
       "      <td>Large Language Model Guided Tree of Thought</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>160.564615</td>\n",
       "      <td>0.161929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Large Language Models Are Human-Level Prompt Engineers</td>\n",
       "      <td>Large Language Models Are Human-Level Prompt Engineers</td>\n",
       "      <td>2022-11-03</td>\n",
       "      <td>2022</td>\n",
       "      <td>194</td>\n",
       "      <td>Large Language Models Are Human Level Prompt Engineers</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>353.564615</td>\n",
       "      <td>0.548697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Large Language Models Can Be Easily Distracted by Irrelevant Context</td>\n",
       "      <td>Large Language Models Can Be Easily Distracted by Irrelevant Context</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>2023</td>\n",
       "      <td>70</td>\n",
       "      <td>Large Language Models Can Be Easily Distracted by Irrelevant Context</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>264.564615</td>\n",
       "      <td>0.264586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Large Language Models are Zero-Shot Reasoners</td>\n",
       "      <td>Large Language Models are Zero-Shot Reasoners</td>\n",
       "      <td>2022-05-24</td>\n",
       "      <td>2022</td>\n",
       "      <td>882</td>\n",
       "      <td>Large Language Models are Zero Shot Reasoners</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>516.564615</td>\n",
       "      <td>1.707434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Large Language Models are reasoners with Self-Verification</td>\n",
       "      <td>Large Language Models are reasoners with Self-Verification</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2022</td>\n",
       "      <td>31</td>\n",
       "      <td>Large Language Models are reasoners with Self Verification</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Larger language models do in-context learning differently</td>\n",
       "      <td>Larger language models do in-context learning differently</td>\n",
       "      <td>2023-03-07</td>\n",
       "      <td>2023</td>\n",
       "      <td>87</td>\n",
       "      <td>Larger language models do in context learning differently</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>229.564615</td>\n",
       "      <td>0.378978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Learning from Task Descriptions</td>\n",
       "      <td>Learning from Task Descriptions</td>\n",
       "      <td>2020-11-01</td>\n",
       "      <td>2020</td>\n",
       "      <td>64</td>\n",
       "      <td>Learning from Task Descriptions</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>1085.564615</td>\n",
       "      <td>0.058955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Learning to Generate Task-Specific Adapters from Task Description</td>\n",
       "      <td>Learning to Generate Task-Specific Adapters from Task Description</td>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>2021</td>\n",
       "      <td>18</td>\n",
       "      <td>Learning to Generate Task Specific Adapters from Task Description</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>1023.564615</td>\n",
       "      <td>0.017586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Learning to Transfer Prompts for Text Generation</td>\n",
       "      <td>Learning to Transfer Prompts for Text Generation</td>\n",
       "      <td>2022-05-03</td>\n",
       "      <td>2022</td>\n",
       "      <td>18</td>\n",
       "      <td>Learning to Transfer Prompts for Text Generation</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>537.564615</td>\n",
       "      <td>0.033484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Least-to-most promptingenables complex reasoning in large language models</td>\n",
       "      <td>Least-to-Most Prompting Enables Complex Reasoning in Large Language Models</td>\n",
       "      <td>2022-05-21</td>\n",
       "      <td>2022</td>\n",
       "      <td>369</td>\n",
       "      <td>Least to most promptingenables complex reasoning in large language models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>519.564615</td>\n",
       "      <td>0.710210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses</td>\n",
       "      <td>Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses</td>\n",
       "      <td>2023-05-30</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>145.564615</td>\n",
       "      <td>0.006870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning with LLMs</td>\n",
       "      <td>Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning with LLMs</td>\n",
       "      <td>2023-05-19</td>\n",
       "      <td>2023</td>\n",
       "      <td>6</td>\n",
       "      <td>Let's Sample Step by Step: Adaptive Consistency for Efficient Reasoning with LLMs</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>156.564615</td>\n",
       "      <td>0.038323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Let's Verify Step by Step</td>\n",
       "      <td>Let's Verify Step by Step</td>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>2023</td>\n",
       "      <td>64</td>\n",
       "      <td>Let's Verify Step by Step</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>144.564615</td>\n",
       "      <td>0.442709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning</td>\n",
       "      <td>MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning</td>\n",
       "      <td>2022-05-01</td>\n",
       "      <td>2022</td>\n",
       "      <td>31</td>\n",
       "      <td>MRKL Systems: A modular, neuro symbolic architecture that combines large language models, external knowledge sources and discrete reasoning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>539.564615</td>\n",
       "      <td>0.057454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods</td>\n",
       "      <td>Machine-Generated Text: A Comprehensive Survey of Threat Models and Detection Methods</td>\n",
       "      <td>2022-10-13</td>\n",
       "      <td>2022</td>\n",
       "      <td>28</td>\n",
       "      <td>Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>374.564615</td>\n",
       "      <td>0.074753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations</td>\n",
       "      <td>Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations</td>\n",
       "      <td>2022-05-24</td>\n",
       "      <td>2022</td>\n",
       "      <td>86</td>\n",
       "      <td>Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>516.564615</td>\n",
       "      <td>0.166484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Making Pre-trained Language Models Better Few-shot Learners</td>\n",
       "      <td>Making Pre-trained Language Models Better Few-shot Learners</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>2021</td>\n",
       "      <td>1082</td>\n",
       "      <td>Making Pre trained Language Models Better Few shot Learners</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>1024.564615</td>\n",
       "      <td>1.056058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Meta-in-context learning in large language models</td>\n",
       "      <td>Meta-in-context learning in large language models</td>\n",
       "      <td>2023-05-22</td>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>Meta in context learning in large language models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>153.564615</td>\n",
       "      <td>0.032560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Mixture of Soft Prompts for Controllable Data Generation</td>\n",
       "      <td>Mixture of Soft Prompts for Controllable Data Generation</td>\n",
       "      <td>2023-03-02</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>Mixture of Soft Prompts for Controllable Data Generation</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>234.564615</td>\n",
       "      <td>0.012790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Model-tuning Via Prompts Makes NLP Models Adversarially Robust</td>\n",
       "      <td>Model-tuning Via Prompts Makes NLP Models Adversarially Robust</td>\n",
       "      <td>2023-03-13</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>Model tuning Via Prompts Makes NLP Models Adversarially Robust</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>223.564615</td>\n",
       "      <td>0.013419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models</td>\n",
       "      <td>More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2023</td>\n",
       "      <td>37</td>\n",
       "      <td>More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application Integrated Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of Thought Prompting</td>\n",
       "      <td>MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of Thought Prompting</td>\n",
       "      <td>2023-05-26</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>MultiTool CoT: GPT 3 Can Use Multiple External Tools with Chain of Thought Prompting</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>149.564615</td>\n",
       "      <td>0.006686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Multimodal Chain-of-Thought Reasoning in Language Models</td>\n",
       "      <td>Multimodal Chain-of-Thought Reasoning in Language Models</td>\n",
       "      <td>2023-02-02</td>\n",
       "      <td>2023</td>\n",
       "      <td>91</td>\n",
       "      <td>Multimodal Chain of Thought Reasoning in Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>262.564615</td>\n",
       "      <td>0.346581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning</td>\n",
       "      <td>Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning</td>\n",
       "      <td>2023-03-06</td>\n",
       "      <td>2023</td>\n",
       "      <td>21</td>\n",
       "      <td>Multitask Prompt Tuning Enables Parameter Efficient Transfer Learning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>230.564615</td>\n",
       "      <td>0.091081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Multitask Prompted Training Enables Zero-Shot Task Generalization</td>\n",
       "      <td>Multitask Prompted Training Enables Zero-Shot Task Generalization</td>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>2021</td>\n",
       "      <td>924</td>\n",
       "      <td>Multitask Prompted Training Enables Zero Shot Task Generalization</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>737.564615</td>\n",
       "      <td>1.252772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning</td>\n",
       "      <td>On Second Thought, Lets Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning</td>\n",
       "      <td>2022-12-15</td>\n",
       "      <td>2022</td>\n",
       "      <td>23</td>\n",
       "      <td>On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero Shot Reasoning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>311.564615</td>\n",
       "      <td>0.073821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>On the Advance of Making Language Models Better Reasoners</td>\n",
       "      <td>On the Advance of Making Language Models Better Reasoners</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2022</td>\n",
       "      <td>106</td>\n",
       "      <td>On the Advance of Making Language Models Better Reasoners</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>OpenICL: An Open-Source Framework for In-context Learning</td>\n",
       "      <td>OpenICL: An Open-Source Framework for In-context Learning</td>\n",
       "      <td>2023-03-06</td>\n",
       "      <td>2023</td>\n",
       "      <td>12</td>\n",
       "      <td>OpenICL: An Open Source Framework for In context Learning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>230.564615</td>\n",
       "      <td>0.052046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>PAL: Program-aided Language Models</td>\n",
       "      <td>PAL: Program-aided Language Models</td>\n",
       "      <td>2022-11-18</td>\n",
       "      <td>2022</td>\n",
       "      <td>198</td>\n",
       "      <td>PAL: Program aided Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>338.564615</td>\n",
       "      <td>0.584822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents</td>\n",
       "      <td>PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents</td>\n",
       "      <td>2023-05-23</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>152.564615</td>\n",
       "      <td>0.026218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>PPT: Pre-trained Prompt Tuning for Few-shot Learning</td>\n",
       "      <td>PPT: Pre-trained Prompt Tuning for Few-shot Learning</td>\n",
       "      <td>2021-09-09</td>\n",
       "      <td>2021</td>\n",
       "      <td>234</td>\n",
       "      <td>PPT: Pre trained Prompt Tuning for Few shot Learning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>773.564615</td>\n",
       "      <td>0.302496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models</td>\n",
       "      <td>Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models</td>\n",
       "      <td>2023-05-06</td>\n",
       "      <td>2023</td>\n",
       "      <td>46</td>\n",
       "      <td>Plan and Solve Prompting: Improving Zero Shot Chain of Thought Reasoning by Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>169.564615</td>\n",
       "      <td>0.271283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Post Hoc Explanations of Language Models Can Improve Language Models</td>\n",
       "      <td>Post Hoc Explanations of Language Models Can Improve Language Models</td>\n",
       "      <td>2023-05-19</td>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>Post Hoc Explanations of Language Models Can Improve Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>156.564615</td>\n",
       "      <td>0.031936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>Pre-Training to Learn in Context</td>\n",
       "      <td>Pre-Training to Learn in Context</td>\n",
       "      <td>2023-05-16</td>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>Pre Training to Learn in Context</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>159.564615</td>\n",
       "      <td>0.031335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Prefix-Tuning: Optimizing Continuous Prompts for Generation</td>\n",
       "      <td>Prefix-Tuning: Optimizing Continuous Prompts for Generation</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2021</td>\n",
       "      <td>1682</td>\n",
       "      <td>Prefix Tuning: Optimizing Continuous Prompts for Generation</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Progressive Prompts: Continual Learning for Language Models</td>\n",
       "      <td>Progressive Prompts: Continual Learning for Language Models</td>\n",
       "      <td>2023-01-29</td>\n",
       "      <td>2023</td>\n",
       "      <td>20</td>\n",
       "      <td>Progressive Prompts: Continual Learning for Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>266.564615</td>\n",
       "      <td>0.075029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Prompt Engineering for Large Language Models</td>\n",
       "      <td>Prompt Engineering for Large Language Models</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>Prompt Engineering for Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm</td>\n",
       "      <td>Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm</td>\n",
       "      <td>2021-02-15</td>\n",
       "      <td>2021</td>\n",
       "      <td>317</td>\n",
       "      <td>Prompt Programming for Large Language Models: Beyond the Few Shot Paradigm</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>979.564615</td>\n",
       "      <td>0.323613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners</td>\n",
       "      <td>Prompt, Generate, Then Cache: Cascade of Foundation Models Makes Strong Few-Shot Learners</td>\n",
       "      <td>2023-03-03</td>\n",
       "      <td>2023</td>\n",
       "      <td>30</td>\n",
       "      <td>Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few shot Learners</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>233.564615</td>\n",
       "      <td>0.128444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>PromptChainer: Chaining Large Language Model Prompts through Visual Programming</td>\n",
       "      <td>PromptChainer: Chaining Large Language Model Prompts through Visual Programming</td>\n",
       "      <td>2022-03-13</td>\n",
       "      <td>2022</td>\n",
       "      <td>78</td>\n",
       "      <td>PromptChainer: Chaining Large Language Model Prompts through Visual Programming</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>588.564615</td>\n",
       "      <td>0.132526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Promptagator: Few-shot Dense Retrieval From 8 Examples</td>\n",
       "      <td>Promptagator: Few-shot Dense Retrieval From 8 Examples</td>\n",
       "      <td>2022-09-23</td>\n",
       "      <td>2022</td>\n",
       "      <td>66</td>\n",
       "      <td>Promptagator: Few shot Dense Retrieval From 8 Examples</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>394.564615</td>\n",
       "      <td>0.167273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Prompted Opinion Summarization with GPT-3.5</td>\n",
       "      <td>Prompted Opinion Summarization with GPT-3.5</td>\n",
       "      <td>2022-11-29</td>\n",
       "      <td>2022</td>\n",
       "      <td>8</td>\n",
       "      <td>Prompted Opinion Summarization with GPT 3.5</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>327.564615</td>\n",
       "      <td>0.024423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Prompting GPT-3 To Be Reliable</td>\n",
       "      <td>Prompting GPT-3 To Be Reliable</td>\n",
       "      <td>2022-10-17</td>\n",
       "      <td>2022</td>\n",
       "      <td>72</td>\n",
       "      <td>Prompting GPT 3 To Be Reliable</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>370.564615</td>\n",
       "      <td>0.194298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL</td>\n",
       "      <td>Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL</td>\n",
       "      <td>2023-09-13</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>Query Dependent Prompt Evaluation and Optimization with Offline Inverse RL</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>39.564615</td>\n",
       "      <td>0.025275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>REFINER: Reasoning Feedback on Intermediate Representations</td>\n",
       "      <td>REFINER: Reasoning Feedback on Intermediate Representations</td>\n",
       "      <td>2023-04-04</td>\n",
       "      <td>2023</td>\n",
       "      <td>35</td>\n",
       "      <td>REFINER: Reasoning Feedback on Intermediate Representations</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>201.564615</td>\n",
       "      <td>0.173642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>Re-Reading Improves Reasoning in Language Models</td>\n",
       "      <td>Re-Reading Improves Reasoning in Language Models</td>\n",
       "      <td>2023-09-12</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>Re Reading Improves Reasoning in Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>40.564615</td>\n",
       "      <td>0.049304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>ReAct: Synergizing Reasoning and Acting in Language Models</td>\n",
       "      <td>ReAct: Synergizing Reasoning and Acting in Language Models</td>\n",
       "      <td>2022-10-06</td>\n",
       "      <td>2022</td>\n",
       "      <td>332</td>\n",
       "      <td>ReAct: Synergizing Reasoning and Acting in Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>381.564615</td>\n",
       "      <td>0.870102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Reasoning with Language Model is Planning with World Model</td>\n",
       "      <td>Reasoning with Language Model is Planning with World Model</td>\n",
       "      <td>2023-05-24</td>\n",
       "      <td>2023</td>\n",
       "      <td>34</td>\n",
       "      <td>Reasoning with Language Model is Planning with World Model</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>151.564615</td>\n",
       "      <td>0.224327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>Recitation-Augmented Language Models</td>\n",
       "      <td>Recitation-Augmented Language Models</td>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>2022</td>\n",
       "      <td>37</td>\n",
       "      <td>Recitation Augmented Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>383.564615</td>\n",
       "      <td>0.096464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Reflexion: an autonomous agent with dynamic memory and self-reflection</td>\n",
       "      <td>Reflexion: an autonomous agent with dynamic memory and self-reflection</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2023</td>\n",
       "      <td>105</td>\n",
       "      <td>Reflexion: an autonomous agent with dynamic memory and self reflection</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Reframing Instructional Prompts to GPTk's Language</td>\n",
       "      <td>Reframing Instructional Prompts to GPTks Language</td>\n",
       "      <td>2021-09-16</td>\n",
       "      <td>2021</td>\n",
       "      <td>121</td>\n",
       "      <td>Reframing Instructional Prompts to GPTk's Language</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>766.564615</td>\n",
       "      <td>0.157847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling</td>\n",
       "      <td>Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling</td>\n",
       "      <td>2023-05-17</td>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>Reprompting: Automated Chain of Thought Prompt Inference Through Gibbs Sampling</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>158.564615</td>\n",
       "      <td>0.031533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?</td>\n",
       "      <td>Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?</td>\n",
       "      <td>2022-02-25</td>\n",
       "      <td>2022</td>\n",
       "      <td>431</td>\n",
       "      <td>Rethinking the Role of Demonstrations: What Makes In Context Learning Work?</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>604.564615</td>\n",
       "      <td>0.712910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</td>\n",
       "      <td>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</td>\n",
       "      <td>2020-05-22</td>\n",
       "      <td>2020</td>\n",
       "      <td>956</td>\n",
       "      <td>Retrieval Augmented Generation for Knowledge Intensive NLP Tasks</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>1248.564615</td>\n",
       "      <td>0.765679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Revisiting Automated Prompting: Are We Actually Doing Better?</td>\n",
       "      <td>Revisiting Automated Prompting: Are We Actually Doing Better?</td>\n",
       "      <td>2023-04-07</td>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>Revisiting Automated Prompting: Are We Actually Doing Better?</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>198.564615</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Satisfiability-Aided Language Models Using Declarative Prompting</td>\n",
       "      <td>Satisfiability-Aided Language Models Using Declarative Prompting</td>\n",
       "      <td>2023-05-16</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>Satisfiability Aided Language Models Using Declarative Prompting</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>159.564615</td>\n",
       "      <td>0.018801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Scalable Prompt Generation for Semi-supervised Learning with Language Models</td>\n",
       "      <td>Scalable Prompt Generation for Semi-supervised Learning with Language Models</td>\n",
       "      <td>2023-02-18</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>Scalable Prompt Generation for Semi supervised Learning with Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>246.564615</td>\n",
       "      <td>0.004056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>Scaling Instruction-Finetuned Language Models</td>\n",
       "      <td>Scaling Instruction-Finetuned Language Models</td>\n",
       "      <td>2022-10-20</td>\n",
       "      <td>2022</td>\n",
       "      <td>823</td>\n",
       "      <td>Scaling Instruction Finetuned Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>367.564615</td>\n",
       "      <td>2.239062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>Scaling Laws for Neural Language Models</td>\n",
       "      <td>Scaling Laws for Neural Language Models</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>2020</td>\n",
       "      <td>1530</td>\n",
       "      <td>Scaling Laws for Neural Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>1368.564615</td>\n",
       "      <td>1.117960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Self-Consistency Improves Chain of Thought Reasoning in Language Models</td>\n",
       "      <td>Self-Consistency Improves Chain of Thought Reasoning in Language Models</td>\n",
       "      <td>2022-03-21</td>\n",
       "      <td>2022</td>\n",
       "      <td>659</td>\n",
       "      <td>Self Consistency Improves Chain of Thought Reasoning in Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>580.564615</td>\n",
       "      <td>1.135102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Self-Critique Prompting with Large Language Models for Inductive Instructions</td>\n",
       "      <td>Self-Critique Prompting with Large Language Models for Inductive Instructions</td>\n",
       "      <td>2023-05-23</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>Self Critique Prompting with Large Language Models for Inductive Instructions</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>152.564615</td>\n",
       "      <td>0.019664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Self-Refine: Iterative Refinement with Self-Feedback</td>\n",
       "      <td>Self-Refine: Iterative Refinement with Self-Feedback</td>\n",
       "      <td>2023-03-30</td>\n",
       "      <td>2023</td>\n",
       "      <td>201</td>\n",
       "      <td>Self Refine: Iterative Refinement with Self Feedback</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>206.564615</td>\n",
       "      <td>0.973061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Show Your Work: Scratchpads for Intermediate Computation with Language Models</td>\n",
       "      <td>Show Your Work: Scratchpads for Intermediate Computation with Language Models</td>\n",
       "      <td>2021-11-30</td>\n",
       "      <td>2021</td>\n",
       "      <td>315</td>\n",
       "      <td>Show Your Work: Scratchpads for Intermediate Computation with Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>691.564615</td>\n",
       "      <td>0.455489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding</td>\n",
       "      <td>Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding</td>\n",
       "      <td>2023-07-28</td>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>Skeleton of Thought: Large Language Models Can Do Parallel Decoding</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>86.564615</td>\n",
       "      <td>0.057760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer</td>\n",
       "      <td>Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer</td>\n",
       "      <td>2023-03-03</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>233.564615</td>\n",
       "      <td>0.017126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>Successive Prompting for Decomposing Complex Questions</td>\n",
       "      <td>Successive Prompting for Decomposing Complex Questions</td>\n",
       "      <td>2022-12-08</td>\n",
       "      <td>2022</td>\n",
       "      <td>40</td>\n",
       "      <td>Successive Prompting for Decomposing Complex Questions</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>318.564615</td>\n",
       "      <td>0.125563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains</td>\n",
       "      <td>SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains</td>\n",
       "      <td>2023-02-14</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>SwitchPrompt: Learning Domain Specific Gated Soft Prompts for Classification in Low Resource Domains</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>250.564615</td>\n",
       "      <td>0.007982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models</td>\n",
       "      <td>Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models</td>\n",
       "      <td>2023-02-01</td>\n",
       "      <td>2023</td>\n",
       "      <td>26</td>\n",
       "      <td>Synthetic Prompting: Generating Chain of Thought Demonstrations for Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>263.564615</td>\n",
       "      <td>0.098648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks</td>\n",
       "      <td>TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks</td>\n",
       "      <td>2023-05-19</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>156.564615</td>\n",
       "      <td>0.025549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Teaching Algorithmic Reasoning via In-context Learning</td>\n",
       "      <td>Teaching Algorithmic Reasoning via In-context Learning</td>\n",
       "      <td>2022-11-15</td>\n",
       "      <td>2022</td>\n",
       "      <td>52</td>\n",
       "      <td>Teaching Algorithmic Reasoning via In context Learning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>341.564615</td>\n",
       "      <td>0.152241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>The Capacity for Moral Self-Correction in Large Language Models</td>\n",
       "      <td>The Capacity for Moral Self-Correction in Large Language Models</td>\n",
       "      <td>2023-02-15</td>\n",
       "      <td>2023</td>\n",
       "      <td>49</td>\n",
       "      <td>The Capacity for Moral Self Correction in Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>249.564615</td>\n",
       "      <td>0.196342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>The Power of Scale for Parameter-Efficient Prompt Tuning</td>\n",
       "      <td>The Power of Scale for Parameter-Efficient Prompt Tuning</td>\n",
       "      <td>2021-04-18</td>\n",
       "      <td>2021</td>\n",
       "      <td>1530</td>\n",
       "      <td>The Power of Scale for Parameter Efficient Prompt Tuning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>917.564615</td>\n",
       "      <td>1.667458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning</td>\n",
       "      <td>The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning</td>\n",
       "      <td>2022-05-06</td>\n",
       "      <td>2022</td>\n",
       "      <td>47</td>\n",
       "      <td>The Unreliability of Explanations in Few shot Prompting for Textual Reasoning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>534.564615</td>\n",
       "      <td>0.087922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>The Web Can Be Your Oyster for Improving Large Language Models</td>\n",
       "      <td>The Web Can Be Your Oyster for Improving Large Language Models</td>\n",
       "      <td>2023-05-18</td>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>The Web Can Be Your Oyster for Improving Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>157.564615</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Toxicity Detection with Generative Prompt-based Inference</td>\n",
       "      <td>Toxicity Detection with Generative Prompt-based Inference</td>\n",
       "      <td>2022-05-24</td>\n",
       "      <td>2022</td>\n",
       "      <td>11</td>\n",
       "      <td>Toxicity Detection with Generative Prompt based Inference</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>516.564615</td>\n",
       "      <td>0.021295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Training language models to follow instructions with human feedback</td>\n",
       "      <td>Training language models to follow instructions with human feedback</td>\n",
       "      <td>2022-03-04</td>\n",
       "      <td>2022</td>\n",
       "      <td>3009</td>\n",
       "      <td>Training language models to follow instructions with human feedback</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>597.564615</td>\n",
       "      <td>5.035439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Tree of Thoughts: Deliberate Problem Solving with Large Language Models</td>\n",
       "      <td>Tree of Thoughts: Deliberate Problem Solving with Large Language Models</td>\n",
       "      <td>2023-05-17</td>\n",
       "      <td>2023</td>\n",
       "      <td>227</td>\n",
       "      <td>Tree of Thoughts: Deliberate Problem Solving with Large Language Models</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>158.564615</td>\n",
       "      <td>1.431593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding</td>\n",
       "      <td>TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding</td>\n",
       "      <td>2023-05-19</td>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>156.564615</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation</td>\n",
       "      <td>UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation</td>\n",
       "      <td>2023-03-15</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>UPRISE: Universal Prompt Retrieval for Improving Zero Shot Evaluation</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>221.564615</td>\n",
       "      <td>0.036107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Universality and Limitations of Prompt Tuning</td>\n",
       "      <td>Universality and Limitations of Prompt Tuning</td>\n",
       "      <td>2023-05-30</td>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>Universality and Limitations of Prompt Tuning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>145.564615</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Visual-Language Prompt Tuning with Knowledge-guided Context Optimization</td>\n",
       "      <td>Visual-Language Prompt Tuning with Knowledge-Guided Context Optimization</td>\n",
       "      <td>2023-03-23</td>\n",
       "      <td>2023</td>\n",
       "      <td>11</td>\n",
       "      <td>Visual Language Prompt Tuning with Knowledge guided Context Optimization</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>213.564615</td>\n",
       "      <td>0.051507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>What In-Context Learning \"Learns\" In-Context: Disentangling Task Recognition and Task Learning</td>\n",
       "      <td>What In-Context Learning \"Learns\" In-Context: Disentangling Task Recognition and Task Learning</td>\n",
       "      <td>2023-05-16</td>\n",
       "      <td>2023</td>\n",
       "      <td>19</td>\n",
       "      <td>What In Context Learning \"Learns\" In Context: Disentangling Task Recognition and Task Learning</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>159.564615</td>\n",
       "      <td>0.119074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>Why think step-by-step? Reasoning emerges from the locality of experience</td>\n",
       "      <td>Why think step-by-step? Reasoning emerges from the locality of experience</td>\n",
       "      <td>2023-04-07</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>Why think step by step? Reasoning emerges from the locality of experience</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>198.564615</td>\n",
       "      <td>0.040289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>ZeroPrompt: Streaming Acoustic Encoders are Zero-Shot Masked LMs</td>\n",
       "      <td>ZeroPrompt: Streaming Acoustic Encoders are Zero-Shot Masked LMs</td>\n",
       "      <td>2023-05-18</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>ZeroPrompt: Streaming Acoustic Encoders are Zero Shot Masked LMs</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>157.564615</td>\n",
       "      <td>0.006347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>kNN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference</td>\n",
       "      <td>kNN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference</td>\n",
       "      <td>2023-03-24</td>\n",
       "      <td>2023</td>\n",
       "      <td>16</td>\n",
       "      <td>kNN Prompting: Beyond Context Learning with Calibration Free Nearest Neighbor Inference</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>212.564615</td>\n",
       "      <td>0.075271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>-la-carte Prompt Tuning (APT): Combining Distinct Data Via Composable Prompting</td>\n",
       "      <td>-la-carte Prompt Tuning (APT): Combining Distinct Data Via Composable Prompting</td>\n",
       "      <td>2023-02-15</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td> la carte Prompt Tuning (APT): Combining Distinct Data Via Composable Prompting</td>\n",
       "      <td>2023-10-22 00:00:00</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>249.564615</td>\n",
       "      <td>0.016028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Structured Prompting: Scaling In-Context Learning to 1, 000 Examples</td>\n",
       "      <td>Structured Prompting: Scaling In-Context Learning to 1, 000 Examples</td>\n",
       "      <td>2022-12-13</td>\n",
       "      <td>2022</td>\n",
       "      <td>20</td>\n",
       "      <td>Structured Prompting: Scaling In Context Learning to 1, 000 Examples</td>\n",
       "      <td>2023-10-22</td>\n",
       "      <td>2023-10-22 13:33:02.768043</td>\n",
       "      <td>313.564615</td>\n",
       "      <td>0.063783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                     paper title  \\\n",
       "0                                                                            A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT   \n",
       "1                                                                                    A Taxonomy of Prompt Modifiers for Text-To-Image Generation   \n",
       "2                                          AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts   \n",
       "3                                                                     ART: Automatic multi-step reasoning and tool-use for large language models   \n",
       "4                                                                               Active Prompting with Chain-of-Thought for Large Language Models   \n",
       "5                                                   An automatically discovered chain-of-thought prompt generalizes to novel models and datasets   \n",
       "6                                                                               Ask Me Anything: A simple strategy for prompting language models   \n",
       "7                                                                              Atlas: Few-shot Learning with Retrieval Augmented Language Models   \n",
       "8                                                      AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts   \n",
       "9                                                                                  Automatic Chain of Thought Prompting in Large Language Models   \n",
       "10                                                           Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data   \n",
       "11                                                                                                            BERTese: Learning to Speak to BERT   \n",
       "12                                                                                            Batch Prompting: Efficient Inference with LLM APIs   \n",
       "13                                                                                       Better Zero-Shot Reasoning with Self-Adaptive Prompting   \n",
       "14                                                                                            Boosted Prompt Ensembles for Large Language Models   \n",
       "15                                            Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints   \n",
       "16                                                      CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale Language Model Society   \n",
       "17                                                                       Calibrate Before Use: Improving Few-Shot Performance of Language Models   \n",
       "18                                                                Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT   \n",
       "19                                                                                         Can We Edit Factual Knowledge by In-Context Learning?   \n",
       "20                                                                                       Chain of Hindsight Aligns Language Models with Feedback   \n",
       "21                                                                         Chain of Thought Prompting Elicits Reasoning in Large Language Models   \n",
       "22                                                                           Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models   \n",
       "23                                                                          Chain-of-Verification Reduces Hallucination in Large Language Models   \n",
       "24                                     ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer   \n",
       "25                                                           CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification   \n",
       "26                                                                                           Complexity-Based Prompting for Multi-Step Reasoning   \n",
       "27                                                                                               Compositional Exemplars for In-context Learning   \n",
       "28                                      Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt   \n",
       "29                                               Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers   \n",
       "30                                                                                              Constitutional AI: Harmlessness from AI Feedback   \n",
       "31                                                                                          Context-faithful Prompting for Large Language Models   \n",
       "32                                                     CooK: Empowering General-Purpose Language Models with Modular and Collaborative Knowledge   \n",
       "33                                                                            Decomposed Prompting: A Modular Approach for Solving Complex Tasks   \n",
       "34                                               Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP   \n",
       "35                                                                      Design Guidelines for Prompt Engineering Text-to-Image Generative Models   \n",
       "36                                                                           Discovering Language Model Behaviors with Model-Written Evaluations   \n",
       "37                                                                                          DocPrompting: Generating Code by Retrieving the Docs   \n",
       "38                                                        Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning   \n",
       "39                                                                                      Dynamic Prompting: A Unified Framework for Prompt Tuning   \n",
       "40                                                                        Effectiveness of Data Augmentation for Prefix Tuning with Limited Data   \n",
       "41                                                                                           Efficient Prompting via Dynamic In-Context Learning   \n",
       "42                                  Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference   \n",
       "43                                                                                              Evaluating Large Language Models Trained on Code   \n",
       "44                                                                                                 Evaluating the Robustness of Discrete Prompts   \n",
       "45                                             Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples   \n",
       "46                                                                       EvoPrompting: Language Models for Code-Level Neural Architecture Search   \n",
       "47                                                                                     Exploring Lottery Prompts for Pre-trained Language Models   \n",
       "48                                                                                  Fairness-guided Few-shot Prompting for Large Language Models   \n",
       "49                                            Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity   \n",
       "50                                                                       Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency   \n",
       "51                                              Flesch or Fumble? Evaluating Readability Standard Alignment of Instruction-Tuned Language Models   \n",
       "52                                                                                        Focused Prefix Tuning for Controllable Text Generation   \n",
       "53                                                                     From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting   \n",
       "54                                                                                                                        GPT-4 Technical Report   \n",
       "55                                                                                       Generated Knowledge Prompting for Commonsense Reasoning   \n",
       "56                                                                            Global Prompt Cell: A Portable Control Module for Effective Prompt   \n",
       "57                                                                      Graph of Thoughts: Solving Elaborate Problems with Large Language Models   \n",
       "58                                                             GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks   \n",
       "59                                                                              Guiding Large Language Models via Directional Stimulus Prompting   \n",
       "60                                                  Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery   \n",
       "61                                                                         Hierarchical Prompting Assists Large Language Model on Web Navigation   \n",
       "62                                                                                                    How Can We Know What Language Models Know?   \n",
       "63                                                                                              How Does In-Context Learning Help Prompt Tuning?   \n",
       "64                                                  How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks   \n",
       "65                                                                                 Ignore Previous Prompt: Attack Techniques For Language Models   \n",
       "66                                                                                                               In-Context Instruction Learning   \n",
       "67                                                                   In-Context Learning of Large Language Models Explained as Kernel Regression   \n",
       "68                                                                                                       Interactive Natural Language Processing   \n",
       "69                                                                                                                 Is GPT-4 a Good Data Analyst?   \n",
       "70                                                          Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning   \n",
       "71                                                                        Language Is Not All You Need: Aligning Perception with Language Models   \n",
       "72                                                        Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought   \n",
       "73                                                                                                         Language Models are Few-Shot Learners   \n",
       "74                                                                                                   Large Language Model Guided Tree-of-Thought   \n",
       "75                                                                                        Large Language Models Are Human-Level Prompt Engineers   \n",
       "76                                                                          Large Language Models Can Be Easily Distracted by Irrelevant Context   \n",
       "77                                                                                                 Large Language Models are Zero-Shot Reasoners   \n",
       "78                                                                                    Large Language Models are reasoners with Self-Verification   \n",
       "79                                                                                     Larger language models do in-context learning differently   \n",
       "80                                                                                                               Learning from Task Descriptions   \n",
       "81                                                                             Learning to Generate Task-Specific Adapters from Task Description   \n",
       "82                                                                                              Learning to Transfer Prompts for Text Generation   \n",
       "83                                                                    Least-to-most promptingenables complex reasoning in large language models   \n",
       "84                                                           Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses   \n",
       "85                                                             Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning with LLMs   \n",
       "86                                                                                                                     Let's Verify Step by Step   \n",
       "87   MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning   \n",
       "88                                                         Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods   \n",
       "89                                                                Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations   \n",
       "90                                                                                   Making Pre-trained Language Models Better Few-shot Learners   \n",
       "91                                                                                             Meta-in-context learning in large language models   \n",
       "92                                                                                      Mixture of Soft Prompts for Controllable Data Generation   \n",
       "93                                                                                Model-tuning Via Prompts Makes NLP Models Adversarially Robust   \n",
       "94        More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models   \n",
       "95                                                          MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of Thought Prompting   \n",
       "96                                                                                      Multimodal Chain-of-Thought Reasoning in Language Models   \n",
       "97                                                                         Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning   \n",
       "98                                                                             Multitask Prompted Training Enables Zero-Shot Task Generalization   \n",
       "99                                                     On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning   \n",
       "100                                                                                    On the Advance of Making Language Models Better Reasoners   \n",
       "101                                                                                    OpenICL: An Open-Source Framework for In-context Learning   \n",
       "102                                                                                                           PAL: Program-aided Language Models   \n",
       "103                                                       PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents   \n",
       "104                                                                                         PPT: Pre-trained Prompt Tuning for Few-shot Learning   \n",
       "105                                            Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models   \n",
       "106                                                                         Post Hoc Explanations of Language Models Can Improve Language Models   \n",
       "107                                                                                                             Pre-Training to Learn in Context   \n",
       "108                                                                                  Prefix-Tuning: Optimizing Continuous Prompts for Generation   \n",
       "109                                                                                  Progressive Prompts: Continual Learning for Language Models   \n",
       "110                                                                                                 Prompt Engineering for Large Language Models   \n",
       "111                                                                   Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm   \n",
       "112                                                    Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners   \n",
       "113                                                              PromptChainer: Chaining Large Language Model Prompts through Visual Programming   \n",
       "114                                                                                       Promptagator: Few-shot Dense Retrieval From 8 Examples   \n",
       "115                                                                                                  Prompted Opinion Summarization with GPT-3.5   \n",
       "116                                                                                                               Prompting GPT-3 To Be Reliable   \n",
       "117                                                                   Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL   \n",
       "118                                                                                  REFINER: Reasoning Feedback on Intermediate Representations   \n",
       "119                                                                                             Re-Reading Improves Reasoning in Language Models   \n",
       "120                                                                                   ReAct: Synergizing Reasoning and Acting in Language Models   \n",
       "121                                                                                   Reasoning with Language Model is Planning with World Model   \n",
       "122                                                                                                         Recitation-Augmented Language Models   \n",
       "123                                                                       Reflexion: an autonomous agent with dynamic memory and self-reflection   \n",
       "124                                                                                           Reframing Instructional Prompts to GPTk's Language   \n",
       "125                                                              Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling   \n",
       "126                                                                  Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?   \n",
       "127                                                                             Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks   \n",
       "128                                                                                Revisiting Automated Prompting: Are We Actually Doing Better?   \n",
       "129                                                                             Satisfiability-Aided Language Models Using Declarative Prompting   \n",
       "130                                                                 Scalable Prompt Generation for Semi-supervised Learning with Language Models   \n",
       "131                                                                                                Scaling Instruction-Finetuned Language Models   \n",
       "132                                                                                                      Scaling Laws for Neural Language Models   \n",
       "133                                                                      Self-Consistency Improves Chain of Thought Reasoning in Language Models   \n",
       "134                                                                Self-Critique Prompting with Large Language Models for Inductive Instructions   \n",
       "135                                                                                         Self-Refine: Iterative Refinement with Self-Feedback   \n",
       "136                                                                Show Your Work: Scratchpads for Intermediate Computation with Language Models   \n",
       "137                                                                          Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding   \n",
       "138                                                                         Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer   \n",
       "139                                                                                       Successive Prompting for Decomposing Complex Questions   \n",
       "140                                         SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains   \n",
       "141                                                    Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models   \n",
       "142                                                                      TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks   \n",
       "143                                                                                       Teaching Algorithmic Reasoning via In-context Learning   \n",
       "144                                                                              The Capacity for Moral Self-Correction in Large Language Models   \n",
       "145                                                                                     The Power of Scale for Parameter-Efficient Prompt Tuning   \n",
       "146                                                                The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning   \n",
       "147                                                                               The Web Can Be Your Oyster for Improving Large Language Models   \n",
       "148                                                                                    Toxicity Detection with Generative Prompt-based Inference   \n",
       "149                                                                          Training language models to follow instructions with human feedback   \n",
       "150                                                                      Tree of Thoughts: Deliberate Problem Solving with Large Language Models   \n",
       "151                                                                TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding   \n",
       "152                                                                        UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation   \n",
       "153                                                                                                Universality and Limitations of Prompt Tuning   \n",
       "154                                                                     Visual-Language Prompt Tuning with Knowledge-guided Context Optimization   \n",
       "155                                               What In-Context Learning \"Learns\" In-Context: Disentangling Task Recognition and Task Learning   \n",
       "156                                                                    Why think step-by-step? Reasoning emerges from the locality of experience   \n",
       "157                                                                             ZeroPrompt: Streaming Acoustic Encoders are Zero-Shot Masked LMs   \n",
       "158                                                      kNN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference   \n",
       "159                                                             -la-carte Prompt Tuning (APT): Combining Distinct Data Via Composable Prompting   \n",
       "160                                                                         Structured Prompting: Scaling In-Context Learning to 1, 000 Examples   \n",
       "\n",
       "                                                                                                                          semantic scholar title  \\\n",
       "0                                                                            A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT   \n",
       "1                                                                                    A Taxonomy of Prompt Modifiers for Text-To-Image Generation   \n",
       "2                                          AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts   \n",
       "3                                                                     ART: Automatic multi-step reasoning and tool-use for large language models   \n",
       "4                                                                               Active Prompting with Chain-of-Thought for Large Language Models   \n",
       "5                                                   An automatically discovered chain-of-thought prompt generalizes to novel models and datasets   \n",
       "6                                                                               Ask Me Anything: A simple strategy for prompting language models   \n",
       "7                                                                                     Few-shot Learning with Retrieval Augmented Language Models   \n",
       "8                                                                 Eliciting Knowledge from Language Models Using Automatically Generated Prompts   \n",
       "9                                                                                  Automatic Chain of Thought Prompting in Large Language Models   \n",
       "10                                                           Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data   \n",
       "11                                                                                                            BERTese: Learning to Speak to BERT   \n",
       "12                                                                           Batch Prompting: Efficient Inference with Large Language Model APIs   \n",
       "13                                                                                       Better Zero-Shot Reasoning with Self-Adaptive Prompting   \n",
       "14                                                                                            Boosted Prompt Ensembles for Large Language Models   \n",
       "15                                            Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints   \n",
       "16                                                      CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale Language Model Society   \n",
       "17                                                                       Calibrate Before Use: Improving Few-Shot Performance of Language Models   \n",
       "18                                                                Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT   \n",
       "19                                                                                         Can We Edit Factual Knowledge by In-Context Learning?   \n",
       "20                                                                                       Chain of Hindsight Aligns Language Models with Feedback   \n",
       "21                                                                         Chain of Thought Prompting Elicits Reasoning in Large Language Models   \n",
       "22                                                                           Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models   \n",
       "23                                                                          Chain-of-Verification Reduces Hallucination in Large Language Models   \n",
       "24                                     ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer   \n",
       "25                                                           CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification   \n",
       "26                                                                                           Complexity-Based Prompting for Multi-Step Reasoning   \n",
       "27                                                                                               Compositional Exemplars for In-context Learning   \n",
       "28                                      Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt   \n",
       "29                                               Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers   \n",
       "30                                                                                              Constitutional AI: Harmlessness from AI Feedback   \n",
       "31                                                                                          Context-faithful Prompting for Large Language Models   \n",
       "32                                                                                                   SHARED OPEN VOCABULARIES AND SEMANTIC MEDIA   \n",
       "33                                                                            Decomposed Prompting: A Modular Approach for Solving Complex Tasks   \n",
       "34                                               Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP   \n",
       "35                                                                      Design Guidelines for Prompt Engineering Text-to-Image Generative Models   \n",
       "36                                                                           Discovering Language Model Behaviors with Model-Written Evaluations   \n",
       "37                                                                                          DocPrompting: Generating Code by Retrieving the Docs   \n",
       "38                                                        Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning   \n",
       "39                                                                                      Dynamic Prompting: A Unified Framework for Prompt Tuning   \n",
       "40                                                                        Effectiveness of Data Augmentation for Prefix Tuning with Limited Data   \n",
       "41                                                                                           Efficient Prompting via Dynamic In-Context Learning   \n",
       "42                                  Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference   \n",
       "43                                                                                              Evaluating Large Language Models Trained on Code   \n",
       "44                                                                                                 Evaluating the Robustness of Discrete Prompts   \n",
       "45                                             Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples   \n",
       "46                                                                       EvoPrompting: Language Models for Code-Level Neural Architecture Search   \n",
       "47                                                                                     Exploring Lottery Prompts for Pre-trained Language Models   \n",
       "48                                                                                  Fairness-guided Few-shot Prompting for Large Language Models   \n",
       "49                                            Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity   \n",
       "50                                                                       Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency   \n",
       "51                                              Flesch or Fumble? Evaluating Readability Standard Alignment of Instruction-Tuned Language Models   \n",
       "52                                                                                        Focused Prefix Tuning for Controllable Text Generation   \n",
       "53                                                                     From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting   \n",
       "54                                                                                                                        GPT-4 Technical Report   \n",
       "55                                                                                       Generated Knowledge Prompting for Commonsense Reasoning   \n",
       "56                                                                            Global Prompt Cell: A Portable Control Module for Effective Prompt   \n",
       "57                                                                      Graph of Thoughts: Solving Elaborate Problems with Large Language Models   \n",
       "58                                                             GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks   \n",
       "59                                                                              Guiding Large Language Models via Directional Stimulus Prompting   \n",
       "60                                                  Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery   \n",
       "61                                                                         Hierarchical Prompting Assists Large Language Model on Web Navigation   \n",
       "62                                                                                                    How Can We Know What Language Models Know?   \n",
       "63                                                                                              How Does In-Context Learning Help Prompt Tuning?   \n",
       "64                                                  How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks   \n",
       "65                                                                                 Ignore Previous Prompt: Attack Techniques For Language Models   \n",
       "66                                                                                                               In-Context Instruction Learning   \n",
       "67                                                                   In-Context Learning of Large Language Models Explained as Kernel Regression   \n",
       "68                                                                                                       Interactive Natural Language Processing   \n",
       "69                                                                                                                 Is GPT-4 a Good Data Analyst?   \n",
       "70                                                          Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning   \n",
       "71                                                                        Language Is Not All You Need: Aligning Perception with Language Models   \n",
       "72                                                        Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought   \n",
       "73                                                                                                         Language Models are Few-Shot Learners   \n",
       "74                                                                                                   Large Language Model Guided Tree-of-Thought   \n",
       "75                                                                                        Large Language Models Are Human-Level Prompt Engineers   \n",
       "76                                                                          Large Language Models Can Be Easily Distracted by Irrelevant Context   \n",
       "77                                                                                                 Large Language Models are Zero-Shot Reasoners   \n",
       "78                                                                                    Large Language Models are reasoners with Self-Verification   \n",
       "79                                                                                     Larger language models do in-context learning differently   \n",
       "80                                                                                                               Learning from Task Descriptions   \n",
       "81                                                                             Learning to Generate Task-Specific Adapters from Task Description   \n",
       "82                                                                                              Learning to Transfer Prompts for Text Generation   \n",
       "83                                                                    Least-to-Most Prompting Enables Complex Reasoning in Large Language Models   \n",
       "84                                                           Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses   \n",
       "85                                                             Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning with LLMs   \n",
       "86                                                                                                                     Let's Verify Step by Step   \n",
       "87   MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning   \n",
       "88                                                         Machine-Generated Text: A Comprehensive Survey of Threat Models and Detection Methods   \n",
       "89                                                                Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations   \n",
       "90                                                                                   Making Pre-trained Language Models Better Few-shot Learners   \n",
       "91                                                                                             Meta-in-context learning in large language models   \n",
       "92                                                                                      Mixture of Soft Prompts for Controllable Data Generation   \n",
       "93                                                                                Model-tuning Via Prompts Makes NLP Models Adversarially Robust   \n",
       "94        More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models   \n",
       "95                                                          MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of Thought Prompting   \n",
       "96                                                                                      Multimodal Chain-of-Thought Reasoning in Language Models   \n",
       "97                                                                         Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning   \n",
       "98                                                                             Multitask Prompted Training Enables Zero-Shot Task Generalization   \n",
       "99                                                     On Second Thought, Lets Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning   \n",
       "100                                                                                    On the Advance of Making Language Models Better Reasoners   \n",
       "101                                                                                    OpenICL: An Open-Source Framework for In-context Learning   \n",
       "102                                                                                                           PAL: Program-aided Language Models   \n",
       "103                                                       PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents   \n",
       "104                                                                                         PPT: Pre-trained Prompt Tuning for Few-shot Learning   \n",
       "105                                            Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models   \n",
       "106                                                                         Post Hoc Explanations of Language Models Can Improve Language Models   \n",
       "107                                                                                                             Pre-Training to Learn in Context   \n",
       "108                                                                                  Prefix-Tuning: Optimizing Continuous Prompts for Generation   \n",
       "109                                                                                  Progressive Prompts: Continual Learning for Language Models   \n",
       "110                                                                                                 Prompt Engineering for Large Language Models   \n",
       "111                                                                   Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm   \n",
       "112                                                    Prompt, Generate, Then Cache: Cascade of Foundation Models Makes Strong Few-Shot Learners   \n",
       "113                                                              PromptChainer: Chaining Large Language Model Prompts through Visual Programming   \n",
       "114                                                                                       Promptagator: Few-shot Dense Retrieval From 8 Examples   \n",
       "115                                                                                                  Prompted Opinion Summarization with GPT-3.5   \n",
       "116                                                                                                               Prompting GPT-3 To Be Reliable   \n",
       "117                                                                   Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL   \n",
       "118                                                                                  REFINER: Reasoning Feedback on Intermediate Representations   \n",
       "119                                                                                             Re-Reading Improves Reasoning in Language Models   \n",
       "120                                                                                   ReAct: Synergizing Reasoning and Acting in Language Models   \n",
       "121                                                                                   Reasoning with Language Model is Planning with World Model   \n",
       "122                                                                                                         Recitation-Augmented Language Models   \n",
       "123                                                                       Reflexion: an autonomous agent with dynamic memory and self-reflection   \n",
       "124                                                                                           Reframing Instructional Prompts to GPTks Language   \n",
       "125                                                              Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling   \n",
       "126                                                                  Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?   \n",
       "127                                                                             Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks   \n",
       "128                                                                                Revisiting Automated Prompting: Are We Actually Doing Better?   \n",
       "129                                                                             Satisfiability-Aided Language Models Using Declarative Prompting   \n",
       "130                                                                 Scalable Prompt Generation for Semi-supervised Learning with Language Models   \n",
       "131                                                                                                Scaling Instruction-Finetuned Language Models   \n",
       "132                                                                                                      Scaling Laws for Neural Language Models   \n",
       "133                                                                      Self-Consistency Improves Chain of Thought Reasoning in Language Models   \n",
       "134                                                                Self-Critique Prompting with Large Language Models for Inductive Instructions   \n",
       "135                                                                                         Self-Refine: Iterative Refinement with Self-Feedback   \n",
       "136                                                                Show Your Work: Scratchpads for Intermediate Computation with Language Models   \n",
       "137                                                                          Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding   \n",
       "138                                                                         Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer   \n",
       "139                                                                                       Successive Prompting for Decomposing Complex Questions   \n",
       "140                                         SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains   \n",
       "141                                                    Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models   \n",
       "142                                                                      TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks   \n",
       "143                                                                                       Teaching Algorithmic Reasoning via In-context Learning   \n",
       "144                                                                              The Capacity for Moral Self-Correction in Large Language Models   \n",
       "145                                                                                     The Power of Scale for Parameter-Efficient Prompt Tuning   \n",
       "146                                                                The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning   \n",
       "147                                                                               The Web Can Be Your Oyster for Improving Large Language Models   \n",
       "148                                                                                    Toxicity Detection with Generative Prompt-based Inference   \n",
       "149                                                                          Training language models to follow instructions with human feedback   \n",
       "150                                                                      Tree of Thoughts: Deliberate Problem Solving with Large Language Models   \n",
       "151                                                                TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding   \n",
       "152                                                                        UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation   \n",
       "153                                                                                                Universality and Limitations of Prompt Tuning   \n",
       "154                                                                     Visual-Language Prompt Tuning with Knowledge-Guided Context Optimization   \n",
       "155                                               What In-Context Learning \"Learns\" In-Context: Disentangling Task Recognition and Task Learning   \n",
       "156                                                                    Why think step-by-step? Reasoning emerges from the locality of experience   \n",
       "157                                                                             ZeroPrompt: Streaming Acoustic Encoders are Zero-Shot Masked LMs   \n",
       "158                                                      kNN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference   \n",
       "159                                                             -la-carte Prompt Tuning (APT): Combining Distinct Data Via Composable Prompting   \n",
       "160                                                                         Structured Prompting: Scaling In-Context Learning to 1, 000 Examples   \n",
       "\n",
       "    ss_publication_date  ss_year  citation_count  \\\n",
       "0            2023-02-21     2023             163   \n",
       "1            2022-04-20     2022              43   \n",
       "2            2021-10-04     2021             146   \n",
       "3            2023-03-16     2023              39   \n",
       "4            2023-02-23     2023              34   \n",
       "5            2023-05-04     2023               6   \n",
       "6            2022-10-05     2022              65   \n",
       "7            2022-08-05     2022             202   \n",
       "8            2020-10-29     2020             217   \n",
       "9            2022-10-07     2022             201   \n",
       "10           2023-02-24     2023              21   \n",
       "11           2021-03-09     2021              74   \n",
       "12           2023-01-19     2023              10   \n",
       "13           2023-05-23     2023               8   \n",
       "14           2023-04-12     2023              12   \n",
       "15           2023-02-17     2023               6   \n",
       "16           2023-03-31     2023              64   \n",
       "17           2021-02-19     2021             616   \n",
       "18           2023-02-19     2023              73   \n",
       "19           2023-05-22     2023              11   \n",
       "20           2023-02-06     2023              37   \n",
       "21           2022-01-28     2022            2105   \n",
       "22                  NaT     2023               2   \n",
       "23           2023-09-20     2023              10   \n",
       "24           2023-06-13     2023              12   \n",
       "25           2023-03-07     2023               3   \n",
       "26           2022-10-03     2022             116   \n",
       "27           2023-02-11     2023              24   \n",
       "28           2023-05-17     2023               3   \n",
       "29           2023-09-15     2023               8   \n",
       "30           2022-12-15     2022             307   \n",
       "31           2023-03-20     2023              21   \n",
       "32                  NaT     2012               1   \n",
       "33           2022-10-05     2022             104   \n",
       "34           2022-12-28     2022              57   \n",
       "35           2021-09-14     2021             142   \n",
       "36           2022-12-19     2022              87   \n",
       "37           2022-07-13     2022              27   \n",
       "38           2022-09-29     2022              51   \n",
       "39           2023-03-06     2023               4   \n",
       "40                  NaT     2023               0   \n",
       "41           2023-05-18     2023               5   \n",
       "42           2022-11-21     2022              16   \n",
       "43           2021-07-07     2021            1617   \n",
       "44           2023-02-11     2023               4   \n",
       "45           2022-09-05     2022              12   \n",
       "46           2023-02-28     2023              12   \n",
       "47           2023-05-31     2023               1   \n",
       "48           2023-03-23     2023               2   \n",
       "49           2021-04-18     2021             408   \n",
       "50           2023-05-18     2023               1   \n",
       "51           2023-09-11     2023               0   \n",
       "52           2023-06-01     2023               0   \n",
       "53           2023-09-08     2023               2   \n",
       "54           2023-03-15     2023            1529   \n",
       "55           2021-10-15     2021             114   \n",
       "56           2023-04-12     2023               0   \n",
       "57           2023-08-18     2023              35   \n",
       "58           2023-02-16     2023              16   \n",
       "59           2023-02-22     2023              10   \n",
       "60           2023-02-07     2023              38   \n",
       "61           2023-05-23     2023               3   \n",
       "62           2019-11-28     2019             763   \n",
       "63           2023-02-22     2023               5   \n",
       "64           2023-03-01     2023              24   \n",
       "65           2022-11-17     2022              68   \n",
       "66           2023-02-28     2023              19   \n",
       "67                  NaT     2023               4   \n",
       "68           2023-05-22     2023              14   \n",
       "69           2023-05-24     2023              17   \n",
       "70           2023-03-18     2023              16   \n",
       "71           2023-02-27     2023             154   \n",
       "72           2022-10-03     2022              58   \n",
       "73           2020-05-28     2020           16440   \n",
       "74           2023-05-15     2023              26   \n",
       "75           2022-11-03     2022             194   \n",
       "76           2023-01-31     2023              70   \n",
       "77           2022-05-24     2022             882   \n",
       "78                  NaT     2022              31   \n",
       "79           2023-03-07     2023              87   \n",
       "80           2020-11-01     2020              64   \n",
       "81           2021-01-02     2021              18   \n",
       "82           2022-05-03     2022              18   \n",
       "83           2022-05-21     2022             369   \n",
       "84           2023-05-30     2023               1   \n",
       "85           2023-05-19     2023               6   \n",
       "86           2023-05-31     2023              64   \n",
       "87           2022-05-01     2022              31   \n",
       "88           2022-10-13     2022              28   \n",
       "89           2022-05-24     2022              86   \n",
       "90           2021-01-01     2021            1082   \n",
       "91           2023-05-22     2023               5   \n",
       "92           2023-03-02     2023               3   \n",
       "93           2023-03-13     2023               3   \n",
       "94                  NaT     2023              37   \n",
       "95           2023-05-26     2023               1   \n",
       "96           2023-02-02     2023              91   \n",
       "97           2023-03-06     2023              21   \n",
       "98           2021-10-15     2021             924   \n",
       "99           2022-12-15     2022              23   \n",
       "100                 NaT     2022             106   \n",
       "101          2023-03-06     2023              12   \n",
       "102          2022-11-18     2022             198   \n",
       "103          2023-05-23     2023               4   \n",
       "104          2021-09-09     2021             234   \n",
       "105          2023-05-06     2023              46   \n",
       "106          2023-05-19     2023               5   \n",
       "107          2023-05-16     2023               5   \n",
       "108                 NaT     2021            1682   \n",
       "109          2023-01-29     2023              20   \n",
       "110                 NaT     2023               3   \n",
       "111          2021-02-15     2021             317   \n",
       "112          2023-03-03     2023              30   \n",
       "113          2022-03-13     2022              78   \n",
       "114          2022-09-23     2022              66   \n",
       "115          2022-11-29     2022               8   \n",
       "116          2022-10-17     2022              72   \n",
       "117          2023-09-13     2023               1   \n",
       "118          2023-04-04     2023              35   \n",
       "119          2023-09-12     2023               2   \n",
       "120          2022-10-06     2022             332   \n",
       "121          2023-05-24     2023              34   \n",
       "122          2022-10-04     2022              37   \n",
       "123                 NaT     2023             105   \n",
       "124          2021-09-16     2021             121   \n",
       "125          2023-05-17     2023               5   \n",
       "126          2022-02-25     2022             431   \n",
       "127          2020-05-22     2020             956   \n",
       "128          2023-04-07     2023               0   \n",
       "129          2023-05-16     2023               3   \n",
       "130          2023-02-18     2023               1   \n",
       "131          2022-10-20     2022             823   \n",
       "132          2020-01-23     2020            1530   \n",
       "133          2022-03-21     2022             659   \n",
       "134          2023-05-23     2023               3   \n",
       "135          2023-03-30     2023             201   \n",
       "136          2021-11-30     2021             315   \n",
       "137          2023-07-28     2023               5   \n",
       "138          2023-03-03     2023               4   \n",
       "139          2022-12-08     2022              40   \n",
       "140          2023-02-14     2023               2   \n",
       "141          2023-02-01     2023              26   \n",
       "142          2023-05-19     2023               4   \n",
       "143          2022-11-15     2022              52   \n",
       "144          2023-02-15     2023              49   \n",
       "145          2021-04-18     2021            1530   \n",
       "146          2022-05-06     2022              47   \n",
       "147          2023-05-18     2023               0   \n",
       "148          2022-05-24     2022              11   \n",
       "149          2022-03-04     2022            3009   \n",
       "150          2023-05-17     2023             227   \n",
       "151          2023-05-19     2023               0   \n",
       "152          2023-03-15     2023               8   \n",
       "153          2023-05-30     2023               0   \n",
       "154          2023-03-23     2023              11   \n",
       "155          2023-05-16     2023              19   \n",
       "156          2023-04-07     2023               8   \n",
       "157          2023-05-18     2023               1   \n",
       "158          2023-03-24     2023              16   \n",
       "159          2023-02-15     2023               4   \n",
       "160          2022-12-13     2022              20   \n",
       "\n",
       "                                                                                                                                           query  \\\n",
       "0                                                                            A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT   \n",
       "1                                                                                    A Taxonomy of Prompt Modifiers for Text To Image Generation   \n",
       "2                                          AI Chains: Transparent and Controllable Human AI Interaction by Chaining Large Language Model Prompts   \n",
       "3                                                                     ART: Automatic multi step reasoning and tool use for large language models   \n",
       "4                                                                               Active Prompting with Chain of Thought for Large Language Models   \n",
       "5                                                   An automatically discovered chain of thought prompt generalizes to novel models and datasets   \n",
       "6                                                                               Ask Me Anything: A simple strategy for prompting language models   \n",
       "7                                                                              Atlas: Few shot Learning with Retrieval Augmented Language Models   \n",
       "8                                                      AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts   \n",
       "9                                                                                  Automatic Chain of Thought Prompting in Large Language Models   \n",
       "10                                                           Automatic Prompt Augmentation and Selection with Chain of Thought from Labeled Data   \n",
       "11                                                                                                            BERTese: Learning to Speak to BERT   \n",
       "12                                                                                            Batch Prompting: Efficient Inference with LLM APIs   \n",
       "13                                                                                       Better Zero Shot Reasoning with Self Adaptive Prompting   \n",
       "14                                                                                            Boosted Prompt Ensembles for Large Language Models   \n",
       "15                                            Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints   \n",
       "16                                                      CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale Language Model Society   \n",
       "17                                                                       Calibrate Before Use: Improving Few Shot Performance of Language Models   \n",
       "18                                                                Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine tuned BERT   \n",
       "19                                                                                         Can We Edit Factual Knowledge by In Context Learning?   \n",
       "20                                                                                       Chain of Hindsight Aligns Language Models with Feedback   \n",
       "21                                                                         Chain of Thought Prompting Elicits Reasoning in Large Language Models   \n",
       "22                                                                           Chain of Symbol Prompting Elicits Planning in Large Langauge Models   \n",
       "23                                                                          Chain of Verification Reduces Hallucination in Large Language Models   \n",
       "24                                     ChatGPT vs Human authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer   \n",
       "25                                                           CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification   \n",
       "26                                                                                           Complexity Based Prompting for Multi Step Reasoning   \n",
       "27                                                                                               Compositional Exemplars for In context Learning   \n",
       "28                                      Compress, Then Prompt: Improving Accuracy Efficiency Trade off of LLM Inference with Transferable Prompt   \n",
       "29                                               Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers   \n",
       "30                                                                                              Constitutional AI: Harmlessness from AI Feedback   \n",
       "31                                                                                          Context faithful Prompting for Large Language Models   \n",
       "32                                                     CooK: Empowering General Purpose Language Models with Modular and Collaborative Knowledge   \n",
       "33                                                                            Decomposed Prompting: A Modular Approach for Solving Complex Tasks   \n",
       "34                                               Demonstrate Search Predict: Composing retrieval and language models for knowledge intensive NLP   \n",
       "35                                                                      Design Guidelines for Prompt Engineering Text to Image Generative Models   \n",
       "36                                                                           Discovering Language Model Behaviors with Model Written Evaluations   \n",
       "37                                                                                          DocPrompting: Generating Code by Retrieving the Docs   \n",
       "38                                                        Dynamic Prompt Learning via Policy Gradient for Semi structured Mathematical Reasoning   \n",
       "39                                                                                      Dynamic Prompting: A Unified Framework for Prompt Tuning   \n",
       "40                                                                        Effectiveness of Data Augmentation for Prefix Tuning with Limited Data   \n",
       "41                                                                                           Efficient Prompting via Dynamic In Context Learning   \n",
       "42                                  Enhancing Self Consistency and Performance of Pre Trained Language Models through Natural Language Inference   \n",
       "43                                                                                              Evaluating Large Language Models Trained on Code   \n",
       "44                                                                                                 Evaluating the Robustness of Discrete Prompts   \n",
       "45                                             Evaluating the Susceptibility of Pre Trained Language Models via Handcrafted Adversarial Examples   \n",
       "46                                                                       EvoPrompting: Language Models for Code Level Neural Architecture Search   \n",
       "47                                                                                     Exploring Lottery Prompts for Pre trained Language Models   \n",
       "48                                                                                  Fairness guided Few shot Prompting for Large Language Models   \n",
       "49                                            Fantastically Ordered Prompts and Where to Find Them: Overcoming Few Shot Prompt Order Sensitivity   \n",
       "50                                                                       Flatness Aware Prompt Selection Improves Accuracy and Sample Efficiency   \n",
       "51                                              Flesch or Fumble? Evaluating Readability Standard Alignment of Instruction Tuned Language Models   \n",
       "52                                                                                        Focused Prefix Tuning for Controllable Text Generation   \n",
       "53                                                                     From Sparse to Dense: GPT 4 Summarization with Chain of Density Prompting   \n",
       "54                                                                                                                        GPT 4 Technical Report   \n",
       "55                                                                                       Generated Knowledge Prompting for Commonsense Reasoning   \n",
       "56                                                                            Global Prompt Cell: A Portable Control Module for Effective Prompt   \n",
       "57                                                                      Graph of Thoughts: Solving Elaborate Problems with Large Language Models   \n",
       "58                                                             GraphPrompt: Unifying Pre Training and Downstream Tasks for Graph Neural Networks   \n",
       "59                                                                              Guiding Large Language Models via Directional Stimulus Prompting   \n",
       "60                                                  Hard Prompts Made Easy: Gradient Based Discrete Optimization for Prompt Tuning and Discovery   \n",
       "61                                                                         Hierarchical Prompting Assists Large Language Model on Web Navigation   \n",
       "62                                                                                                    How Can We Know What Language Models Know?   \n",
       "63                                                                                              How Does In Context Learning Help Prompt Tuning?   \n",
       "64                                                  How Robust is GPT 3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks   \n",
       "65                                                                                 Ignore Previous Prompt: Attack Techniques For Language Models   \n",
       "66                                                                                                               In Context Instruction Learning   \n",
       "67                                                                   In Context Learning of Large Language Models Explained as Kernel Regression   \n",
       "68                                                                                                       Interactive Natural Language Processing   \n",
       "69                                                                                                                 Is GPT 4 a Good Data Analyst?   \n",
       "70                                                          Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning   \n",
       "71                                                                        Language Is Not All You Need: Aligning Perception with Language Models   \n",
       "72                                                        Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain of Thought   \n",
       "73                                                                                                         Language Models are Few Shot Learners   \n",
       "74                                                                                                   Large Language Model Guided Tree of Thought   \n",
       "75                                                                                        Large Language Models Are Human Level Prompt Engineers   \n",
       "76                                                                          Large Language Models Can Be Easily Distracted by Irrelevant Context   \n",
       "77                                                                                                 Large Language Models are Zero Shot Reasoners   \n",
       "78                                                                                    Large Language Models are reasoners with Self Verification   \n",
       "79                                                                                     Larger language models do in context learning differently   \n",
       "80                                                                                                               Learning from Task Descriptions   \n",
       "81                                                                             Learning to Generate Task Specific Adapters from Task Description   \n",
       "82                                                                                              Learning to Transfer Prompts for Text Generation   \n",
       "83                                                                    Least to most promptingenables complex reasoning in large language models   \n",
       "84                                                           Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses   \n",
       "85                                                             Let's Sample Step by Step: Adaptive Consistency for Efficient Reasoning with LLMs   \n",
       "86                                                                                                                     Let's Verify Step by Step   \n",
       "87   MRKL Systems: A modular, neuro symbolic architecture that combines large language models, external knowledge sources and discrete reasoning   \n",
       "88                                                         Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods   \n",
       "89                                                                Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations   \n",
       "90                                                                                   Making Pre trained Language Models Better Few shot Learners   \n",
       "91                                                                                             Meta in context learning in large language models   \n",
       "92                                                                                      Mixture of Soft Prompts for Controllable Data Generation   \n",
       "93                                                                                Model tuning Via Prompts Makes NLP Models Adversarially Robust   \n",
       "94        More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application Integrated Large Language Models   \n",
       "95                                                          MultiTool CoT: GPT 3 Can Use Multiple External Tools with Chain of Thought Prompting   \n",
       "96                                                                                      Multimodal Chain of Thought Reasoning in Language Models   \n",
       "97                                                                         Multitask Prompt Tuning Enables Parameter Efficient Transfer Learning   \n",
       "98                                                                             Multitask Prompted Training Enables Zero Shot Task Generalization   \n",
       "99                                                     On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero Shot Reasoning   \n",
       "100                                                                                    On the Advance of Making Language Models Better Reasoners   \n",
       "101                                                                                    OpenICL: An Open Source Framework for In context Learning   \n",
       "102                                                                                                           PAL: Program aided Language Models   \n",
       "103                                                       PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents   \n",
       "104                                                                                         PPT: Pre trained Prompt Tuning for Few shot Learning   \n",
       "105                                            Plan and Solve Prompting: Improving Zero Shot Chain of Thought Reasoning by Large Language Models   \n",
       "106                                                                         Post Hoc Explanations of Language Models Can Improve Language Models   \n",
       "107                                                                                                             Pre Training to Learn in Context   \n",
       "108                                                                                  Prefix Tuning: Optimizing Continuous Prompts for Generation   \n",
       "109                                                                                  Progressive Prompts: Continual Learning for Language Models   \n",
       "110                                                                                                 Prompt Engineering for Large Language Models   \n",
       "111                                                                   Prompt Programming for Large Language Models: Beyond the Few Shot Paradigm   \n",
       "112                                                    Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few shot Learners   \n",
       "113                                                              PromptChainer: Chaining Large Language Model Prompts through Visual Programming   \n",
       "114                                                                                       Promptagator: Few shot Dense Retrieval From 8 Examples   \n",
       "115                                                                                                  Prompted Opinion Summarization with GPT 3.5   \n",
       "116                                                                                                               Prompting GPT 3 To Be Reliable   \n",
       "117                                                                   Query Dependent Prompt Evaluation and Optimization with Offline Inverse RL   \n",
       "118                                                                                  REFINER: Reasoning Feedback on Intermediate Representations   \n",
       "119                                                                                             Re Reading Improves Reasoning in Language Models   \n",
       "120                                                                                   ReAct: Synergizing Reasoning and Acting in Language Models   \n",
       "121                                                                                   Reasoning with Language Model is Planning with World Model   \n",
       "122                                                                                                         Recitation Augmented Language Models   \n",
       "123                                                                       Reflexion: an autonomous agent with dynamic memory and self reflection   \n",
       "124                                                                                           Reframing Instructional Prompts to GPTk's Language   \n",
       "125                                                              Reprompting: Automated Chain of Thought Prompt Inference Through Gibbs Sampling   \n",
       "126                                                                  Rethinking the Role of Demonstrations: What Makes In Context Learning Work?   \n",
       "127                                                                             Retrieval Augmented Generation for Knowledge Intensive NLP Tasks   \n",
       "128                                                                                Revisiting Automated Prompting: Are We Actually Doing Better?   \n",
       "129                                                                             Satisfiability Aided Language Models Using Declarative Prompting   \n",
       "130                                                                 Scalable Prompt Generation for Semi supervised Learning with Language Models   \n",
       "131                                                                                                Scaling Instruction Finetuned Language Models   \n",
       "132                                                                                                      Scaling Laws for Neural Language Models   \n",
       "133                                                                      Self Consistency Improves Chain of Thought Reasoning in Language Models   \n",
       "134                                                                Self Critique Prompting with Large Language Models for Inductive Instructions   \n",
       "135                                                                                         Self Refine: Iterative Refinement with Self Feedback   \n",
       "136                                                                Show Your Work: Scratchpads for Intermediate Computation with Language Models   \n",
       "137                                                                          Skeleton of Thought: Large Language Models Can Do Parallel Decoding   \n",
       "138                                                                         Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer   \n",
       "139                                                                                       Successive Prompting for Decomposing Complex Questions   \n",
       "140                                         SwitchPrompt: Learning Domain Specific Gated Soft Prompts for Classification in Low Resource Domains   \n",
       "141                                                    Synthetic Prompting: Generating Chain of Thought Demonstrations for Large Language Models   \n",
       "142                                                                      TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks   \n",
       "143                                                                                       Teaching Algorithmic Reasoning via In context Learning   \n",
       "144                                                                              The Capacity for Moral Self Correction in Large Language Models   \n",
       "145                                                                                     The Power of Scale for Parameter Efficient Prompt Tuning   \n",
       "146                                                                The Unreliability of Explanations in Few shot Prompting for Textual Reasoning   \n",
       "147                                                                               The Web Can Be Your Oyster for Improving Large Language Models   \n",
       "148                                                                                    Toxicity Detection with Generative Prompt based Inference   \n",
       "149                                                                          Training language models to follow instructions with human feedback   \n",
       "150                                                                      Tree of Thoughts: Deliberate Problem Solving with Large Language Models   \n",
       "151                                                                TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding   \n",
       "152                                                                        UPRISE: Universal Prompt Retrieval for Improving Zero Shot Evaluation   \n",
       "153                                                                                                Universality and Limitations of Prompt Tuning   \n",
       "154                                                                     Visual Language Prompt Tuning with Knowledge guided Context Optimization   \n",
       "155                                               What In Context Learning \"Learns\" In Context: Disentangling Task Recognition and Task Learning   \n",
       "156                                                                    Why think step by step? Reasoning emerges from the locality of experience   \n",
       "157                                                                             ZeroPrompt: Streaming Acoustic Encoders are Zero Shot Masked LMs   \n",
       "158                                                      kNN Prompting: Beyond Context Learning with Calibration Free Nearest Neighbor Inference   \n",
       "159                                                              la carte Prompt Tuning (APT): Combining Distinct Data Via Composable Prompting   \n",
       "160                                                                         Structured Prompting: Scaling In Context Learning to 1, 000 Examples   \n",
       "\n",
       "             day_queried                   end_date  \\\n",
       "0    2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "1    2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "2    2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "3    2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "4    2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "5    2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "6    2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "7    2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "8    2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "9    2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "10   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "11   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "12   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "13   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "14   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "15   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "16   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "17   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "18   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "19   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "20   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "21   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "22   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "23   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "24   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "25   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "26   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "27   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "28   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "29   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "30   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "31   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "32   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "33   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "34   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "35   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "36   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "37   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "38   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "39   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "40   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "41   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "42   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "43   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "44   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "45   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "46   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "47   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "48   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "49   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "50   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "51   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "52   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "53   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "54   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "55   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "56   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "57   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "58   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "59   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "60   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "61   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "62   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "63   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "64   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "65   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "66   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "67   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "68   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "69   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "70   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "71   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "72   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "73   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "74   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "75   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "76   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "77   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "78   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "79   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "80   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "81   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "82   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "83   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "84   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "85   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "86   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "87   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "88   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "89   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "90   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "91   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "92   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "93   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "94   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "95   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "96   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "97   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "98   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "99   2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "100  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "101  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "102  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "103  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "104  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "105  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "106  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "107  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "108  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "109  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "110  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "111  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "112  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "113  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "114  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "115  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "116  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "117  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "118  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "119  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "120  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "121  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "122  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "123  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "124  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "125  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "126  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "127  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "128  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "129  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "130  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "131  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "132  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "133  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "134  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "135  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "136  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "137  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "138  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "139  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "140  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "141  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "142  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "143  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "144  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "145  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "146  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "147  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "148  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "149  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "150  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "151  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "152  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "153  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "154  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "155  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "156  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "157  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "158  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "159  2023-10-22 00:00:00 2023-10-22 13:33:02.768043   \n",
       "160           2023-10-22 2023-10-22 13:33:02.768043   \n",
       "\n",
       "     days_from_pub_to_end_date  citations_per_day  \n",
       "0                   243.564615           0.669227  \n",
       "1                   550.564615           0.078102  \n",
       "2                   748.564615           0.195040  \n",
       "3                   220.564615           0.176819  \n",
       "4                   241.564615           0.140749  \n",
       "5                   171.564615           0.034972  \n",
       "6                   382.564615           0.169906  \n",
       "7                   443.564615           0.455402  \n",
       "8                  1088.564615           0.199345  \n",
       "9                   380.564615           0.528163  \n",
       "10                  240.564615           0.087295  \n",
       "11                  957.564615           0.077279  \n",
       "12                  276.564615           0.036158  \n",
       "13                  152.564615           0.052437  \n",
       "14                  193.564615           0.061995  \n",
       "15                  247.564615           0.024236  \n",
       "16                  205.564615           0.311338  \n",
       "17                  975.564615           0.631429  \n",
       "18                  245.564615           0.297274  \n",
       "19                  153.564615           0.071631  \n",
       "20                  258.564615           0.143098  \n",
       "21                  632.564615           3.327723  \n",
       "22                         NaN                NaN  \n",
       "23                   32.564615           0.307082  \n",
       "24                  131.564615           0.091210  \n",
       "25                  229.564615           0.013068  \n",
       "26                  384.564615           0.301640  \n",
       "27                  253.564615           0.094650  \n",
       "28                  158.564615           0.018920  \n",
       "29                   37.564615           0.212966  \n",
       "30                  311.564615           0.985349  \n",
       "31                  216.564615           0.096969  \n",
       "32                         NaN                NaN  \n",
       "33                  382.564615           0.271850  \n",
       "34                  298.564615           0.190913  \n",
       "35                  768.564615           0.184760  \n",
       "36                  307.564615           0.282867  \n",
       "37                  466.564615           0.057870  \n",
       "38                  388.564615           0.131252  \n",
       "39                  230.564615           0.017349  \n",
       "40                         NaN                NaN  \n",
       "41                  157.564615           0.031733  \n",
       "42                  335.564615           0.047681  \n",
       "43                  837.564615           1.930597  \n",
       "44                  253.564615           0.015775  \n",
       "45                  412.564615           0.029086  \n",
       "46                  236.564615           0.050726  \n",
       "47                  144.564615           0.006917  \n",
       "48                  213.564615           0.009365  \n",
       "49                  917.564615           0.444655  \n",
       "50                  157.564615           0.006347  \n",
       "51                   41.564615           0.000000  \n",
       "52                  143.564615           0.000000  \n",
       "53                   44.564615           0.044879  \n",
       "54                  221.564615           6.900921  \n",
       "55                  737.564615           0.154563  \n",
       "56                  193.564615           0.000000  \n",
       "57                   65.564615           0.533825  \n",
       "58                  248.564615           0.064370  \n",
       "59                  242.564615           0.041226  \n",
       "60                  257.564615           0.147536  \n",
       "61                  152.564615           0.019664  \n",
       "62                 1424.564615           0.535602  \n",
       "63                  242.564615           0.020613  \n",
       "64                  235.564615           0.101883  \n",
       "65                  339.564615           0.200256  \n",
       "66                  236.564615           0.080316  \n",
       "67                         NaN                NaN  \n",
       "68                  153.564615           0.091167  \n",
       "69                  151.564615           0.112163  \n",
       "70                  218.564615           0.073205  \n",
       "71                  237.564615           0.648245  \n",
       "72                  384.564615           0.150820  \n",
       "73                 1242.564615          13.230700  \n",
       "74                  160.564615           0.161929  \n",
       "75                  353.564615           0.548697  \n",
       "76                  264.564615           0.264586  \n",
       "77                  516.564615           1.707434  \n",
       "78                         NaN                NaN  \n",
       "79                  229.564615           0.378978  \n",
       "80                 1085.564615           0.058955  \n",
       "81                 1023.564615           0.017586  \n",
       "82                  537.564615           0.033484  \n",
       "83                  519.564615           0.710210  \n",
       "84                  145.564615           0.006870  \n",
       "85                  156.564615           0.038323  \n",
       "86                  144.564615           0.442709  \n",
       "87                  539.564615           0.057454  \n",
       "88                  374.564615           0.074753  \n",
       "89                  516.564615           0.166484  \n",
       "90                 1024.564615           1.056058  \n",
       "91                  153.564615           0.032560  \n",
       "92                  234.564615           0.012790  \n",
       "93                  223.564615           0.013419  \n",
       "94                         NaN                NaN  \n",
       "95                  149.564615           0.006686  \n",
       "96                  262.564615           0.346581  \n",
       "97                  230.564615           0.091081  \n",
       "98                  737.564615           1.252772  \n",
       "99                  311.564615           0.073821  \n",
       "100                        NaN                NaN  \n",
       "101                 230.564615           0.052046  \n",
       "102                 338.564615           0.584822  \n",
       "103                 152.564615           0.026218  \n",
       "104                 773.564615           0.302496  \n",
       "105                 169.564615           0.271283  \n",
       "106                 156.564615           0.031936  \n",
       "107                 159.564615           0.031335  \n",
       "108                        NaN                NaN  \n",
       "109                 266.564615           0.075029  \n",
       "110                        NaN                NaN  \n",
       "111                 979.564615           0.323613  \n",
       "112                 233.564615           0.128444  \n",
       "113                 588.564615           0.132526  \n",
       "114                 394.564615           0.167273  \n",
       "115                 327.564615           0.024423  \n",
       "116                 370.564615           0.194298  \n",
       "117                  39.564615           0.025275  \n",
       "118                 201.564615           0.173642  \n",
       "119                  40.564615           0.049304  \n",
       "120                 381.564615           0.870102  \n",
       "121                 151.564615           0.224327  \n",
       "122                 383.564615           0.096464  \n",
       "123                        NaN                NaN  \n",
       "124                 766.564615           0.157847  \n",
       "125                 158.564615           0.031533  \n",
       "126                 604.564615           0.712910  \n",
       "127                1248.564615           0.765679  \n",
       "128                 198.564615           0.000000  \n",
       "129                 159.564615           0.018801  \n",
       "130                 246.564615           0.004056  \n",
       "131                 367.564615           2.239062  \n",
       "132                1368.564615           1.117960  \n",
       "133                 580.564615           1.135102  \n",
       "134                 152.564615           0.019664  \n",
       "135                 206.564615           0.973061  \n",
       "136                 691.564615           0.455489  \n",
       "137                  86.564615           0.057760  \n",
       "138                 233.564615           0.017126  \n",
       "139                 318.564615           0.125563  \n",
       "140                 250.564615           0.007982  \n",
       "141                 263.564615           0.098648  \n",
       "142                 156.564615           0.025549  \n",
       "143                 341.564615           0.152241  \n",
       "144                 249.564615           0.196342  \n",
       "145                 917.564615           1.667458  \n",
       "146                 534.564615           0.087922  \n",
       "147                 157.564615           0.000000  \n",
       "148                 516.564615           0.021295  \n",
       "149                 597.564615           5.035439  \n",
       "150                 158.564615           1.431593  \n",
       "151                 156.564615           0.000000  \n",
       "152                 221.564615           0.036107  \n",
       "153                 145.564615           0.000000  \n",
       "154                 213.564615           0.051507  \n",
       "155                 159.564615           0.119074  \n",
       "156                 198.564615           0.040289  \n",
       "157                 157.564615           0.006347  \n",
       "158                 212.564615           0.075271  \n",
       "159                 249.564615           0.016028  \n",
       "160                 313.564615           0.063783  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a column for citations per day\n",
    "semantic_scholar_df[\"citations_per_day\"] = semantic_scholar_df[\"citation_count\"] / semantic_scholar_df[\"days_from_pub_to_end_date\"]\n",
    "\n",
    "semantic_scholar_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161\n"
     ]
    }
   ],
   "source": [
    "# Ensure no duplicates on 'semantic scholar title'\n",
    "\n",
    "no_duplicates_df = semantic_scholar_df.drop_duplicates(subset=['semantic scholar title'])\n",
    "\n",
    "print(len(no_duplicates_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper title</th>\n",
       "      <th>semantic scholar title</th>\n",
       "      <th>ss_publication_date</th>\n",
       "      <th>ss_year</th>\n",
       "      <th>citation_count</th>\n",
       "      <th>query</th>\n",
       "      <th>day_queried</th>\n",
       "      <th>end_date</th>\n",
       "      <th>days_from_pub_to_end_date</th>\n",
       "      <th>citations_per_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [paper title, semantic scholar title, ss_publication_date, ss_year, citation_count, query, day_queried, end_date, days_from_pub_to_end_date, citations_per_day]\n",
       "Index: []"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print any duplicates on 'semantic scholar title'\n",
    "\n",
    "duplicates_df = semantic_scholar_df[semantic_scholar_df.duplicated(subset=['semantic scholar title'], keep=False)]\n",
    "\n",
    "# Sort by 'semantic scholar title' to make it easier to read\n",
    "duplicates_df = duplicates_df.sort_values(by=['semantic scholar title'])\n",
    "\n",
    "duplicates_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicated rows in semantic_scholar_df\n",
    "semantic_scholar_df = semantic_scholar_df.drop_duplicates()\n",
    "\n",
    "# Trim whitespace in 'paper title' column\n",
    "semantic_scholar_df['paper title'] = semantic_scholar_df['paper title'].str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by citations per day, descending\n",
    "semantic_scholar_df = semantic_scholar_df.sort_values(by=[\"citations_per_day\"], ascending=False)\n",
    "\n",
    "# Output to Excel\n",
    "semantic_scholar_df.to_excel(\"Semantic Scholar Citations Per Day.xlsx\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
