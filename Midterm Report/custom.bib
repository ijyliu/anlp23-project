
@misc{openai_gpt-4_2023,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	urldate = {2023-09-25},
	publisher = {arXiv},
	author = {OpenAI},
	month = mar,
	year = {2023},
	note = {arXiv:2303.08774 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 100 pages},
	annote = {Comment: 100 pages},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\ZLQQGQUP\\2303.html:text/html;Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\LM72BFL3\\OpenAI - 2023 - GPT-4 Technical Report.pdf:application/pdf},
}

@misc{cheng_is_2023,
	title = {Is {GPT}-4 a {Good} {Data} {Analyst}?},
	url = {http://arxiv.org/abs/2305.15038},
	abstract = {As large language models (LLMs) have demonstrated their powerful capabilities in plenty of domains and tasks, including context understanding, code generation, language generation, data storytelling, etc., many data analysts may raise concerns if their jobs will be replaced by AI. This controversial topic has drawn a lot of attention in public. However, we are still at a stage of divergent opinions without any definitive conclusion. Motivated by this, we raise the research question of "is GPT-4 a good data analyst?" in this work and aim to answer it by conducting head-to-head comparative studies. In detail, we regard GPT-4 as a data analyst to perform end-to-end data analysis with databases from a wide range of domains. We propose a framework to tackle the problems by carefully designing the prompts for GPT-4 to conduct experiments. We also design several task-specific evaluation metrics to systematically compare the performance between several professional human data analysts and GPT-4. Experimental results show that GPT-4 can achieve comparable performance to humans. We also provide in-depth discussions about our results to shed light on further studies before we reach the conclusion that GPT-4 can replace data analysts.},
	urldate = {2023-09-25},
	publisher = {arXiv},
	author = {Cheng, Liying and Li, Xingxuan and Bing, Lidong},
	month = may,
	year = {2023},
	note = {arXiv:2305.15038 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 11 pages, 2 figures},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\4N77GWHJ\\2305.html:text/html;Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\QDTTIL6T\\Cheng et al. - 2023 - Is GPT-4 a Good Data Analyst.pdf:application/pdf},
}

@article{roose_conversation_2023,
	chapter = {Technology},
	title = {A {Conversation} {With} {Bing}’s {Chatbot} {Left} {Me} {Deeply} {Unsettled}},
	issn = {0362-4331},
	url = {https://www.nytimes.com/2023/02/16/technology/bing-chatbot-microsoft-chatgpt.html},
	abstract = {A very strange conversation with the chatbot built into Microsoft’s search engine led to it declaring its love for me.},
	language = {en-US},
	urldate = {2023-09-25},
	journal = {The New York Times},
	author = {Roose, Kevin},
	month = feb,
	year = {2023},
	keywords = {Artificial Intelligence, ChatGPT, Computers and the Internet, Conversation, internal-sub-only, Microsoft Corp, OpenAI Labs, Search Engines},
	file = {Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\XQCJKDEG\\bing-chatbot-microsoft-chatgpt.html:text/html},
}

@misc{ethan_mollick_emollick_i_2023,
	type = {Tweet},
	title = {I have a strong suspicion that “prompt engineering” is not going to be a big deal in the long-term \& prompt engineer is not the job of the future {AI} gets easier. {You} can already see in {Midjourney} how basic prompts went from complex in v3 to easy in v4. {Same} with {ChatGPT} to {Bing}. https://t.co/{BTtSN4oVF4}},
	url = {https://twitter.com/emollick/status/1627804798224580608},
	language = {en},
	urldate = {2023-09-26},
	journal = {Twitter},
	author = {{Ethan Mollick [@emollick]}},
	month = feb,
	year = {2023},
	file = {Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\EUFKYTRM\\1627804798224580608.html:text/html},
}

@inproceedings{wu_ai_2022,
	address = {New Orleans LA USA},
	title = {{AI} {Chains}: {Transparent} and {Controllable} {Human}-{AI} {Interaction} by {Chaining} {Large} {Language} {Model} {Prompts}},
	isbn = {978-1-4503-9157-3},
	shorttitle = {{AI} {Chains}},
	url = {https://dl.acm.org/doi/10.1145/3491102.3517582},
	doi = {10.1145/3491102.3517582},
	language = {en},
	urldate = {2023-09-26},
	booktitle = {{CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Wu, Tongshuang and Terry, Michael and Cai, Carrie Jun},
	month = apr,
	year = {2022},
	pages = {1--22},
	file = {Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\G3NM64QK\\Wu et al. - 2022 - AI Chains Transparent and Controllable Human-AI I.pdf:application/pdf},
}

@article{acar_ai_2023,
	title = {{AI} {Prompt} {Engineering} {Isn}’t the {Future}},
	issn = {0017-8012},
	url = {https://hbr.org/2023/06/ai-prompt-engineering-isnt-the-future},
	abstract = {Despite the buzz surrounding it, the prominence of prompt engineering may be fleeting. A more enduring and adaptable skill will keep enabling us to harness the potential of generative AI? It is called problem formulation — the ability to identify, analyze, and delineate problems.},
	urldate = {2023-09-26},
	journal = {Harvard Business Review},
	author = {Acar, Oguz A.},
	month = jun,
	year = {2023},
	note = {Section: Technology and analytics},
	keywords = {AI and machine learning, Algorithms, Analytics and data science, Automation, Cybersecurity and digital privacy, Data management, Enterprise computing, Information management, Technology and analytics, Web-based technologies},
	file = {Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\I72G55WX\\ai-prompt-engineering-isnt-the-future.html:text/html},
}

@misc{diao_active_2023,
	title = {Active {Prompting} with {Chain}-of-{Thought} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2302.12246},
	abstract = {The increasing scale of large language models (LLMs) brings emergent abilities to various complex tasks requiring reasoning, such as arithmetic and commonsense reasoning. It is known that the effective design of task-specific prompts is critical for LLMs' ability to produce high-quality answers. In particular, an effective approach for complex question-and-answer tasks is example-based prompting with chain-of-thought (CoT) reasoning, which significantly improves the performance of LLMs. However, current CoT methods rely on a fixed set of human-annotated exemplars, which are not necessarily the most effective examples for different tasks. This paper proposes a new method, Active-Prompt, to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning). For this purpose, we propose a solution to the key problem of determining which questions are the most important and helpful ones to annotate from a pool of task-specific queries. By borrowing ideas from the related problem of uncertainty-based active learning, we introduce several metrics to characterize the uncertainty so as to select the most uncertain questions for annotation. Experimental results demonstrate the superiority of our proposed method, achieving state-of-the-art on eight complex reasoning tasks. Further analyses of different uncertainty metrics, pool sizes, zero-shot learning, and accuracy-uncertainty relationship demonstrate the effectiveness of our method. Our code will be available at https://github.com/shizhediao/active-prompt.},
	urldate = {2023-09-26},
	publisher = {arXiv},
	author = {Diao, Shizhe and Wang, Pengcheng and Lin, Yong and Zhang, Tong},
	month = may,
	year = {2023},
	note = {arXiv:2302.12246 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 20 pages, 3 figures, 11 tables},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\WSCU8EX6\\2302.html:text/html;Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\EZGKACIN\\Diao et al. - 2023 - Active Prompting with Chain-of-Thought for Large L.pdf:application/pdf},
}

@misc{noauthor_promptbase_nodate,
	title = {{PromptBase}},
	shorttitle = {{PromptBase} {\textbar} {Prompt} {Marketplace}},
	url = {https://promptbase.com},
	abstract = {Search 100,000+ quality AI prompts from top prompt engineers. Produce better outputs, save on time \& API costs, sell your own prompts.},
	language = {en},
	urldate = {2023-09-26},
}

@misc{hebenstreit_automatically_2023,
	title = {An automatically discovered chain-of-thought prompt generalizes to novel models and datasets},
	url = {http://arxiv.org/abs/2305.02897},
	abstract = {Emergent chain-of-thought (CoT) reasoning capabilities promise to improve performance and explainability of large language models (LLMs). However, uncertainties remain about how reasoning strategies formulated for previous model generations generalize to new model generations and different datasets. In this small-scale study, we compare different reasoning strategies induced by zero-shot prompting across six recently released LLMs (davinci-002, davinci-003, GPT-3.5-turbo, GPT-4, Flan-T5-xxl and Cohere command-xlarge) on a mixture of six question-answering datasets, including datasets from scientific and medical domains. Our findings demonstrate that while some variations in effectiveness occur, gains from CoT reasoning strategies remain robust across different models and datasets. GPT-4 has the most benefit from current state-of-the-art reasoning strategies and exhibits the best performance by applying a prompt previously discovered through automated discovery.},
	urldate = {2023-09-26},
	publisher = {arXiv},
	author = {Hebenstreit, Konstantin and Praas, Robert and Kiesewetter, Louis P. and Samwald, Matthias},
	month = aug,
	year = {2023},
	note = {arXiv:2305.02897 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\EZIPW94R\\2305.html:text/html;Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\9DQ7M9VQ\\Hebenstreit et al. - 2023 - An automatically discovered chain-of-thought promp.pdf:application/pdf},
}

@article{wei_chain--thought_nodate,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	abstract = {We explore how generating a chain of thought—a series of intermediate reasoning steps—signiﬁcantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufﬁciently large language models via a simple method called chain-ofthought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even ﬁnetuned GPT-3 with a veriﬁer.},
	language = {en},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed H and Le, Quoc V and Zhou, Denny},
	file = {Wei et al. - Chain-of-Thought Prompting Elicits Reasoning in La.pdf:C\:\\Users\\ijyli\\Zotero\\storage\\8AG6AWPT\\Wei et al. - Chain-of-Thought Prompting Elicits Reasoning in La.pdf:application/pdf},
}

@misc{zhang_automatic_2022,
	title = {Automatic {Chain} of {Thought} {Prompting} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2210.03493},
	abstract = {Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like "Let's think step by step" to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the "Let's think step by step" prompt to generate reasoning chains for demonstrations one by one, i.e., let's think not just step by step, but also one by one. However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at https://github.com/amazon-research/auto-cot},
	urldate = {2023-09-26},
	publisher = {arXiv},
	author = {Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Smola, Alex},
	month = oct,
	year = {2022},
	note = {arXiv:2210.03493 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\8TAIA3HA\\2210.html:text/html;Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\BREVBII4\\Zhang et al. - 2022 - Automatic Chain of Thought Prompting in Large Lang.pdf:application/pdf},
}

@misc{dhuliawala_chain--verification_2023,
	title = {Chain-of-{Verification} {Reduces} {Hallucination} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2309.11495},
	abstract = {Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. We study the ability of language models to deliberate on the responses they give in order to correct their mistakes. We develop the Chain-of-Verification (CoVe) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. In experiments, we show CoVe decreases hallucinations across a variety of tasks, from list-based questions from Wikidata, closed book MultiSpanQA and longform text generation.},
	urldate = {2023-09-26},
	publisher = {arXiv},
	author = {Dhuliawala, Shehzaad and Komeili, Mojtaba and Xu, Jing and Raileanu, Roberta and Li, Xian and Celikyilmaz, Asli and Weston, Jason},
	month = sep,
	year = {2023},
	note = {arXiv:2309.11495 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\6HVW7SMC\\2309.html:text/html},
}

@misc{liu_generated_2022,
	title = {Generated {Knowledge} {Prompting} for {Commonsense} {Reasoning}},
	url = {http://arxiv.org/abs/2110.08387},
	abstract = {It remains an open question whether incorporating external knowledge benefits commonsense reasoning while maintaining the flexibility of pretrained sequence models. To investigate this question, we develop generated knowledge prompting, which consists of generating knowledge from a language model, then providing the knowledge as additional input when answering a question. Our method does not require task-specific supervision for knowledge integration, or access to a structured knowledge base, yet it improves performance of large-scale, state-of-the-art models on four commonsense reasoning tasks, achieving state-of-the-art results on numerical commonsense (NumerSense), general commonsense (CommonsenseQA 2.0), and scientific commonsense (QASC) benchmarks. Generated knowledge prompting highlights large-scale language models as flexible sources of external knowledge for improving commonsense reasoning. Our code is available at https://github.com/liujch1998/GKP},
	urldate = {2023-09-26},
	publisher = {arXiv},
	author = {Liu, Jiacheng and Liu, Alisa and Lu, Ximing and Welleck, Sean and West, Peter and Bras, Ronan Le and Choi, Yejin and Hajishirzi, Hannaneh},
	month = sep,
	year = {2022},
	note = {arXiv:2110.08387 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: ACL 2022 main conference},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\QWF7MR96\\2110.html:text/html;Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\74ILI65I\\Liu et al. - 2022 - Generated Knowledge Prompting for Commonsense Reas.pdf:application/pdf},
}

@misc{shackell_prompt_2023,
	title = {Prompt engineering: is being an {AI} 'whisperer' the job of the future or a short-lived fad?},
	shorttitle = {Prompt engineering},
	url = {http://theconversation.com/prompt-engineering-is-being-an-ai-whisperer-the-job-of-the-future-or-a-short-lived-fad-211833},
	abstract = {Media articles and influencers have helped give the impression that prompt engineering could be a ticket to a six-figure salary. The reality, as always, is a different story.},
	language = {en},
	urldate = {2023-09-26},
	journal = {The Conversation},
	author = {Shackell, Cameron},
	month = aug,
	year = {2023},
	file = {Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\A6IN6CYE\\prompt-engineering-is-being-an-ai-whisperer-the-job-of-the-future-or-a-short-lived-fad-211833.html:text/html},
}

@misc{martineau_what_2021,
	title = {What is prompt tuning?},
	copyright = {© Copyright IBM Corp. 2021},
	url = {https://research.ibm.com/blog/what-is-ai-prompt-tuning},
	abstract = {It's an efficient, low-cost way of adapting a foundation model to new tasks without retraining the model.},
	language = {en-US},
	urldate = {2023-09-26},
	journal = {IBM Research Blog},
	author = {Martineau, Kim},
	month = feb,
	year = {2021},
	file = {Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\TLSRX94R\\what-is-ai-prompt-tuning.html:text/html},
}

@misc{mann_microsoft_nodate,
	title = {Microsoft limits {Bing} chat exchanges and conversation lengths after 'creepy' interactions with some users},
	url = {https://www.businessinsider.com/microsoft-limits-bing-chat-exchanges-and-conversation-lengths-2023-2},
	abstract = {Bing users will get a prompt to start a new topic once they hit the new limits imposed with the search engine's AI chatbot, Microsoft said.},
	language = {en-US},
	urldate = {2023-09-26},
	journal = {Business Insider},
	author = {Mann, Jyoti},
	file = {Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\MFSIBBQJ\\microsoft-limits-bing-chat-exchanges-and-conversation-lengths-2023-2.html:text/html},
}

@inproceedings{zhou_large_2022,
	title = {Large {Language} {Models} are {Human}-{Level} {Prompt} {Engineers}},
	url = {https://openreview.net/forum?id=92gvk82DE-},
	abstract = {By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the "program," optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 21/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts.},
	language = {en},
	urldate = {2023-09-26},
	author = {Zhou, Yongchao and Muresanu, Andrei Ioan and Han, Ziwen and Paster, Keiran and Pitis, Silviu and Chan, Harris and Ba, Jimmy},
	month = sep,
	year = {2022},
	file = {Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\K2UC5R8B\\Zhou et al. - 2022 - Large Language Models are Human-Level Prompt Engin.pdf:application/pdf},
}

@misc{hulbert_using_2023,
	title = {Using {Tree}-of-{Thought} {Prompting} to boost {ChatGPT}'s reasoning},
	copyright = {MIT},
	url = {https://github.com/dave1010/tree-of-thought-prompting},
	abstract = {Using Tree-of-Thought Prompting to boost ChatGPT's reasoning},
	urldate = {2023-09-26},
	author = {Hulbert, Dave},
	month = sep,
	year = {2023},
	note = {original-date: 2023-05-22T19:03:27Z},
}

@misc{chen_evaluating_2021,
	title = {Evaluating {Large} {Language} {Models} {Trained} on {Code}},
	url = {http://arxiv.org/abs/2107.03374},
	abstract = {We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8\% of the problems, while GPT-3 solves 0\% and GPT-J solves 11.4\%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2\% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.},
	urldate = {2023-09-26},
	publisher = {arXiv},
	author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
	month = jul,
	year = {2021},
	note = {arXiv:2107.03374 [cs]
version: 2},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: corrected typos, added references, added authors, added acknowledgements},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\46D6JMMY\\2107.html:text/html;Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\VTN24YRF\\Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf:application/pdf},
}

@misc{min_rethinking_2022,
	title = {Rethinking the {Role} of {Demonstrations}: {What} {Makes} {In}-{Context} {Learning} {Work}?},
	shorttitle = {Rethinking the {Role} of {Demonstrations}},
	url = {http://arxiv.org/abs/2202.12837},
	abstract = {Large language models (LMs) are able to in-context learn -- perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required -- randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of end task performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.},
	urldate = {2023-09-26},
	publisher = {arXiv},
	author = {Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
	month = oct,
	year = {2022},
	note = {arXiv:2202.12837 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 17 pages; 12 figures. Published as a conference paper at EMNLP 2022 (long). Code available at https://github.com/Alrope123/rethinking-demonstrations},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\KNK3T7S7\\2202.html:text/html;Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\S9E7MHMC\\Min et al. - 2022 - Rethinking the Role of Demonstrations What Makes .pdf:application/pdf},
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2023-09-26},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 40+32 pages},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\FGWYX2HG\\2005.html:text/html;Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\8TELU3ML\\Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf},
}

@misc{flesch_how_2016,
	title = {How to {Write} {Plain} {English}},
	url = {https://web.archive.org/web/20160712094308/http://www.mang.canterbury.ac.nz/writing_guide/writing/flesch.shtml},
	urldate = {2023-09-26},
	author = {Flesch, Rudolf},
	month = jul,
	year = {2016},
	file = {Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\IIWNEDT4\\flesch.html:text/html},
}

@misc{aggarwal_textstat_nodate,
	title = {textstat: {Calculate} statistical features from text},
	copyright = {MIT},
	shorttitle = {textstat},
	url = {https://github.com/shivam5992/textstat},
	urldate = {2023-09-26},
	author = {Aggarwal, Chaitanya, Shivam Bansal},
	keywords = {Text Processing},
	file = {Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\E4TZ4QDP\\textstat.html:text/html},
}

@misc{lacchia_radon_nodate,
	title = {radon: {Code} {Metrics} in {Python}},
	copyright = {MIT License},
	shorttitle = {radon},
	url = {https://radon.readthedocs.org/},
	urldate = {2023-09-26},
	author = {Lacchia, Michele},
	keywords = {analysis,, code,, complexity,, metrics, Software Development, Software Development - Libraries - Python Modules, Software Development - Quality Assurance, static,, Utilities},
	file = {Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\U3HJ4L9D\\radon.html:text/html},
}

@article{gardner_new_2014,
	title = {A {New} {Academic} {Vocabulary} {List}},
	volume = {35},
	issn = {0142-6001},
	url = {https://doi.org/10.1093/applin/amt015},
	doi = {10.1093/applin/amt015},
	abstract = {This article presents our new Academic Vocabulary List (AVL), derived from a 120-million-word academic subcorpus of the 425-million-word Corpus of Contemporary American English (COCA; Davies 2012). We first explore reasons why a new academic core list is warranted, and why such a list is still needed in English language education. We also provide a detailed description of the large academic corpus from which the AVL was derived, as well as the robust frequency and dispersion statistics used to identify the AVL. Our concluding case studies show that the AVL discriminates between academic and other materials, and that it covers ∼14\% of academic materials in both COCA (120 million+ words) and the British National Corpus (33 million+ words). The article concludes with a discussion of how the AVL can be used in settings where academic English is the focus of instruction. In this discussion, we introduce a new web-based interface that can be used to learn AVL words, and to identify and interact with AVL words in any text entered in the search window.},
	number = {3},
	urldate = {2023-09-26},
	journal = {Applied Linguistics},
	author = {Gardner, Dee and Davies, Mark},
	month = jul,
	year = {2014},
	pages = {305--327},
	file = {Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\QWBXRZZF\\Gardner and Davies - 2014 - A New Academic Vocabulary List.pdf:application/pdf;Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\V52UREL4\\146569.html:text/html},
}

@misc{noauthor_semantic_nodate,
	title = {Semantic {Scholar} {\textbar} {AI}-{Powered} {Research} {Tool}},
	url = {https://www.semanticscholar.org/},
	abstract = {Semantic Scholar uses groundbreaking AI and engineering to understand the semantics of scientific literature to help Scholars discover relevant research.},
	language = {en},
	urldate = {2023-10-10},
	file = {Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\SU74PNWR\\www.semanticscholar.org.html:text/html},
}

@misc{gao_prompt_2023,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Prompt {Engineering} for {Large} {Language} {Models}},
	url = {https://papers.ssrn.com/abstract=4504303},
	doi = {10.2139/ssrn.4504303},
	abstract = {With the popularization of software like OpenAI’s ChatGPT and Google’s Bard, large language models (LLMs) have pervaded many aspects of life and work. For instance, ChatGPT can be used to provide customized recipes, suggesting substitutions for missing ingredients. It can be used to draft research proposals, write working code in many programming languages, translate text between languages, assist in policy making, and more (Gao 2023). Users interact with large language models through “prompts'', or natural language instructions. Carefully designed prompts can lead to significantly better outputs.In this review, common strategies for LLM prompt engineering will be explained. Additionally, considerations, recommended resources, and current directions of research on LLM prompt engineering will be discussed. Prompt engineering strategies based on finetuning will not be covered. The goal of this article is to introduce practical and validated prompt engineering techniques to a non-technical audience.},
	language = {en},
	urldate = {2023-10-21},
	author = {Gao, Andrew},
	month = jul,
	year = {2023},
	keywords = {AI, artificial intelligence, chatgpt, large language model, prompt engineering},
	file = {Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\LDEGTRGJ\\Gao - 2023 - Prompt Engineering for Large Language Models.pdf:application/pdf},
}

@misc{fu_complexity-based_2023,
	title = {Complexity-{Based} {Prompting} for {Multi}-{Step} {Reasoning}},
	url = {http://arxiv.org/abs/2210.00720},
	abstract = {We study the task of prompting large-scale language models to perform multi-step reasoning. Existing work shows that when prompted with a chain of thoughts (CoT), sequences of short sentences describing intermediate reasoning steps towards a final answer, large language models can generate new reasoning chains and predict answers for new inputs. A central question is which reasoning examples make the most effective prompts. In this work, we propose complexity-based prompting, a simple and effective example selection scheme for multi-step reasoning. We show that prompts with higher reasoning complexity, i.e., chains with more reasoning steps, achieve substantially better performance on multi-step reasoning tasks over strong baselines. We further extend our complexity-based criteria from prompting (selecting inputs) to decoding (selecting outputs), where we sample multiple reasoning chains from the model, then choose the majority of generated answers from complex reasoning chains (over simple chains). When used to prompt GPT-3 and Codex, our approach substantially improves multi-step reasoning accuracy and achieves new state-of-the-art (SOTA) performance on three math benchmarks (GSM8K, MultiArith, and MathQA) and two BigBenchHard tasks (Date Understanding and Penguins), with an average +5.3 and up to +18 accuracy improvements. Compared with existing example selection schemes like manual tuning or retrieval-based selection, selection based on reasoning complexity is intuitive, easy to implement, and annotation-efficient. Further results demonstrate the robustness of performance gains from complex prompts under format perturbation and distribution shift.},
	urldate = {2023-10-22},
	publisher = {arXiv},
	author = {Fu, Yao and Peng, Hao and Sabharwal, Ashish and Clark, Peter and Khot, Tushar},
	month = jan,
	year = {2023},
	note = {arXiv:2210.00720 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Preprint},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\PGYBJ363\\2210.html:text/html;Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\LFZ8YNNV\\Fu et al. - 2023 - Complexity-Based Prompting for Multi-Step Reasonin.pdf:application/pdf},
}

@misc{shum_automatic_2023,
	title = {Automatic {Prompt} {Augmentation} and {Selection} with {Chain}-of-{Thought} from {Labeled} {Data}},
	url = {http://arxiv.org/abs/2302.12822},
	abstract = {Chain-of-thought prompting (CoT) advances the reasoning abilities of large language models (LLMs) and achieves superior performance in arithmetic, commonsense, and symbolic reasoning tasks. However, most CoT studies rely on carefully designed human-annotated rational chains to prompt the language model, which poses challenges for real-world applications where labeled training data is available without human-annotated rational chains. This creates barriers to applications of CoT prompting to these general tasks. This paper proposes a new strategy, Automate-CoT (Automatic Prompt Augmentation and Selection with Chain-of-Thought), that can bypass human engineering of CoTs by automatically augmenting rational chains from a small labeled dataset, and then pruning low-quality chains to construct a candidate pool of machine-generated rationale chains based on the labels. Finally, it selects the optimal combination of several rationale chains from the pool for CoT prompting by employing a variance-reduced policy gradient strategy to estimate the significance of each example in a black-box language model. Automate-CoT enables a quick adaptation of the CoT technique to different tasks. Experimental results demonstrate the effectiveness of our method, where state-of-the-art results are achieved on arithmetic reasoning (+2.7{\textbackslash}\%), commonsense reasoning (+3.4{\textbackslash}\%), symbolic reasoning (+3.2{\textbackslash}\%), and non-reasoning tasks (+2.5{\textbackslash}\%). Our code will be available at https://github.com/shizhediao/automate-cot.},
	urldate = {2023-10-22},
	publisher = {arXiv},
	author = {Shum, KaShun and Diao, Shizhe and Zhang, Tong},
	month = feb,
	year = {2023},
	note = {arXiv:2302.12822 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 22 pages, 4 figures, 13 tables},
	annote = {Comment: 22 pages, 4 figures, 13 tables},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\JC3349S2\\2302.html:text/html;Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\8VSI6DSX\\Shum et al. - 2023 - Automatic Prompt Augmentation and Selection with C.pdf:application/pdf},
}

@misc{adams_sparse_2023,
	title = {From {Sparse} to {Dense}: {GPT}-4 {Summarization} with {Chain} of {Density} {Prompting}},
	shorttitle = {From {Sparse} to {Dense}},
	url = {http://arxiv.org/abs/2309.04269},
	abstract = {Selecting the ``right'' amount of information to include in a summary is a difficult task. A good summary should be detailed and entity-centric without being overly dense and hard to follow. To better understand this tradeoff, we solicit increasingly dense GPT-4 summaries with what we refer to as a ``Chain of Density'' (CoD) prompt. Specifically, GPT-4 generates an initial entity-sparse summary before iteratively incorporating missing salient entities without increasing the length. Summaries generated by CoD are more abstractive, exhibit more fusion, and have less of a lead bias than GPT-4 summaries generated by a vanilla prompt. We conduct a human preference study on 100 CNN DailyMail articles and find that that humans prefer GPT-4 summaries that are more dense than those generated by a vanilla prompt and almost as dense as human written summaries. Qualitative analysis supports the notion that there exists a tradeoff between informativeness and readability. 500 annotated CoD summaries, as well as an extra 5,000 unannotated summaries, are freely available on HuggingFace (https://huggingface.co/datasets/griffin/chain\_of\_density).},
	urldate = {2023-10-22},
	publisher = {arXiv},
	author = {Adams, Griffin and Fabbri, Alexander and Ladhak, Faisal and Lehman, Eric and Elhadad, Noémie},
	month = sep,
	year = {2023},
	note = {arXiv:2309.04269 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: preprint},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\QZMTMBS8\\2309.html:text/html;Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\ZQW9FTMM\\Adams et al. - 2023 - From Sparse to Dense GPT-4 Summarization with Cha.pdf:application/pdf},
}

@inproceedings{pu_chatgpt_2023,
	address = {Toronto, Canada},
	title = {{ChatGPT} vs {Human}-authored {Text}: {Insights} into {Controllable} {Text} {Summarization} and {Sentence} {Style} {Transfer}},
	shorttitle = {{ChatGPT} vs {Human}-authored {Text}},
	url = {https://aclanthology.org/2023.acl-srw.1},
	doi = {10.18653/v1/2023.acl-srw.1},
	abstract = {Large-scale language models, like ChatGPT, have garnered significant media attention and stunned the public with their remarkable capacity for generating coherent text from short natural language prompts. In this paper, we aim to conduct a systematic inspection of ChatGPT's performance in two controllable generation tasks, with respect to ChatGPT's ability to adapt its output to different target audiences (expert vs. layman) and writing styles (formal vs. informal). Additionally, we evaluate the faithfulness of the generated text, and compare the model's performance with human-authored texts. Our findings indicate that the stylistic variations produced by humans are considerably larger than those demonstrated by ChatGPT, and the generated texts diverge from human samples in several characteristics, such as the distribution of word types. Moreover, we observe that ChatGPT sometimes incorporates factual errors or hallucinations when adapting the text to suit a specific style.},
	urldate = {2023-10-22},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 4: {Student} {Research} {Workshop})},
	publisher = {Association for Computational Linguistics},
	author = {Pu, Dongqi and Demberg, Vera},
	month = jul,
	year = {2023},
	pages = {1--18},
	file = {Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\8WWZQWEK\\Pu and Demberg - 2023 - ChatGPT vs Human-authored Text Insights into Cont.pdf:application/pdf},
}

@misc{imperial_flesch_2023,
	title = {Flesch or {Fumble}? {Evaluating} {Readability} {Standard} {Alignment} of {Instruction}-{Tuned} {Language} {Models}},
	shorttitle = {Flesch or {Fumble}?},
	url = {http://arxiv.org/abs/2309.05454},
	abstract = {Readability metrics and standards such as Flesch Kincaid Grade Level (FKGL) and the Common European Framework of Reference for Languages (CEFR) exist to guide teachers and educators to properly assess the complexity of educational materials before administering them for classroom use. In this study, we select a diverse set of open and closed-source instruction-tuned language models and investigate their performances in writing story completions and simplifying narratives\$-\$tasks that teachers perform\$-\$using standard-guided prompts controlling text readability. Our extensive findings provide empirical proof of how globally recognized models like ChatGPT may be considered less effective and may require more refined prompts for these generative tasks compared to other open-sourced models such as BLOOMZ and FlanT5\$-\$which have shown promising results.},
	urldate = {2023-10-22},
	publisher = {arXiv},
	author = {Imperial, Joseph Marvin and Madabushi, Harish Tayyar},
	month = sep,
	year = {2023},
	note = {arXiv:2309.05454 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\77FB3SIU\\2309.html:text/html;Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\5ELXLRI3\\Imperial and Madabushi - 2023 - Flesch or Fumble Evaluating Readability Standard .pdf:application/pdf},
}

@inproceedings{bhaskar_prompted_2023,
	address = {Toronto, Canada},
	title = {Prompted {Opinion} {Summarization} with {GPT}-3.5},
	url = {https://aclanthology.org/2023.findings-acl.591},
	doi = {10.18653/v1/2023.findings-acl.591},
	abstract = {Large language models have shown impressive performance across a wide variety of tasks, including text summarization. In this paper, we show that this strong performance extends to opinion summarization. We explore several pipeline methods for applying GPT-3.5 to summarize a large collection of user reviews in aprompted fashion. To handle arbitrarily large numbers of user reviews, we explore recursive summarization as well as methods for selecting salient content to summarize through supervised clustering or extraction. On two datasets, an aspect-oriented summarization dataset of hotel reviews (SPACE) and a generic summarization dataset of Amazon and Yelp reviews (FewSum), we show that GPT-3.5 models achieve very strong performance in human evaluation. We argue that standard evaluation metrics do not reflect this, and introduce three new metrics targeting faithfulness, factuality, and genericity to contrast these different methods.},
	urldate = {2023-10-22},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Bhaskar, Adithya and Fabbri, Alex and Durrett, Greg},
	month = jul,
	year = {2023},
	pages = {9282--9300},
	file = {Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\SSHKLZVA\\Bhaskar et al. - 2023 - Prompted Opinion Summarization with GPT-3.5.pdf:application/pdf},
}
