\pdfoutput=1
\documentclass[11pt]{article}
% Remove the "review" option to generate the final version.
\usepackage{ACL2023}
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}

% Set math text size
\DeclareMathSizes{8}{8}{8}{8}

\title{Midterm Report: The Practicality of Prompt Engineering}

\author{Isaac Liu \\
  University of California, Berkeley \\
  \texttt{ijyliu@berkeley.edu}}

\begin{document}
\maketitle
\begin{abstract}
  This paper examines the practicality of prompt engineering in improving the performance of Large Language Models (LLMs). Through empirical analysis, we evaluate the trade-offs between costs and benefits of prompting using novel metrics. Different prompting methods are assessed using standardized tasks and both modern and older models.
\end{abstract}

\section*{Introduction}

Prompt engineering, the practice of developing specialized prompts and queries to improve the accuracy of Large Language Models after training, is a prominent topic of interest in the NLP community, and among the general public. The practice is believed to allow for improvements in LLM performance a variety of domains without investment in underlying training \cite{martineau_what_2021}. It is not, however, without its critics. Some commentators believe that the practice will become irrelevant as models grow larger and more powerful, becoming more capable of directly interpreting a user's intent. \cite{ethan_mollick_emollick_i_2023}. Others question the need for specialized professionals or training to attain minimal improvements which are often not repeatable across contexts \cite{shackell_prompt_2023, acar_ai_2023}. 

Despite such controversy, it is difficult to find empirical analyses of the tradeoff between costs and accuracy benefits associated with advanced prompting. Papers introducing new prompting techniques often only include performance benchmarks concerning the techniques' efficacy, typically within a limited domain. Some authors briefly mention problems associated with human-tailored prompts, such as the increased complexity induced by prompt-chaining and limitations on creativity and randomness \cite{wu_ai_2022}, and others suggest the automation of prompting to avoid these costs \cite{diao_active_2023}, but the extent of these issues has not been (to my knowledge) quantified. Online marketplaces such as \href{https://promptbase.com/}{Promptbase} provide input token costs for prompt texts sold on the platform, but do not provide any other information.

Briefly some practical considerations for prompt engineering 
On the cost side there are token costs, the filling of context window lengths leading to lower performance quality
Dubious benefits due to a lack of research for some anecdotal techniques and decreased effectiveness with larger models
but does not quantify these measures for specific techniques
https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4504303

The quality, length, and complexity of LLM responses have been analyzed within several individual task and technique domains. 

Some research with GPT-3 series models on math and non-math reasoning tasks suggests the addition of complexity through the introduction of extra reasoning steps for both input prompts and output responses improves performance when using chain-of-thought prompting techniques https://browse.arxiv.org/pdf/2210.00720.pdf. Effects are on the magnitude of several points of accuracy per added step, with generally low costs as long as prompt examples are selected carefully. Improvements from complex chain-of-thought prompting are not fully accepted in the literature, however - some research has noted a tendency for the method to lead to degraded performance on simple questions https://browse.arxiv.org/pdf/2302.12822.pdf.

Costs and complexity are important!

The Chain of Density prompting technique seeks to optimize the named entity density of generated summaries through choice from a series of repeated, increasingly dense iterations https://browse.arxiv.org/pdf/2309.04269.pdf. Human preferences tend to align with 0.1 - 0.15 named entities per token, a point near the middle of the usual sequence of generations, demonstrating the existence of a tradeoff between informativeness and clarity.

Response to why can't you just ask for a certain level of length/complexity:
It is difficult to control language model output length/complexity on tasks:
Research also demonstrates current models are not yet able to achieve compliance with desired readability and complexity instructions for the tasks of story generation, simplification, and summarization, though a small amount of improvement is achievable through careful prompt word choice and the use of few-shot examples https://aclanthology.org/2023.acl-srw.1.pdf https://browse.arxiv.org/pdf/2309.05454.pdf.

This paper uses several metrics to evaluate the costs of prompt engineering methods systematically, and analyzes the tradeoffs inherent in their application to standardized data. Such an assessment is valuable on several dimensions. Beyond quantifiably testing the practicality of prompt engineering as a whole, it can be used to compare the performance of different approaches, useful in a world where so many competing techniques are available. I also offer a new look at these prompting methods in a period long after ideas were introduced and in an enviroment with greater capabilities in underlying models. Finally, I introduce and adapt some useful measures of costs and complexity, such as the ratio of interaction length with prompting to the length of an accepted human-generated answer, to the challenge of LLM evaluation.

I provide a newly constructed dataset summarizing the wide variety of existing techniques and data on their popularity as measured by Semantic Scholar citations, which may be useful for future surveys of the field

\section*{Prompting Methods Assessed}

This list is subject to change, pending further assessment of popularity, implementation difficulty, and potential accuracy gains for each method. Higher accuracy methods are likely to be more interesting.

The below items are listed in order of increasing complexity/human intervention:

\begin{itemize}
  \item Zero-Shot Control Baseline/Direct Prompting: Providing the question/task directly.
  \item Zero-Shot Chain of Thought Prompting: Existing literature mentions several examples of this; a simple one to append to every initial question/task, found to be optimal through automated testing is "Let's work this out in a step by step way to be sure we have the right answer." \cite{hebenstreit_automatically_2023, zhou_large_2022}
  \item Tree of Thought Prompting: This prepends the following to the task/question: "Imagine three different experts are answering this question. All experts will write down 1 step of their thinking, then share it with the group. Then all experts will go on to the next step, etc. If any expert realises they're wrong at any point then they leave. The question is..." \cite{hulbert_using_2023}
  \item Generated Knowledge Prompting: The LLM is first prompted to generate some related facts and knowledge about the task/question. It is then prompted to answer the task/question. \cite{liu_generated_2022} The knowledge generating and task/question models could be different, but for my purposes I use the same model.
  \item Chain of Verification Prompting: The LLM is prompted a series of times to produce an initial baseline response, write its own verification questions, answer those verification questions, and write a final, verified response. \cite{dhuliawala_chain--verification_2023}
  \item Few-Shot Prompting: The prompter provides a few examples of successfully/answered questions or tasks before the main question/task. Despite the work involved in implementing this method, I believe it has potential to be effective for two reasons. First, prior evidence indicates that larger and more modern language models benefit more from few-shot learning, potentially making this a consistently useful technique. \cite{brown_language_2020} Second, earlier research has found that the formatting and input/label space distribution is more important than example correctness, meaning this method is somewhat robust to human error. \cite{min_rethinking_2022}
\end{itemize}

\section*{Metrics}

\subsection*{Accuracy}

Improved accuracy can be a benefit of prompt engineering. I report:

\begin{itemize}
  \item Correct/Incorrect accuracy at the point a technique has been fully implemented (the end of the chain of thought, etc., modelling the real world)
\end{itemize}

\subsection*{Length}

The length of responses and interactions can effect the practicality of prompting. It could indicate that a model is carefully and correctly solving through the steps of a problem (though it may actually be a confounder of other factors in such cases https://browse.arxiv.org/pdf/2210.00720.pdf). It can also impose time and financial costs to users, or become an indicator of degraded performance as models sometimes tend to go off on tangents or repeat themselves (to the extent some platforms have imposed length limitations) \cite{mann_microsoft_nodate}. I report:

\begin{itemize}
  \item Length of the entire interaction in tokens
  \item Financial cost of the entire interaction in tokens
  \item Length of the entire interaction in tokens relative to the length of the task/question + a human/solved out/generally accepted as correct answer (or relative to direct prompting). How much is prompt engineering stretching the interaction out? This ratio can be informative.
  \item The change in accuracy (in percentage points, 0 to 100) divided by the change in tokens (difference in token counts), between the prompt engineering technique and direct prompting. Is any stretching of output adding value/improving accuracy? % token cost of accuracy
  \begin{displaymath}
    \frac{Accuracy_{PE} - Accuracy_{B}}{Tokens_{PE} - Tokens_{B}}
  \end{displaymath}
  \item Length of the entire interaction in time (seconds). This can include time writing a response, waiting for a response, or reviewing a response. More granular data on these each of the component steps may be hard to collect, but it might be possible to look at human assessments of time spent on these activities. Attempts will be made to have queries to models made at a consistent time during off-peak hours to minimize confounding due to server load, connectivity issues, etc.
\end{itemize}

\subsection*{Complexity}

Similarly to length, complexity could be an indicator of high accuracy. However, it has substantial costs in potentially making review of LLM output more difficult, and on simple questions it may even lead to degraded accuracy https://browse.arxiv.org/pdf/2302.12822.pdf. I report:

\begin{itemize}
  \item Vocabulary - share of words on the Academic Vocabulary List (AVL) for natural language and non-code components of responses. \cite{gardner_new_2014} Share of novel n-grams in the response (words not in the prompt), presence of contrasting words {'while', 'but', 'though', 'although', 'other', 'others', 'however'}. https://aclanthology.org/2023.findings-acl.591.pdf
  \item Number of named entities https://browse.arxiv.org/pdf/2309.04269.pdf
  \item Number of reasoning steps - linebreaks, periods, "step i" strings, and semicolons serve as separators. https://browse.arxiv.org/pdf/2210.00720.pdf
  \item Sentence length and Flesch reading ease (implemented via the textstat Python package) for natural language and non-code components of responses. \cite{flesch_how_2016, aggarwal_textstat_nodate}
  \item Cyclomatic complexity for code responses (implemented via the radon Python package). \cite{lacchia_radon_nodate}
  \item Ratio or difference of these scores in prompts vs. responses, responses vs. accepted/outside correct answer
  \item Human assessment of need for specialized knowledge/difficulty of implementation of the technique. This could be task specific (done for some novel real world example questions/prompting scenarios), or it could be done overall based on a pre-existing description of the technique. Perhaps a balance of both is best. The actual metric will be a numeric score and a qualitative description.
  \item Human assessment of output complexity (ease of evaluating results). This could be task specific (done for some novel real world example questions/prompting scenarios), or it could be done overall based on pre-existing examples of the technique. Perhaps a balance of both is best. The actual metric will be a numeric score and a qualitative description.
\end{itemize}

\section*{Data}

To evaluate performance, I attempt to use tasks that are both general-purpose and close to practical, real-world applications. In this spirit, I use GRE General Test questions (from a purchased prep book or practice exams recently published online to minimize contamination), as well as HumanEval coding problems. For multiple choice choice questions, I will perform a string search, or use another (very capable) LLM, or perform manual interpretation to determine the letter answer the model selected based on its response (experimentation will be necessary before picking a method to use here). HumanEval uses the pass@k metric to automatically assess the probability a code solution is correct given a set of unit tests. \cite{chen_evaluating_2021}

I perform the analysis on one cutting-edge model and one older model, closer to the time that these techniques were introduced. This will provide a picture of the changing costs and benefits of advanced prompting, a trend that may even be extrapolated into the future if current LLM scaling laws continue to hold. As the most widely used models and the ones behind much original work in the field, I select two models from the OpenAI series: GPT-4 and text-davinci-003 on OpenAI playground. Should text-davinci-003 (a legacy model) become unavailable during the course of the project, or should I encounter other difficulties, I will use the simplest/smallest model available, likely GPT-3.5 (something to note is that models older than GPT-3.5 have, in the past, scored 0\% for accuracy on coding problems - I will need to test all of my evaluations quickly to get a sense of feasible model choices before scaling up). All of these models are available both via the OpenAI API and in web interfaces - where possible I will use web interfaces to limit resources required.

To the extent possible, I will also report accuracy scores on the domain dataset as they are in the original paper introducing each technique. It may also be possible to use any prompts, LLM responses, and correct responses provided along with original papers to calculate other simple metrics such as response lengths and complexity. However, some metrics (time taken, human assessments of complexity) will require my own evaluations.

\section*{Analyses}

I provide summary statistics of the metrics for each prompting method by model by question/task type. In cases where human/textual assessment and comments have been provided, it might be interesting to use NLP methods to evaluate responses (ex: for sentiment).

\section*{Limitations}

It was difficult to select prompt engineering methods to try for this paper, and there is potential for my choice of methods to be somewhat biased. I mostly picked methods based my perception of their popularity and ease of implementation. If anything, this may lead to an underestimation of costs.

Just as my evaluation comes at a time with significantly more capable LLMs relative to those available when much work began on prompting, I expect the underlying calculus concerning prompting to continue to change in the future. However, I again only expect relative costs of complex engineering to increase as models get better.

Another potential problem is the extent that prompting techniques have been absorbed into default LLM behavior, likely through reinforcement learning. GPT-4 in particular does seem to automatically implement chain-of-thought methods when presented with a sufficiently complex problem. In this environment, this paper become less of an evaluation of prompting techniques themselves, but more of an evaluation of their intentional and manual implementation.

Finally, though I have taken steps to limit it, data contamination remains a real concern. The questions/tasks I use are unlikely to have been used in pretraining, but they may have been introduced to LLMs through reinforcement learning and other evaluations. On the other hand, this seems unlikely to bias the results for any one particular prompting method relative to the others or versus the control/direct prompting - comparisons internal to this paper are still likely to be useful.

\section*{Acknowledgements}
The template for this document was adapted by Jordan Boyd-Graber, Naoaki Okazaki, and Anna Rogers.

\bibliography{custom}
\bibliographystyle{acl_natbib}

\appendix

\section{The Popularity of Some Prompting Methods}

I used Semantic Scholar \cite{noauthor_semantic_nodate} to compile the following statistics concerning the popularity of X_NUMBER general-purpose prompt engineering methods in the collection at \url{https://www.promptingguide.ai/papers#approaches} on October 8, 2023. Here are the top 50 papers and their methods in terms of citations per day.

(See Internet Archive for list of paper collection stablelink)

\end{document}
