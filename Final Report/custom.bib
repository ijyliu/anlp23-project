
@misc{openai_gpt-4_2023,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	urldate = {2023-09-25},
	publisher = {arXiv},
	author = {OpenAI},
	month = mar,
	year = {2023},
	note = {arXiv:2303.08774 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 100 pages},
	annote = {Comment: 100 pages},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\ZLQQGQUP\\2303.html:text/html;Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\LM72BFL3\\OpenAI - 2023 - GPT-4 Technical Report.pdf:application/pdf},
}

@misc{cheng_is_2023,
	title = {Is {GPT}-4 a {Good} {Data} {Analyst}?},
	url = {http://arxiv.org/abs/2305.15038},
	abstract = {As large language models (LLMs) have demonstrated their powerful capabilities in plenty of domains and tasks, including context understanding, code generation, language generation, data storytelling, etc., many data analysts may raise concerns if their jobs will be replaced by AI. This controversial topic has drawn a lot of attention in public. However, we are still at a stage of divergent opinions without any definitive conclusion. Motivated by this, we raise the research question of "is GPT-4 a good data analyst?" in this work and aim to answer it by conducting head-to-head comparative studies. In detail, we regard GPT-4 as a data analyst to perform end-to-end data analysis with databases from a wide range of domains. We propose a framework to tackle the problems by carefully designing the prompts for GPT-4 to conduct experiments. We also design several task-specific evaluation metrics to systematically compare the performance between several professional human data analysts and GPT-4. Experimental results show that GPT-4 can achieve comparable performance to humans. We also provide in-depth discussions about our results to shed light on further studies before we reach the conclusion that GPT-4 can replace data analysts.},
	urldate = {2023-09-25},
	publisher = {arXiv},
	author = {Cheng, Liying and Li, Xingxuan and Bing, Lidong},
	month = may,
	year = {2023},
	note = {arXiv:2305.15038 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 11 pages, 2 figures},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\4N77GWHJ\\2305.html:text/html;Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\QDTTIL6T\\Cheng et al. - 2023 - Is GPT-4 a Good Data Analyst.pdf:application/pdf},
}

@article{roose_conversation_2023,
	chapter = {Technology},
	title = {A {Conversation} {With} {Bing}’s {Chatbot} {Left} {Me} {Deeply} {Unsettled}},
	issn = {0362-4331},
	url = {https://www.nytimes.com/2023/02/16/technology/bing-chatbot-microsoft-chatgpt.html},
	abstract = {A very strange conversation with the chatbot built into Microsoft’s search engine led to it declaring its love for me.},
	language = {en-US},
	urldate = {2023-09-25},
	journal = {The New York Times},
	author = {Roose, Kevin},
	month = feb,
	year = {2023},
	keywords = {Artificial Intelligence, ChatGPT, Computers and the Internet, Conversation, internal-sub-only, Microsoft Corp, OpenAI Labs, Search Engines},
	file = {Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\XQCJKDEG\\bing-chatbot-microsoft-chatgpt.html:text/html},
}

@misc{ethan_mollick_emollick_i_2023,
	type = {Tweet},
	title = {I have a strong suspicion that “prompt engineering” is not going to be a big deal in the long-term \& prompt engineer is not the job of the future {AI} gets easier. {You} can already see in {Midjourney} how basic prompts went from complex in v3 to easy in v4. {Same} with {ChatGPT} to {Bing}. https://t.co/{BTtSN4oVF4}},
	url = {https://twitter.com/emollick/status/1627804798224580608},
	language = {en},
	urldate = {2023-09-26},
	journal = {Twitter},
	author = {{Ethan Mollick [@emollick]}},
	month = feb,
	year = {2023},
	file = {Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\EUFKYTRM\\1627804798224580608.html:text/html},
}

@inproceedings{wu_ai_2022,
	address = {New Orleans LA USA},
	title = {{AI} {Chains}: {Transparent} and {Controllable} {Human}-{AI} {Interaction} by {Chaining} {Large} {Language} {Model} {Prompts}},
	isbn = {978-1-4503-9157-3},
	shorttitle = {{AI} {Chains}},
	url = {https://dl.acm.org/doi/10.1145/3491102.3517582},
	doi = {10.1145/3491102.3517582},
	language = {en},
	urldate = {2023-09-26},
	booktitle = {{CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Wu, Tongshuang and Terry, Michael and Cai, Carrie Jun},
	month = apr,
	year = {2022},
	pages = {1--22},
	file = {Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\G3NM64QK\\Wu et al. - 2022 - AI Chains Transparent and Controllable Human-AI I.pdf:application/pdf},
}

@article{acar_ai_2023,
	title = {{AI} {Prompt} {Engineering} {Isn}’t the {Future}},
	issn = {0017-8012},
	url = {https://hbr.org/2023/06/ai-prompt-engineering-isnt-the-future},
	abstract = {Despite the buzz surrounding it, the prominence of prompt engineering may be fleeting. A more enduring and adaptable skill will keep enabling us to harness the potential of generative AI? It is called problem formulation — the ability to identify, analyze, and delineate problems.},
	urldate = {2023-09-26},
	journal = {Harvard Business Review},
	author = {Acar, Oguz A.},
	month = jun,
	year = {2023},
	note = {Section: Technology and analytics},
	keywords = {AI and machine learning, Algorithms, Analytics and data science, Automation, Cybersecurity and digital privacy, Data management, Enterprise computing, Information management, Technology and analytics, Web-based technologies},
	file = {Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\I72G55WX\\ai-prompt-engineering-isnt-the-future.html:text/html},
}

@misc{diao_active_2023,
	title = {Active {Prompting} with {Chain}-of-{Thought} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2302.12246},
	abstract = {The increasing scale of large language models (LLMs) brings emergent abilities to various complex tasks requiring reasoning, such as arithmetic and commonsense reasoning. It is known that the effective design of task-specific prompts is critical for LLMs' ability to produce high-quality answers. In particular, an effective approach for complex question-and-answer tasks is example-based prompting with chain-of-thought (CoT) reasoning, which significantly improves the performance of LLMs. However, current CoT methods rely on a fixed set of human-annotated exemplars, which are not necessarily the most effective examples for different tasks. This paper proposes a new method, Active-Prompt, to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning). For this purpose, we propose a solution to the key problem of determining which questions are the most important and helpful ones to annotate from a pool of task-specific queries. By borrowing ideas from the related problem of uncertainty-based active learning, we introduce several metrics to characterize the uncertainty so as to select the most uncertain questions for annotation. Experimental results demonstrate the superiority of our proposed method, achieving state-of-the-art on eight complex reasoning tasks. Further analyses of different uncertainty metrics, pool sizes, zero-shot learning, and accuracy-uncertainty relationship demonstrate the effectiveness of our method. Our code will be available at https://github.com/shizhediao/active-prompt.},
	urldate = {2023-09-26},
	publisher = {arXiv},
	author = {Diao, Shizhe and Wang, Pengcheng and Lin, Yong and Zhang, Tong},
	month = may,
	year = {2023},
	note = {arXiv:2302.12246 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 20 pages, 3 figures, 11 tables},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\WSCU8EX6\\2302.html:text/html;Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\EZGKACIN\\Diao et al. - 2023 - Active Prompting with Chain-of-Thought for Large L.pdf:application/pdf},
}

@misc{noauthor_promptbase_nodate,
	title = {{PromptBase}},
	shorttitle = {{PromptBase} {\textbar} {Prompt} {Marketplace}},
	url = {https://promptbase.com},
	abstract = {Search 100,000+ quality AI prompts from top prompt engineers. Produce better outputs, save on time \& API costs, sell your own prompts.},
	language = {en},
	urldate = {2023-09-26},
}

@misc{hebenstreit_automatically_2023,
	title = {An automatically discovered chain-of-thought prompt generalizes to novel models and datasets},
	url = {http://arxiv.org/abs/2305.02897},
	abstract = {Emergent chain-of-thought (CoT) reasoning capabilities promise to improve performance and explainability of large language models (LLMs). However, uncertainties remain about how reasoning strategies formulated for previous model generations generalize to new model generations and different datasets. In this small-scale study, we compare different reasoning strategies induced by zero-shot prompting across six recently released LLMs (davinci-002, davinci-003, GPT-3.5-turbo, GPT-4, Flan-T5-xxl and Cohere command-xlarge) on a mixture of six question-answering datasets, including datasets from scientific and medical domains. Our findings demonstrate that while some variations in effectiveness occur, gains from CoT reasoning strategies remain robust across different models and datasets. GPT-4 has the most benefit from current state-of-the-art reasoning strategies and exhibits the best performance by applying a prompt previously discovered through automated discovery.},
	urldate = {2023-09-26},
	publisher = {arXiv},
	author = {Hebenstreit, Konstantin and Praas, Robert and Kiesewetter, Louis P. and Samwald, Matthias},
	month = aug,
	year = {2023},
	note = {arXiv:2305.02897 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\EZIPW94R\\2305.html:text/html;Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\9DQ7M9VQ\\Hebenstreit et al. - 2023 - An automatically discovered chain-of-thought promp.pdf:application/pdf},
}

@article{wei_chain--thought_nodate,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	abstract = {We explore how generating a chain of thought—a series of intermediate reasoning steps—signiﬁcantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufﬁciently large language models via a simple method called chain-ofthought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even ﬁnetuned GPT-3 with a veriﬁer.},
	language = {en},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed H and Le, Quoc V and Zhou, Denny},
	file = {Wei et al. - Chain-of-Thought Prompting Elicits Reasoning in La.pdf:C\:\\Users\\ijyli\\Zotero\\storage\\8AG6AWPT\\Wei et al. - Chain-of-Thought Prompting Elicits Reasoning in La.pdf:application/pdf},
}

@misc{zhang_automatic_2022,
	title = {Automatic {Chain} of {Thought} {Prompting} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2210.03493},
	abstract = {Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like "Let's think step by step" to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the "Let's think step by step" prompt to generate reasoning chains for demonstrations one by one, i.e., let's think not just step by step, but also one by one. However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at https://github.com/amazon-research/auto-cot},
	urldate = {2023-09-26},
	publisher = {arXiv},
	author = {Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Smola, Alex},
	month = oct,
	year = {2022},
	note = {arXiv:2210.03493 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\8TAIA3HA\\2210.html:text/html;Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\BREVBII4\\Zhang et al. - 2022 - Automatic Chain of Thought Prompting in Large Lang.pdf:application/pdf},
}

@misc{dhuliawala_chain--verification_2023,
	title = {Chain-of-{Verification} {Reduces} {Hallucination} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2309.11495},
	abstract = {Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. We study the ability of language models to deliberate on the responses they give in order to correct their mistakes. We develop the Chain-of-Verification (CoVe) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. In experiments, we show CoVe decreases hallucinations across a variety of tasks, from list-based questions from Wikidata, closed book MultiSpanQA and longform text generation.},
	urldate = {2023-09-26},
	publisher = {arXiv},
	author = {Dhuliawala, Shehzaad and Komeili, Mojtaba and Xu, Jing and Raileanu, Roberta and Li, Xian and Celikyilmaz, Asli and Weston, Jason},
	month = sep,
	year = {2023},
	note = {arXiv:2309.11495 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\6HVW7SMC\\2309.html:text/html},
}

@misc{liu_generated_2022,
	title = {Generated {Knowledge} {Prompting} for {Commonsense} {Reasoning}},
	url = {http://arxiv.org/abs/2110.08387},
	abstract = {It remains an open question whether incorporating external knowledge benefits commonsense reasoning while maintaining the flexibility of pretrained sequence models. To investigate this question, we develop generated knowledge prompting, which consists of generating knowledge from a language model, then providing the knowledge as additional input when answering a question. Our method does not require task-specific supervision for knowledge integration, or access to a structured knowledge base, yet it improves performance of large-scale, state-of-the-art models on four commonsense reasoning tasks, achieving state-of-the-art results on numerical commonsense (NumerSense), general commonsense (CommonsenseQA 2.0), and scientific commonsense (QASC) benchmarks. Generated knowledge prompting highlights large-scale language models as flexible sources of external knowledge for improving commonsense reasoning. Our code is available at https://github.com/liujch1998/GKP},
	urldate = {2023-09-26},
	publisher = {arXiv},
	author = {Liu, Jiacheng and Liu, Alisa and Lu, Ximing and Welleck, Sean and West, Peter and Bras, Ronan Le and Choi, Yejin and Hajishirzi, Hannaneh},
	month = sep,
	year = {2022},
	note = {arXiv:2110.08387 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: ACL 2022 main conference},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\QWF7MR96\\2110.html:text/html;Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\74ILI65I\\Liu et al. - 2022 - Generated Knowledge Prompting for Commonsense Reas.pdf:application/pdf},
}

@misc{shackell_prompt_2023,
	title = {Prompt engineering: is being an {AI} 'whisperer' the job of the future or a short-lived fad?},
	shorttitle = {Prompt engineering},
	url = {http://theconversation.com/prompt-engineering-is-being-an-ai-whisperer-the-job-of-the-future-or-a-short-lived-fad-211833},
	abstract = {Media articles and influencers have helped give the impression that prompt engineering could be a ticket to a six-figure salary. The reality, as always, is a different story.},
	language = {en},
	urldate = {2023-09-26},
	journal = {The Conversation},
	author = {Shackell, Cameron},
	month = aug,
	year = {2023},
	file = {Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\A6IN6CYE\\prompt-engineering-is-being-an-ai-whisperer-the-job-of-the-future-or-a-short-lived-fad-211833.html:text/html},
}

@misc{martineau_what_2021,
	title = {What is prompt tuning?},
	copyright = {© Copyright IBM Corp. 2021},
	url = {https://research.ibm.com/blog/what-is-ai-prompt-tuning},
	abstract = {It's an efficient, low-cost way of adapting a foundation model to new tasks without retraining the model.},
	language = {en-US},
	urldate = {2023-09-26},
	journal = {IBM Research Blog},
	author = {Martineau, Kim},
	month = feb,
	year = {2021},
	file = {Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\TLSRX94R\\what-is-ai-prompt-tuning.html:text/html},
}

@misc{mann_microsoft_nodate,
	title = {Microsoft limits {Bing} chat exchanges and conversation lengths after 'creepy' interactions with some users},
	url = {https://www.businessinsider.com/microsoft-limits-bing-chat-exchanges-and-conversation-lengths-2023-2},
	abstract = {Bing users will get a prompt to start a new topic once they hit the new limits imposed with the search engine's AI chatbot, Microsoft said.},
	language = {en-US},
	urldate = {2023-09-26},
	journal = {Business Insider},
	author = {Mann, Jyoti},
	file = {Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\MFSIBBQJ\\microsoft-limits-bing-chat-exchanges-and-conversation-lengths-2023-2.html:text/html},
}

@inproceedings{zhou_large_2022,
	title = {Large {Language} {Models} are {Human}-{Level} {Prompt} {Engineers}},
	url = {https://openreview.net/forum?id=92gvk82DE-},
	abstract = {By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the "program," optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 21/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts.},
	language = {en},
	urldate = {2023-09-26},
	author = {Zhou, Yongchao and Muresanu, Andrei Ioan and Han, Ziwen and Paster, Keiran and Pitis, Silviu and Chan, Harris and Ba, Jimmy},
	month = sep,
	year = {2022},
	file = {Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\K2UC5R8B\\Zhou et al. - 2022 - Large Language Models are Human-Level Prompt Engin.pdf:application/pdf},
}

@misc{hulbert_using_2023,
	title = {Using {Tree}-of-{Thought} {Prompting} to boost {ChatGPT}'s reasoning},
	copyright = {MIT},
	url = {https://github.com/dave1010/tree-of-thought-prompting},
	abstract = {Using Tree-of-Thought Prompting to boost ChatGPT's reasoning},
	urldate = {2023-09-26},
	author = {Hulbert, Dave},
	month = sep,
	year = {2023},
	note = {original-date: 2023-05-22T19:03:27Z},
}

@misc{chen_evaluating_2021,
	title = {Evaluating {Large} {Language} {Models} {Trained} on {Code}},
	url = {http://arxiv.org/abs/2107.03374},
	abstract = {We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8\% of the problems, while GPT-3 solves 0\% and GPT-J solves 11.4\%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2\% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.},
	urldate = {2023-09-26},
	publisher = {arXiv},
	author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
	month = jul,
	year = {2021},
	note = {arXiv:2107.03374 [cs]
version: 2},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: corrected typos, added references, added authors, added acknowledgements},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\46D6JMMY\\2107.html:text/html;Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\VTN24YRF\\Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf:application/pdf},
}

@misc{min_rethinking_2022,
	title = {Rethinking the {Role} of {Demonstrations}: {What} {Makes} {In}-{Context} {Learning} {Work}?},
	shorttitle = {Rethinking the {Role} of {Demonstrations}},
	url = {http://arxiv.org/abs/2202.12837},
	abstract = {Large language models (LMs) are able to in-context learn -- perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required -- randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of end task performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.},
	urldate = {2023-09-26},
	publisher = {arXiv},
	author = {Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
	month = oct,
	year = {2022},
	note = {arXiv:2202.12837 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 17 pages; 12 figures. Published as a conference paper at EMNLP 2022 (long). Code available at https://github.com/Alrope123/rethinking-demonstrations},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\KNK3T7S7\\2202.html:text/html;Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\S9E7MHMC\\Min et al. - 2022 - Rethinking the Role of Demonstrations What Makes .pdf:application/pdf},
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2023-09-26},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 40+32 pages},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\FGWYX2HG\\2005.html:text/html;Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\8TELU3ML\\Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf},
}

@misc{flesch_how_2016,
	title = {How to {Write} {Plain} {English}},
	url = {https://web.archive.org/web/20160712094308/http://www.mang.canterbury.ac.nz/writing_guide/writing/flesch.shtml},
	urldate = {2023-09-26},
	author = {Flesch, Rudolf},
	month = jul,
	year = {2016},
	file = {Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\IIWNEDT4\\flesch.html:text/html},
}

@misc{aggarwal_textstat_nodate,
	title = {textstat: {Calculate} statistical features from text},
	copyright = {MIT},
	shorttitle = {textstat},
	url = {https://github.com/shivam5992/textstat},
	urldate = {2023-09-26},
	author = {Aggarwal, Chaitanya, Shivam Bansal},
	keywords = {Text Processing},
	file = {Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\E4TZ4QDP\\textstat.html:text/html},
}

@misc{lacchia_radon_nodate,
	title = {radon: {Code} {Metrics} in {Python}},
	copyright = {MIT License},
	shorttitle = {radon},
	url = {https://radon.readthedocs.org/},
	urldate = {2023-09-26},
	author = {Lacchia, Michele},
	keywords = {analysis,, code,, complexity,, metrics, Software Development, Software Development - Libraries - Python Modules, Software Development - Quality Assurance, static,, Utilities},
	file = {Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\U3HJ4L9D\\radon.html:text/html},
}

@article{gardner_new_2014,
	title = {A {New} {Academic} {Vocabulary} {List}},
	volume = {35},
	issn = {0142-6001},
	url = {https://doi.org/10.1093/applin/amt015},
	doi = {10.1093/applin/amt015},
	abstract = {This article presents our new Academic Vocabulary List (AVL), derived from a 120-million-word academic subcorpus of the 425-million-word Corpus of Contemporary American English (COCA; Davies 2012). We first explore reasons why a new academic core list is warranted, and why such a list is still needed in English language education. We also provide a detailed description of the large academic corpus from which the AVL was derived, as well as the robust frequency and dispersion statistics used to identify the AVL. Our concluding case studies show that the AVL discriminates between academic and other materials, and that it covers ∼14\% of academic materials in both COCA (120 million+ words) and the British National Corpus (33 million+ words). The article concludes with a discussion of how the AVL can be used in settings where academic English is the focus of instruction. In this discussion, we introduce a new web-based interface that can be used to learn AVL words, and to identify and interact with AVL words in any text entered in the search window.},
	number = {3},
	urldate = {2023-09-26},
	journal = {Applied Linguistics},
	author = {Gardner, Dee and Davies, Mark},
	month = jul,
	year = {2014},
	pages = {305--327},
	file = {Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\QWBXRZZF\\Gardner and Davies - 2014 - A New Academic Vocabulary List.pdf:application/pdf;Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\V52UREL4\\146569.html:text/html},
}

@misc{semantic_scholar_semantic_nodate,
	title = {Semantic {Scholar} {\textbar} {AI}-{Powered} {Research} {Tool}},
	url = {https://www.semanticscholar.org/},
	abstract = {Semantic Scholar uses groundbreaking AI and engineering to understand the semantics of scientific literature to help Scholars discover relevant research.},
	language = {en},
	urldate = {2023-10-10},
	author = {{Semantic Scholar}},
	file = {Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\SU74PNWR\\www.semanticscholar.org.html:text/html},
}

@misc{gao_prompt_2023,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Prompt {Engineering} for {Large} {Language} {Models}},
	url = {https://papers.ssrn.com/abstract=4504303},
	doi = {10.2139/ssrn.4504303},
	abstract = {With the popularization of software like OpenAI’s ChatGPT and Google’s Bard, large language models (LLMs) have pervaded many aspects of life and work. For instance, ChatGPT can be used to provide customized recipes, suggesting substitutions for missing ingredients. It can be used to draft research proposals, write working code in many programming languages, translate text between languages, assist in policy making, and more (Gao 2023). Users interact with large language models through “prompts'', or natural language instructions. Carefully designed prompts can lead to significantly better outputs.In this review, common strategies for LLM prompt engineering will be explained. Additionally, considerations, recommended resources, and current directions of research on LLM prompt engineering will be discussed. Prompt engineering strategies based on finetuning will not be covered. The goal of this article is to introduce practical and validated prompt engineering techniques to a non-technical audience.},
	language = {en},
	urldate = {2023-10-21},
	author = {Gao, Andrew},
	month = jul,
	year = {2023},
	keywords = {AI, artificial intelligence, chatgpt, large language model, prompt engineering},
	file = {Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\LDEGTRGJ\\Gao - 2023 - Prompt Engineering for Large Language Models.pdf:application/pdf},
}

@misc{fu_complexity-based_2023,
	title = {Complexity-{Based} {Prompting} for {Multi}-{Step} {Reasoning}},
	url = {http://arxiv.org/abs/2210.00720},
	abstract = {We study the task of prompting large-scale language models to perform multi-step reasoning. Existing work shows that when prompted with a chain of thoughts (CoT), sequences of short sentences describing intermediate reasoning steps towards a final answer, large language models can generate new reasoning chains and predict answers for new inputs. A central question is which reasoning examples make the most effective prompts. In this work, we propose complexity-based prompting, a simple and effective example selection scheme for multi-step reasoning. We show that prompts with higher reasoning complexity, i.e., chains with more reasoning steps, achieve substantially better performance on multi-step reasoning tasks over strong baselines. We further extend our complexity-based criteria from prompting (selecting inputs) to decoding (selecting outputs), where we sample multiple reasoning chains from the model, then choose the majority of generated answers from complex reasoning chains (over simple chains). When used to prompt GPT-3 and Codex, our approach substantially improves multi-step reasoning accuracy and achieves new state-of-the-art (SOTA) performance on three math benchmarks (GSM8K, MultiArith, and MathQA) and two BigBenchHard tasks (Date Understanding and Penguins), with an average +5.3 and up to +18 accuracy improvements. Compared with existing example selection schemes like manual tuning or retrieval-based selection, selection based on reasoning complexity is intuitive, easy to implement, and annotation-efficient. Further results demonstrate the robustness of performance gains from complex prompts under format perturbation and distribution shift.},
	urldate = {2023-10-22},
	publisher = {arXiv},
	author = {Fu, Yao and Peng, Hao and Sabharwal, Ashish and Clark, Peter and Khot, Tushar},
	month = jan,
	year = {2023},
	note = {arXiv:2210.00720 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Preprint},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\PGYBJ363\\2210.html:text/html;Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\LFZ8YNNV\\Fu et al. - 2023 - Complexity-Based Prompting for Multi-Step Reasonin.pdf:application/pdf},
}

@misc{shum_automatic_2023,
	title = {Automatic {Prompt} {Augmentation} and {Selection} with {Chain}-of-{Thought} from {Labeled} {Data}},
	url = {http://arxiv.org/abs/2302.12822},
	abstract = {Chain-of-thought prompting (CoT) advances the reasoning abilities of large language models (LLMs) and achieves superior performance in arithmetic, commonsense, and symbolic reasoning tasks. However, most CoT studies rely on carefully designed human-annotated rational chains to prompt the language model, which poses challenges for real-world applications where labeled training data is available without human-annotated rational chains. This creates barriers to applications of CoT prompting to these general tasks. This paper proposes a new strategy, Automate-CoT (Automatic Prompt Augmentation and Selection with Chain-of-Thought), that can bypass human engineering of CoTs by automatically augmenting rational chains from a small labeled dataset, and then pruning low-quality chains to construct a candidate pool of machine-generated rationale chains based on the labels. Finally, it selects the optimal combination of several rationale chains from the pool for CoT prompting by employing a variance-reduced policy gradient strategy to estimate the significance of each example in a black-box language model. Automate-CoT enables a quick adaptation of the CoT technique to different tasks. Experimental results demonstrate the effectiveness of our method, where state-of-the-art results are achieved on arithmetic reasoning (+2.7{\textbackslash}\%), commonsense reasoning (+3.4{\textbackslash}\%), symbolic reasoning (+3.2{\textbackslash}\%), and non-reasoning tasks (+2.5{\textbackslash}\%). Our code will be available at https://github.com/shizhediao/automate-cot.},
	urldate = {2023-10-22},
	publisher = {arXiv},
	author = {Shum, KaShun and Diao, Shizhe and Zhang, Tong},
	month = feb,
	year = {2023},
	note = {arXiv:2302.12822 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 22 pages, 4 figures, 13 tables},
	annote = {Comment: 22 pages, 4 figures, 13 tables},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\JC3349S2\\2302.html:text/html;Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\8VSI6DSX\\Shum et al. - 2023 - Automatic Prompt Augmentation and Selection with C.pdf:application/pdf},
}

@misc{adams_sparse_2023,
	title = {From {Sparse} to {Dense}: {GPT}-4 {Summarization} with {Chain} of {Density} {Prompting}},
	shorttitle = {From {Sparse} to {Dense}},
	url = {http://arxiv.org/abs/2309.04269},
	abstract = {Selecting the ``right'' amount of information to include in a summary is a difficult task. A good summary should be detailed and entity-centric without being overly dense and hard to follow. To better understand this tradeoff, we solicit increasingly dense GPT-4 summaries with what we refer to as a ``Chain of Density'' (CoD) prompt. Specifically, GPT-4 generates an initial entity-sparse summary before iteratively incorporating missing salient entities without increasing the length. Summaries generated by CoD are more abstractive, exhibit more fusion, and have less of a lead bias than GPT-4 summaries generated by a vanilla prompt. We conduct a human preference study on 100 CNN DailyMail articles and find that that humans prefer GPT-4 summaries that are more dense than those generated by a vanilla prompt and almost as dense as human written summaries. Qualitative analysis supports the notion that there exists a tradeoff between informativeness and readability. 500 annotated CoD summaries, as well as an extra 5,000 unannotated summaries, are freely available on HuggingFace (https://huggingface.co/datasets/griffin/chain\_of\_density).},
	urldate = {2023-10-22},
	publisher = {arXiv},
	author = {Adams, Griffin and Fabbri, Alexander and Ladhak, Faisal and Lehman, Eric and Elhadad, Noémie},
	month = sep,
	year = {2023},
	note = {arXiv:2309.04269 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: preprint},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\QZMTMBS8\\2309.html:text/html;Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\ZQW9FTMM\\Adams et al. - 2023 - From Sparse to Dense GPT-4 Summarization with Cha.pdf:application/pdf},
}

@inproceedings{pu_chatgpt_2023,
	address = {Toronto, Canada},
	title = {{ChatGPT} vs {Human}-authored {Text}: {Insights} into {Controllable} {Text} {Summarization} and {Sentence} {Style} {Transfer}},
	shorttitle = {{ChatGPT} vs {Human}-authored {Text}},
	url = {https://aclanthology.org/2023.acl-srw.1},
	doi = {10.18653/v1/2023.acl-srw.1},
	abstract = {Large-scale language models, like ChatGPT, have garnered significant media attention and stunned the public with their remarkable capacity for generating coherent text from short natural language prompts. In this paper, we aim to conduct a systematic inspection of ChatGPT's performance in two controllable generation tasks, with respect to ChatGPT's ability to adapt its output to different target audiences (expert vs. layman) and writing styles (formal vs. informal). Additionally, we evaluate the faithfulness of the generated text, and compare the model's performance with human-authored texts. Our findings indicate that the stylistic variations produced by humans are considerably larger than those demonstrated by ChatGPT, and the generated texts diverge from human samples in several characteristics, such as the distribution of word types. Moreover, we observe that ChatGPT sometimes incorporates factual errors or hallucinations when adapting the text to suit a specific style.},
	urldate = {2023-10-22},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 4: {Student} {Research} {Workshop})},
	publisher = {Association for Computational Linguistics},
	author = {Pu, Dongqi and Demberg, Vera},
	month = jul,
	year = {2023},
	pages = {1--18},
	file = {Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\8WWZQWEK\\Pu and Demberg - 2023 - ChatGPT vs Human-authored Text Insights into Cont.pdf:application/pdf},
}

@misc{imperial_flesch_2023,
	title = {Flesch or {Fumble}? {Evaluating} {Readability} {Standard} {Alignment} of {Instruction}-{Tuned} {Language} {Models}},
	shorttitle = {Flesch or {Fumble}?},
	url = {http://arxiv.org/abs/2309.05454},
	abstract = {Readability metrics and standards such as Flesch Kincaid Grade Level (FKGL) and the Common European Framework of Reference for Languages (CEFR) exist to guide teachers and educators to properly assess the complexity of educational materials before administering them for classroom use. In this study, we select a diverse set of open and closed-source instruction-tuned language models and investigate their performances in writing story completions and simplifying narratives\$-\$tasks that teachers perform\$-\$using standard-guided prompts controlling text readability. Our extensive findings provide empirical proof of how globally recognized models like ChatGPT may be considered less effective and may require more refined prompts for these generative tasks compared to other open-sourced models such as BLOOMZ and FlanT5\$-\$which have shown promising results.},
	urldate = {2023-10-22},
	publisher = {arXiv},
	author = {Imperial, Joseph Marvin and Madabushi, Harish Tayyar},
	month = sep,
	year = {2023},
	note = {arXiv:2309.05454 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\77FB3SIU\\2309.html:text/html;Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\5ELXLRI3\\Imperial and Madabushi - 2023 - Flesch or Fumble Evaluating Readability Standard .pdf:application/pdf},
}

@inproceedings{bhaskar_prompted_2023,
	address = {Toronto, Canada},
	title = {Prompted {Opinion} {Summarization} with {GPT}-3.5},
	url = {https://aclanthology.org/2023.findings-acl.591},
	doi = {10.18653/v1/2023.findings-acl.591},
	abstract = {Large language models have shown impressive performance across a wide variety of tasks, including text summarization. In this paper, we show that this strong performance extends to opinion summarization. We explore several pipeline methods for applying GPT-3.5 to summarize a large collection of user reviews in aprompted fashion. To handle arbitrarily large numbers of user reviews, we explore recursive summarization as well as methods for selecting salient content to summarize through supervised clustering or extraction. On two datasets, an aspect-oriented summarization dataset of hotel reviews (SPACE) and a generic summarization dataset of Amazon and Yelp reviews (FewSum), we show that GPT-3.5 models achieve very strong performance in human evaluation. We argue that standard evaluation metrics do not reflect this, and introduce three new metrics targeting faithfulness, factuality, and genericity to contrast these different methods.},
	urldate = {2023-10-22},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Bhaskar, Adithya and Fabbri, Alex and Durrett, Greg},
	month = jul,
	year = {2023},
	pages = {9282--9300},
	file = {Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\SSHKLZVA\\Bhaskar et al. - 2023 - Prompted Opinion Summarization with GPT-3.5.pdf:application/pdf},
}

@misc{kojima_large_2023,
	title = {Large {Language} {Models} are {Zero}-{Shot} {Reasoners}},
	url = {http://arxiv.org/abs/2205.11916},
	doi = {10.48550/arXiv.2205.11916},
	abstract = {Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding "Let's think step by step" before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7\% to 78.7\% and GSM8K from 10.4\% to 40.7\% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.},
	urldate = {2023-10-29},
	publisher = {arXiv},
	author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
	month = jan,
	year = {2023},
	note = {arXiv:2205.11916 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Accepted to NeurIPS2022. Our code is available at https://github.com/kojima-takeshi188/zero\_shot\_cot},
	file = {arXiv Fulltext PDF:C\:\\Users\\ijyli\\Zotero\\storage\\4UKI4AFY\\Kojima et al. - 2023 - Large Language Models are Zero-Shot Reasoners.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\BKRVQR4I\\2205.html:text/html},
}

@misc{madaan_self-refine_2023,
	title = {Self-{Refine}: {Iterative} {Refinement} with {Self}-{Feedback}},
	shorttitle = {Self-{Refine}},
	url = {http://arxiv.org/abs/2303.17651},
	abstract = {Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by {\textasciitilde}20\% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.},
	urldate = {2023-10-29},
	publisher = {arXiv},
	author = {Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and Gupta, Shashank and Majumder, Bodhisattwa Prasad and Hermann, Katherine and Welleck, Sean and Yazdanbakhsh, Amir and Clark, Peter},
	month = may,
	year = {2023},
	note = {arXiv:2303.17651 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Code, data, and demo at https://selfrefine.info/},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\PSNHCZMR\\2303.html:text/html;Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\LSS86R2C\\Madaan et al. - 2023 - Self-Refine Iterative Refinement with Self-Feedba.pdf:application/pdf},
}

@misc{zhou_least--most_2023,
	title = {Least-to-{Most} {Prompting} {Enables} {Complex} {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2205.10625},
	abstract = {Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99\% using just 14 exemplars, compared to only 16\% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.},
	urldate = {2023-10-29},
	publisher = {arXiv},
	author = {Zhou, Denny and Schärli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Cui, Claire and Bousquet, Olivier and Le, Quoc and Chi, Ed},
	month = apr,
	year = {2023},
	note = {arXiv:2205.10625 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: ICLR 2023},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\5KBPUC4F\\2205.html:text/html;Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\HCUHQRAB\\Zhou et al. - 2023 - Least-to-Most Prompting Enables Complex Reasoning .pdf:application/pdf},
}

@misc{yao_tree_2023,
	title = {Tree of {Thoughts}: {Deliberate} {Problem} {Solving} with {Large} {Language} {Models}},
	shorttitle = {Tree of {Thoughts}},
	url = {http://arxiv.org/abs/2305.10601},
	doi = {10.48550/arXiv.2305.10601},
	abstract = {Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4\% of tasks, our method achieved a success rate of 74\%. Code repo with all prompts: https://github.com/ysymyth/tree-of-thought-llm.},
	urldate = {2023-10-29},
	publisher = {arXiv},
	author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L. and Cao, Yuan and Narasimhan, Karthik},
	month = may,
	year = {2023},
	note = {arXiv:2305.10601 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Code repo with all prompts: https://github.com/ysymyth/tree-of-thought-llm},
	file = {arXiv Fulltext PDF:C\:\\Users\\ijyli\\Zotero\\storage\\PTNJKLSE\\Yao et al. - 2023 - Tree of Thoughts Deliberate Problem Solving with .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\4JY6IH9K\\2305.html:text/html},
}

@misc{cobbe_training_2021,
	title = {Training {Verifiers} to {Solve} {Math} {Word} {Problems}},
	url = {http://arxiv.org/abs/2110.14168},
	abstract = {State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.},
	urldate = {2023-10-29},
	publisher = {arXiv},
	author = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
	month = nov,
	year = {2021},
	note = {arXiv:2110.14168 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\JJGU8BZU\\2110.html:text/html;Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\3N8S3D2Z\\Cobbe et al. - 2021 - Training Verifiers to Solve Math Word Problems.pdf:application/pdf},
}

@misc{lester_power_2021,
	title = {The {Power} of {Scale} for {Parameter}-{Efficient} {Prompt} {Tuning}},
	url = {http://arxiv.org/abs/2104.08691},
	abstract = {In this work, we explore "prompt tuning", a simple yet effective mechanism for learning "soft prompts" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signal from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's "few-shot" learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method "closes the gap" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant in that large models are costly to share and serve, and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed "prefix tuning" of Li and Liang (2021), and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer, as compared to full model tuning.},
	urldate = {2023-11-01},
	publisher = {arXiv},
	author = {Lester, Brian and Al-Rfou, Rami and Constant, Noah},
	month = sep,
	year = {2021},
	note = {arXiv:2104.08691 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted to EMNLP 2021},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\X4SFQUBY\\2104.html:text/html;Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\Z76HVMW8\\Lester et al. - 2021 - The Power of Scale for Parameter-Efficient Prompt .pdf:application/pdf},
}

@inproceedings{shi_large_2023,
	title = {Large {Language} {Models} {Can} {Be} {Easily} {Distracted} by {Irrelevant} {Context}},
	url = {https://proceedings.mlr.press/v202/shi23a.html},
	abstract = {Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the distractibility of large language models, i.e., how the model prediction can be distracted by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of different prompting techniques for large language models, and find that the model is easily distracted by irrelevant information. We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.},
	language = {en},
	urldate = {2023-11-05},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Shi, Freda and Chen, Xinyun and Misra, Kanishka and Scales, Nathan and Dohan, David and Chi, Ed H. and Schärli, Nathanael and Zhou, Denny},
	month = jul,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {31210--31227},
	file = {Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\NGXAFLUQ\\Shi et al. - 2023 - Large Language Models Can Be Easily Distracted by .pdf:application/pdf},
}

@misc{nltk_nltk_nodate,
	title = {{NLTK} :: {Natural} {Language} {Toolkit}},
	url = {https://www.nltk.org/index.html},
	urldate = {2023-11-07},
	author = {{NLTK}},
	file = {NLTK \:\: Natural Language Toolkit:C\:\\Users\\ijyli\\Zotero\\storage\\Y7ZZGAD3\\index.html:text/html},
}

@misc{reimers_sentence-bert_2019,
	title = {Sentence-{BERT}: {Sentence} {Embeddings} using {Siamese} {BERT}-{Networks}},
	shorttitle = {Sentence-{BERT}},
	url = {http://arxiv.org/abs/1908.10084},
	abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
	urldate = {2023-11-30},
	publisher = {arXiv},
	author = {Reimers, Nils and Gurevych, Iryna},
	month = aug,
	year = {2019},
	note = {arXiv:1908.10084 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Published at EMNLP 2019},
	file = {arXiv.org Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\FSNKUKGJ\\1908.html:text/html;Full Text PDF:C\:\\Users\\ijyli\\Zotero\\storage\\57XNGFV5\\Reimers and Gurevych - 2019 - Sentence-BERT Sentence Embeddings using Siamese B.pdf:application/pdf},
}

@misc{noauthor_sentence-transformersall-distilroberta-v1_nodate,
	title = {sentence-transformers/all-distilroberta-v1 · {Hugging} {Face}},
	url = {https://huggingface.co/sentence-transformers/all-distilroberta-v1},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2023-11-30},
	file = {Snapshot:C\:\\Users\\ijyli\\Zotero\\storage\\B5HC8M2V\\all-distilroberta-v1.html:text/html},
}

@article{landauer_introduction_1998,
	title = {An introduction to latent semantic analysis},
	volume = {25},
	issn = {0163-853X, 1532-6950},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01638539809545028},
	doi = {10.1080/01638539809545028},
	abstract = {Latent Semantic Analysis (LSA) is a theory and method for extracting and representing the contextual-usage meaning of words by statistical computations applied to a large corpus of text (Landauer and Dumais, 1997). The underlying idea is that the aggregate of all the word contexts in which a given word does and does not appear provides a set of mutual constraints that largely determines the similarity of meaning of words and sets of words to each other. The adequacy of LSA’s reflection of human knowledge has been established in a variety of ways. For example, its scores overlap those of humans on standard vocabulary and subject matter tests; it mimics human word sorting and category judgments; it simulates word–word and passage–word lexical priming data; and, as reported in 3 following articles in this issue, it accurately estimates passage coherence, learnability of passages by individual students, and the quality and quantity of knowledge contained in an essay.},
	language = {en},
	number = {2-3},
	urldate = {2023-12-07},
	journal = {Discourse Processes},
	author = {Landauer, Thomas K and Foltz, Peter W. and Laham, Darrell},
	month = jan,
	year = {1998},
	pages = {259--284},
	file = {Landauer et al. - 1998 - An introduction to latent semantic analysis.pdf:C\:\\Users\\ijyli\\Zotero\\storage\\TDVJZL4E\\Landauer et al. - 1998 - An introduction to latent semantic analysis.pdf:application/pdf},
}
